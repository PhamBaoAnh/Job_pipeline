[2025-09-09T01:28:12.174+0000] {processor.py:161} INFO - Started process (PID=22) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:12.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:28:12.179+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:12.179+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:12.205+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:12.250+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:12.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:28:12.264+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:12.264+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:28:12.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.119 seconds
[2025-09-09T01:28:42.479+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:42.480+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:28:42.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:42.481+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:42.496+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:28:42.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:42.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:28:42.532+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:28:42.532+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:28:42.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T01:29:12.748+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:12.749+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:29:12.750+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:12.750+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:12.762+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:12.784+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:12.784+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:29:12.795+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:12.795+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:29:12.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T01:29:43.033+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:43.034+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:29:43.036+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:43.036+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:43.050+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:29:43.077+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:43.077+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:29:43.087+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:29:43.087+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:29:43.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T01:30:13.276+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:13.278+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:30:13.281+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:13.281+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:13.298+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:13.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:13.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:30:13.336+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:13.335+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:30:13.349+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T01:30:43.558+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:43.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:30:43.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:43.559+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:43.572+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:30:43.593+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:43.593+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:30:43.601+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:30:43.601+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:30:43.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T01:31:13.792+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:13.793+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:31:13.794+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:13.794+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:13.805+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:13.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:13.829+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:31:13.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:13.837+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:31:13.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T01:31:44.074+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:44.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:31:44.076+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:44.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:44.088+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:31:44.110+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:44.109+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:31:44.120+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:31:44.119+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:31:44.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T01:32:14.320+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:14.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:32:14.322+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:14.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:14.336+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:14.359+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:14.359+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:32:14.369+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:14.369+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:32:14.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T01:32:44.575+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:44.576+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:32:44.577+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:44.577+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:44.591+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:32:44.613+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:44.612+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:32:44.625+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:32:44.624+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:32:44.642+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T01:33:14.830+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:14.830+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:33:14.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:14.832+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:14.847+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:14.874+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:14.874+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:33:14.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:14.887+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:33:14.905+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T01:33:45.157+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:45.159+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:33:45.161+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:45.161+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:45.175+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:33:45.198+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:45.197+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:33:45.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:33:45.207+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:33:45.222+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T01:34:15.266+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:15.267+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:34:15.269+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:15.269+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:15.288+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:15.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:15.312+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:34:15.322+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:15.322+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:34:15.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T01:34:45.537+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:45.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:34:45.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:45.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:45.558+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:34:45.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:45.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:34:45.599+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:34:45.599+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:34:45.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.202 seconds
[2025-09-09T01:35:15.904+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:15.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:35:15.907+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:15.906+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:15.919+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:15.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:15.939+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:35:16.045+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:16.045+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:35:16.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.157 seconds
[2025-09-09T01:35:46.208+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:46.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:35:46.211+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:46.211+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:46.223+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:35:46.360+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:46.359+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:35:46.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:35:46.367+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:35:46.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.181 seconds
[2025-09-09T01:36:16.582+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:16.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:36:16.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:16.689+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:16.700+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:16.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:16.717+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:36:16.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:16.725+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:36:16.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.162 seconds
[2025-09-09T01:36:46.906+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:46.907+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:36:46.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:46.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:46.920+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:36:46.941+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:46.940+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:36:46.953+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:36:46.953+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:36:46.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T01:37:17.187+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:17.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:37:17.196+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:17.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:17.221+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:17.267+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:17.267+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:37:17.302+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:17.301+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:37:17.340+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.164 seconds
[2025-09-09T01:37:47.510+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:47.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:37:47.513+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:47.513+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:47.527+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:37:47.550+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:47.550+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:37:47.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:37:47.559+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:37:47.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T01:38:17.785+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:17.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:38:17.788+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:17.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:17.807+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:17.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:17.852+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:38:17.870+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:17.870+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:38:17.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.123 seconds
[2025-09-09T01:38:48.080+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:48.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:38:48.083+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:48.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:48.098+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:38:48.129+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:48.129+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:38:48.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:38:48.140+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:38:48.156+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T01:39:18.370+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:39:18.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T01:39:18.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:39:18.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:39:18.384+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T01:39:18.403+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:39:18.403+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T01:39:18.411+0000] {logging_mixin.py:188} INFO - [2025-09-09T01:39:18.411+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T01:39:18.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T02:08:39.214+0000] {processor.py:161} INFO - Started process (PID=22) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:08:39.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:08:39.226+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:08:39.220+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:08:39.293+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:08:39.378+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:08:39.378+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:08:39.398+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:08:39.398+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:08:39.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.215 seconds
[2025-09-09T02:09:09.638+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:09.640+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:09:09.642+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:09.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:09.675+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:09.708+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:09.707+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:09:09.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:09.720+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:09:09.757+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.124 seconds
[2025-09-09T02:09:39.926+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:39.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:09:39.929+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:39.928+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:39.943+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:09:39.975+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:39.975+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:09:39.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:09:39.985+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:09:40.000+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T02:10:10.182+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:10.183+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:10:10.185+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:10.184+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:10.199+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:10.234+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:10.233+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:10:10.246+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:10.246+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:10:10.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.098 seconds
[2025-09-09T02:10:40.531+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:40.532+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:10:40.533+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:40.533+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:40.547+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:10:40.572+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:40.572+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:10:40.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:10:40.588+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:10:40.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T02:11:10.840+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:10.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:11:10.843+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:10.843+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:10.856+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:10.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:10.878+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:11:10.890+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:10.889+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:11:10.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T02:11:41.079+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:41.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:11:41.084+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:41.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:41.101+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:11:41.132+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:41.131+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:11:41.147+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:11:41.147+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:11:41.164+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.094 seconds
[2025-09-09T02:12:11.306+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:11.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:12:11.309+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:11.309+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:11.326+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:11.356+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:11.356+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:12:11.368+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:11.368+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:12:11.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T02:12:41.504+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:41.504+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:12:41.506+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:41.505+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:41.516+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:12:41.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:41.525+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:12:41.534+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:12:41.534+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:12:41.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.046 seconds
[2025-09-09T02:13:11.719+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:11.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:13:11.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:11.720+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:11.731+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:11.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:11.897+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:13:11.904+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:11.904+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:13:11.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.202 seconds
[2025-09-09T02:13:42.083+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:42.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:13:42.085+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:42.084+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:42.098+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:13:42.126+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:42.126+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:13:42.135+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:13:42.135+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:13:42.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T02:14:12.394+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:12.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:14:12.396+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:12.396+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:12.407+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:12.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:12.426+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:14:12.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:12.436+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:14:12.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T02:14:42.607+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:42.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:14:42.610+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:42.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:42.622+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:14:42.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:42.648+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:14:42.657+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:14:42.656+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:14:42.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:15:12.848+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:12.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:15:12.850+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:12.850+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:12.861+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:12.881+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:12.881+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:15:12.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:12.891+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:15:13.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.192 seconds
[2025-09-09T02:15:43.206+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:43.206+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:15:43.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:43.208+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:43.223+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:15:43.247+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:43.246+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:15:43.383+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:15:43.383+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:15:43.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.198 seconds
[2025-09-09T02:16:13.560+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:13.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:16:13.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:13.563+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:13.576+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:13.726+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:13.726+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:16:13.736+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:13.735+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:16:13.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.196 seconds
[2025-09-09T02:16:43.908+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:43.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:16:43.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:43.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:44.056+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:16:44.081+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:44.080+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:16:44.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:16:44.091+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:16:44.106+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.203 seconds
[2025-09-09T02:17:14.280+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:14.281+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:17:14.282+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:14.282+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:14.304+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:14.331+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:14.331+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:17:14.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:14.341+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:17:14.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T02:17:44.526+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:44.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:17:44.528+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:44.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:44.541+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:17:44.567+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:44.567+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:17:44.578+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:17:44.578+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:17:44.596+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T02:18:14.753+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:14.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:18:14.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:14.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:14.771+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:14.794+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:14.794+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:18:14.806+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:14.806+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:18:14.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T02:18:44.992+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:44.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:18:44.997+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:44.996+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:45.012+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:18:45.034+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:45.033+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:18:45.043+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:18:45.043+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:18:45.056+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T02:19:15.235+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:15.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:19:15.237+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:15.237+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:15.249+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:15.270+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:15.270+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:19:15.281+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:15.280+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:19:15.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:19:45.476+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:45.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:19:45.478+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:45.478+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:45.492+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:19:45.512+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:45.512+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:19:45.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:19:45.520+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:19:45.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T02:20:15.712+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:15.714+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:20:15.716+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:15.715+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:15.731+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:15.747+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:15.747+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:20:15.762+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:15.762+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:20:15.780+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T02:20:45.949+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:45.950+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:20:45.952+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:45.952+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:45.968+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:20:46.064+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:46.064+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:20:46.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:20:46.071+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:20:46.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.149 seconds
[2025-09-09T02:21:16.369+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:16.370+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:21:16.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:16.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:16.393+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:16.420+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:16.419+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:21:16.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:16.429+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:21:16.444+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.091 seconds
[2025-09-09T02:21:46.637+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:46.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:21:46.639+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:46.639+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:46.650+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:21:46.669+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:46.669+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:21:46.677+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:21:46.677+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:21:46.690+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T02:22:16.869+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:16.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:22:16.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:16.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:16.883+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:16.912+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:16.912+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:22:16.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:16.921+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:22:16.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T02:22:47.154+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:47.156+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:22:47.159+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:47.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:47.170+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:22:47.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:47.192+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:22:47.203+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:22:47.203+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:22:47.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T02:23:17.396+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:17.398+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:23:17.400+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:17.400+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:17.416+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:17.445+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:17.445+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:23:17.455+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:17.455+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:23:17.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T02:23:47.645+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:47.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:23:47.647+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:47.647+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:47.660+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:23:47.680+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:47.680+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:23:47.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:23:47.688+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:23:47.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T02:24:17.890+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:17.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:24:17.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:17.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:17.904+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:17.925+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:17.925+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:24:17.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:17.934+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:24:17.946+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T02:24:48.138+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:48.139+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:24:48.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:48.140+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:48.155+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:24:48.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:48.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:24:48.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:24:48.192+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:24:48.209+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T02:25:18.387+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:18.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:25:18.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:18.389+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:18.404+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:18.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:18.425+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:25:18.433+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:18.433+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:25:18.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T02:25:48.616+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:48.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:25:48.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:48.618+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:48.631+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:25:48.654+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:48.654+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:25:48.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:25:48.665+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:25:48.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T02:26:18.919+0000] {processor.py:161} INFO - Started process (PID=124) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:18.920+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:26:18.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:18.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:18.933+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:18.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:18.956+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:26:18.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:18.967+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:26:18.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T02:26:49.253+0000] {processor.py:161} INFO - Started process (PID=127) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:49.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:26:49.256+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:49.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:49.271+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:26:49.296+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:49.296+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:26:49.306+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:26:49.306+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:26:49.322+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T02:27:19.529+0000] {processor.py:161} INFO - Started process (PID=130) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:19.530+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:27:19.532+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:19.532+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:19.549+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:19.577+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:19.577+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:27:19.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:19.588+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:27:19.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T02:27:49.811+0000] {processor.py:161} INFO - Started process (PID=133) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:49.812+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:27:49.814+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:49.813+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:49.826+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:27:49.852+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:49.852+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:27:49.861+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:27:49.861+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:27:49.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:28:20.086+0000] {processor.py:161} INFO - Started process (PID=136) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:20.087+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:28:20.088+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:20.088+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:20.101+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:20.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:20.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:28:20.135+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:20.135+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:28:20.149+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:28:50.371+0000] {processor.py:161} INFO - Started process (PID=139) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:50.372+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:28:50.374+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:50.374+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:50.387+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:28:50.409+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:50.409+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:28:50.419+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:28:50.419+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:28:50.433+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:29:20.643+0000] {processor.py:161} INFO - Started process (PID=142) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:20.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:29:20.646+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:20.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:20.675+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:20.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:20.722+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:29:20.733+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:20.733+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:29:20.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T02:29:50.942+0000] {processor.py:161} INFO - Started process (PID=145) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:50.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:29:50.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:50.944+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:50.957+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:29:50.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:50.981+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:29:50.991+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:29:50.991+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:29:51.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T02:30:21.190+0000] {processor.py:161} INFO - Started process (PID=148) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:21.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:30:21.194+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:21.194+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:21.213+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:21.252+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:21.252+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:30:21.268+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:21.268+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:30:21.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.115 seconds
[2025-09-09T02:30:51.472+0000] {processor.py:161} INFO - Started process (PID=151) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:51.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:30:51.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:51.474+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:51.488+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:30:51.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:51.509+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:30:51.518+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:30:51.518+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:30:51.533+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T02:31:21.755+0000] {processor.py:161} INFO - Started process (PID=154) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:21.756+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:31:21.759+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:21.758+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:21.793+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:21.928+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:21.928+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:31:21.975+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:21.975+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:31:22.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.285 seconds
[2025-09-09T02:31:52.238+0000] {processor.py:161} INFO - Started process (PID=157) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:52.239+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:31:52.240+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:52.240+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:52.255+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:31:52.280+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:52.280+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:31:52.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:31:52.291+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:31:52.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T02:32:22.487+0000] {processor.py:161} INFO - Started process (PID=160) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:22.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:32:22.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:22.488+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:22.502+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:22.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:22.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:32:22.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:22.540+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:32:22.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T02:32:52.759+0000] {processor.py:161} INFO - Started process (PID=163) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:52.760+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:32:52.761+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:52.761+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:52.780+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:32:52.808+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:52.808+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:32:52.821+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:32:52.821+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:32:52.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T02:33:23.031+0000] {processor.py:161} INFO - Started process (PID=166) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:23.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:33:23.035+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:23.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:23.066+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:23.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:23.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:33:23.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:23.146+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:33:23.187+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.169 seconds
[2025-09-09T02:33:53.386+0000] {processor.py:161} INFO - Started process (PID=169) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:53.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:33:53.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:53.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:53.403+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:33:53.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:53.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:33:53.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:33:53.436+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:33:53.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T02:34:23.646+0000] {processor.py:161} INFO - Started process (PID=172) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:23.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:34:23.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:23.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:23.663+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:23.684+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:23.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:34:23.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:23.693+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:34:23.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T02:34:54.065+0000] {processor.py:161} INFO - Started process (PID=175) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:54.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:34:54.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:54.067+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:54.083+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:34:54.108+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:54.108+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:34:54.119+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:34:54.119+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:34:54.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T02:35:24.304+0000] {processor.py:161} INFO - Started process (PID=178) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:24.305+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:35:24.306+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:24.306+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:24.320+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:24.344+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:24.344+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:35:24.354+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:24.354+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:35:24.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T02:35:54.607+0000] {processor.py:161} INFO - Started process (PID=181) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:54.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:35:54.610+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:54.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:54.627+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:35:54.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:54.679+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:35:54.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:35:54.694+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:35:54.710+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.115 seconds
[2025-09-09T02:36:25.224+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:25.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:36:25.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:25.226+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:25.254+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:25.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:25.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:36:25.305+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:25.305+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:36:25.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.117 seconds
[2025-09-09T02:36:55.552+0000] {processor.py:161} INFO - Started process (PID=207) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:55.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:36:55.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:55.555+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:55.580+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:36:55.608+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:55.607+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:36:55.619+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:36:55.618+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:36:55.634+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T02:37:25.804+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:25.805+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:37:25.805+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:25.805+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:25.818+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:25.839+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:25.838+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:37:25.847+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:25.847+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:37:25.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T02:37:56.017+0000] {processor.py:161} INFO - Started process (PID=213) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:56.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:37:56.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:56.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:56.032+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:37:56.055+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:56.055+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:37:56.066+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:37:56.066+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:37:56.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T02:38:26.278+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:26.278+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:38:26.279+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:26.279+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:26.291+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:26.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:26.311+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:38:26.319+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:26.319+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:38:26.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T02:38:56.491+0000] {processor.py:161} INFO - Started process (PID=224) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:56.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:38:56.492+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:56.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:56.511+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:38:56.542+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:56.542+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:38:56.556+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:38:56.556+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:38:56.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.092 seconds
[2025-09-09T02:39:26.836+0000] {processor.py:161} INFO - Started process (PID=226) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:26.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:39:26.838+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:26.838+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:26.854+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:26.877+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:26.876+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:39:26.885+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:26.885+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:39:26.898+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T02:39:57.101+0000] {processor.py:161} INFO - Started process (PID=229) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:57.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:39:57.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:57.107+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:57.128+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:39:57.151+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:57.151+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:39:57.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:39:57.163+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:39:57.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.102 seconds
[2025-09-09T02:40:27.370+0000] {processor.py:161} INFO - Started process (PID=232) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:27.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:40:27.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:27.372+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:27.386+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:27.409+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:27.409+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:40:27.419+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:27.419+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:40:27.432+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:40:57.621+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:57.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:40:57.623+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:57.622+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:57.637+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:40:57.662+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:57.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:40:57.677+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:40:57.677+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:40:57.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.094 seconds
[2025-09-09T02:41:28.058+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:28.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:41:28.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:28.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:28.081+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:28.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:28.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:41:28.139+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:28.138+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:41:28.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.104 seconds
[2025-09-09T02:41:58.346+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:58.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:41:58.348+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:58.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:58.361+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:41:58.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:58.382+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:41:58.391+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:41:58.391+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:41:58.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T02:42:28.623+0000] {processor.py:161} INFO - Started process (PID=280) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:28.625+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:42:28.626+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:28.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:28.644+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:28.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:28.674+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:42:28.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:28.686+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:42:28.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T02:42:58.874+0000] {processor.py:161} INFO - Started process (PID=289) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:58.875+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:42:58.877+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:58.876+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:58.890+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:42:58.912+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:58.912+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:42:58.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:42:58.920+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:42:58.935+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T02:43:29.054+0000] {processor.py:161} INFO - Started process (PID=310) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:29.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:43:29.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:29.055+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:29.068+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:29.090+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:29.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:43:29.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:29.099+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:43:29.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T02:43:59.325+0000] {processor.py:161} INFO - Started process (PID=319) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:59.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:43:59.327+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:59.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:59.345+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:43:59.380+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:59.379+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:43:59.397+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:43:59.397+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:43:59.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.120 seconds
[2025-09-09T02:44:29.626+0000] {processor.py:161} INFO - Started process (PID=334) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:29.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:44:29.628+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:29.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:29.640+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:29.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:29.660+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:44:29.670+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:29.670+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:44:29.686+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T02:44:59.903+0000] {processor.py:161} INFO - Started process (PID=337) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:59.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:44:59.905+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:59.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:59.918+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:44:59.944+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:59.943+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:44:59.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:44:59.954+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:44:59.969+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T02:45:30.190+0000] {processor.py:161} INFO - Started process (PID=346) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:45:30.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:45:30.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:45:30.192+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:45:30.209+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:45:30.243+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:45:30.243+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:45:30.254+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:45:30.254+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:45:30.270+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T02:46:00.501+0000] {processor.py:161} INFO - Started process (PID=349) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:00.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:46:00.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:00.503+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:00.520+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:00.546+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:00.546+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:46:00.555+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:00.555+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:46:00.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T02:46:30.862+0000] {processor.py:161} INFO - Started process (PID=352) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:30.863+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:46:30.864+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:30.863+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:30.876+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:46:30.900+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:30.900+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:46:30.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:46:30.908+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:46:30.921+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T02:47:01.164+0000] {processor.py:161} INFO - Started process (PID=355) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:01.165+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:47:01.166+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:01.166+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:01.193+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:01.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:01.220+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:47:01.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:01.229+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:47:01.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.098 seconds
[2025-09-09T02:47:31.556+0000] {processor.py:161} INFO - Started process (PID=358) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:31.557+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:47:31.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:31.557+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:31.574+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:47:31.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:31.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:47:31.608+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:47:31.608+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:47:31.624+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T02:48:01.836+0000] {processor.py:161} INFO - Started process (PID=361) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:01.837+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:48:01.838+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:01.838+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:01.853+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:01.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:01.879+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:48:01.892+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:01.891+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:48:01.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T02:48:32.121+0000] {processor.py:161} INFO - Started process (PID=364) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:32.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:48:32.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:32.122+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:32.138+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:48:32.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:32.175+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:48:32.188+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:48:32.187+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:48:32.203+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T02:49:02.444+0000] {processor.py:161} INFO - Started process (PID=367) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:02.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:49:02.446+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:02.446+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:02.459+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:02.485+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:02.485+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:49:02.497+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:02.497+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:49:02.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T02:49:32.797+0000] {processor.py:161} INFO - Started process (PID=370) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:32.798+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:49:32.799+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:32.799+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:32.816+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:49:32.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:32.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:49:32.851+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:49:32.851+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:49:32.865+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T02:50:03.019+0000] {processor.py:161} INFO - Started process (PID=373) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:03.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:50:03.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:03.021+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:03.035+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:03.057+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:03.057+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:50:03.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:03.066+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:50:03.081+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T02:50:33.231+0000] {processor.py:161} INFO - Started process (PID=376) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:33.233+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:50:33.234+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:33.234+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:33.252+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:50:33.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:33.291+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:50:33.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:50:33.303+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:50:33.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.097 seconds
[2025-09-09T02:51:03.465+0000] {processor.py:161} INFO - Started process (PID=379) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:03.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:51:03.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:03.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:03.480+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:03.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:03.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:51:03.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:03.513+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:51:03.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T02:51:33.680+0000] {processor.py:161} INFO - Started process (PID=382) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:33.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:51:33.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:33.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:33.697+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:51:33.723+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:33.723+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:51:33.734+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:51:33.733+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:51:33.752+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T02:52:03.895+0000] {processor.py:161} INFO - Started process (PID=385) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:03.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:52:03.896+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:03.896+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:03.911+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:03.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:03.936+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:52:03.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:03.945+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:52:03.959+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T02:52:34.097+0000] {processor.py:161} INFO - Started process (PID=388) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:34.098+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:52:34.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:34.099+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:34.111+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:52:34.133+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:34.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:52:34.142+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:52:34.142+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:52:34.158+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T02:53:04.295+0000] {processor.py:161} INFO - Started process (PID=391) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:04.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:53:04.297+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:04.296+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:04.310+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:04.332+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:04.332+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:53:04.342+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:04.341+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:53:04.357+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T02:53:34.493+0000] {processor.py:161} INFO - Started process (PID=394) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:34.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:53:34.495+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:34.494+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:34.511+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:53:34.533+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:34.532+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:53:34.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:53:34.543+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:53:34.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T02:54:04.758+0000] {processor.py:161} INFO - Started process (PID=396) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:04.759+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:54:04.760+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:04.759+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:04.773+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:04.801+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:04.801+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:54:04.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:04.819+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:54:04.838+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T02:54:35.075+0000] {processor.py:161} INFO - Started process (PID=398) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:35.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:54:35.076+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:35.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:35.089+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:54:35.111+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:35.110+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:54:35.121+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:54:35.120+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:54:35.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T02:55:05.404+0000] {processor.py:161} INFO - Started process (PID=401) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:05.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:55:05.406+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:05.406+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:05.418+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:05.439+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:05.439+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:55:05.448+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:05.448+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:55:05.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T02:55:35.673+0000] {processor.py:161} INFO - Started process (PID=404) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:35.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:55:35.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:35.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:35.687+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:55:35.708+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:35.707+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:55:35.716+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:55:35.716+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:55:35.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T02:56:05.929+0000] {processor.py:161} INFO - Started process (PID=407) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:05.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:56:05.930+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:05.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:05.941+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:05.961+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:05.961+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:56:05.969+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:05.969+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:56:05.982+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T02:56:36.221+0000] {processor.py:161} INFO - Started process (PID=410) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:36.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:56:36.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:36.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:36.234+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:56:36.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:36.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:56:36.272+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:56:36.272+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:56:36.286+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T02:57:06.620+0000] {processor.py:161} INFO - Started process (PID=413) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:06.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:57:06.622+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:06.622+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:06.633+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:06.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:06.653+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:57:06.662+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:06.662+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:57:06.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T02:57:36.939+0000] {processor.py:161} INFO - Started process (PID=416) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:36.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:57:36.940+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:36.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:36.953+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:57:36.973+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:36.973+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:57:36.982+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:57:36.982+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:57:36.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T02:58:07.271+0000] {processor.py:161} INFO - Started process (PID=419) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:07.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:58:07.273+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:07.272+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:07.285+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:07.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:07.308+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:58:07.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:07.318+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:58:07.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T02:58:37.538+0000] {processor.py:161} INFO - Started process (PID=422) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:37.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:58:37.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:37.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:37.555+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:58:37.578+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:37.578+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:58:37.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:58:37.587+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:58:37.601+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T02:59:07.836+0000] {processor.py:161} INFO - Started process (PID=425) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:07.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:59:07.839+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:07.839+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:07.877+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:07.920+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:07.920+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:59:07.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:07.938+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:59:07.963+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.137 seconds
[2025-09-09T02:59:38.185+0000] {processor.py:161} INFO - Started process (PID=428) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:38.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T02:59:38.187+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:38.186+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:38.203+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T02:59:38.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:38.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T02:59:38.239+0000] {logging_mixin.py:188} INFO - [2025-09-09T02:59:38.239+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T02:59:38.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T03:00:08.466+0000] {processor.py:161} INFO - Started process (PID=431) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:08.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:00:08.468+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:08.468+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:08.479+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:08.500+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:08.500+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:00:08.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:08.509+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:00:08.521+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:00:38.729+0000] {processor.py:161} INFO - Started process (PID=434) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:38.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:00:38.731+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:38.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:38.747+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:00:38.771+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:38.771+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:00:38.781+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:00:38.781+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:00:38.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T03:01:08.972+0000] {processor.py:161} INFO - Started process (PID=437) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:08.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:01:08.974+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:08.974+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:08.987+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:09.016+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:09.016+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:01:09.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:09.026+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:01:09.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T03:01:39.234+0000] {processor.py:161} INFO - Started process (PID=440) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:39.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:01:39.235+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:39.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:39.249+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:01:39.274+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:39.274+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:01:39.283+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:01:39.283+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:01:39.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T03:02:09.617+0000] {processor.py:161} INFO - Started process (PID=443) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:09.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:02:09.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:09.618+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:09.630+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:09.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:09.651+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:02:09.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:09.660+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:02:09.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:02:39.909+0000] {processor.py:161} INFO - Started process (PID=446) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:39.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:02:39.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:39.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:39.922+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:02:39.944+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:39.944+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:02:39.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:02:39.954+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:02:39.967+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:03:10.189+0000] {processor.py:161} INFO - Started process (PID=449) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:10.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:03:10.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:10.191+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:10.214+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:10.242+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:10.242+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:03:10.252+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:10.252+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:03:10.265+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T03:03:40.504+0000] {processor.py:161} INFO - Started process (PID=452) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:40.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:03:40.505+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:40.505+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:40.517+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:03:40.537+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:40.536+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:03:40.546+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:03:40.546+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:03:40.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T03:04:10.916+0000] {processor.py:161} INFO - Started process (PID=455) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:10.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:04:10.918+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:10.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:10.934+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:10.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:10.959+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:04:10.969+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:10.969+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:04:10.982+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T03:04:41.207+0000] {processor.py:161} INFO - Started process (PID=458) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:41.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:04:41.209+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:41.208+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:41.220+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:04:41.245+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:41.245+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:04:41.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:04:41.253+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:04:41.266+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:05:11.444+0000] {processor.py:161} INFO - Started process (PID=461) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:11.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:05:11.446+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:11.445+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:11.460+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:11.484+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:11.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:05:11.493+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:11.493+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:05:11.507+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T03:05:41.676+0000] {processor.py:161} INFO - Started process (PID=464) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:41.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:05:41.678+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:41.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:41.690+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:05:41.710+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:41.710+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:05:41.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:05:41.719+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:05:41.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T03:06:11.876+0000] {processor.py:161} INFO - Started process (PID=467) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:11.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:06:11.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:11.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:11.895+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:11.922+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:11.922+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:06:11.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:11.932+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:06:11.947+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T03:06:42.132+0000] {processor.py:161} INFO - Started process (PID=470) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:42.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:06:42.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:42.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:42.145+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:06:42.167+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:42.167+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:06:42.177+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:06:42.177+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:06:42.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:07:12.391+0000] {processor.py:161} INFO - Started process (PID=473) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:12.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:07:12.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:12.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:12.406+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:12.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:12.428+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:07:12.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:12.438+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:07:12.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T03:07:42.612+0000] {processor.py:161} INFO - Started process (PID=476) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:42.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:07:42.614+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:42.614+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:42.625+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:07:42.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:42.643+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:07:42.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:07:42.652+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:07:42.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T03:08:12.929+0000] {processor.py:161} INFO - Started process (PID=479) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:12.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:08:12.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:12.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:12.943+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:12.964+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:12.964+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:08:12.972+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:12.972+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:08:12.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T03:08:43.191+0000] {processor.py:161} INFO - Started process (PID=482) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:43.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:08:43.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:43.192+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:43.205+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:08:43.225+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:43.225+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:08:43.234+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:08:43.234+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:08:43.246+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T03:09:13.537+0000] {processor.py:161} INFO - Started process (PID=485) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:13.538+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:09:13.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:13.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:13.551+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:13.571+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:13.570+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:09:13.579+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:13.578+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:09:13.591+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T03:09:43.840+0000] {processor.py:161} INFO - Started process (PID=488) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:43.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:09:43.842+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:43.842+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:43.854+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:09:43.873+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:43.873+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:09:43.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:09:43.881+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:09:43.894+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.057 seconds
[2025-09-09T03:10:14.143+0000] {processor.py:161} INFO - Started process (PID=491) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:14.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:10:14.144+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:14.144+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:14.156+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:14.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:14.175+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:10:14.184+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:14.184+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:10:14.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:10:44.405+0000] {processor.py:161} INFO - Started process (PID=493) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:44.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:10:44.406+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:44.406+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:44.421+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:10:44.444+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:44.444+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:10:44.453+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:10:44.452+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:10:44.466+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T03:11:14.740+0000] {processor.py:161} INFO - Started process (PID=495) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:14.740+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:11:14.741+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:14.741+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:14.753+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:14.773+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:14.773+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:11:14.781+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:14.781+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:11:14.793+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:11:44.998+0000] {processor.py:161} INFO - Started process (PID=498) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:44.999+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:11:45.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:45.000+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:45.015+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:11:45.038+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:45.038+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:11:45.047+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:11:45.047+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:11:45.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T03:12:15.359+0000] {processor.py:161} INFO - Started process (PID=501) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:15.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:12:15.360+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:15.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:15.381+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:15.414+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:15.413+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:12:15.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:15.425+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:12:15.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.091 seconds
[2025-09-09T03:12:45.727+0000] {processor.py:161} INFO - Started process (PID=504) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:45.727+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:12:45.728+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:45.728+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:45.741+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:12:45.762+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:45.761+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:12:45.771+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:12:45.770+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:12:45.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T03:13:16.001+0000] {processor.py:161} INFO - Started process (PID=507) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:16.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:13:16.004+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:16.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:16.022+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:16.048+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:16.048+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:13:16.057+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:16.057+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:13:16.072+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T03:13:46.325+0000] {processor.py:161} INFO - Started process (PID=510) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:46.325+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:13:46.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:46.326+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:46.339+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:13:46.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:46.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:13:46.370+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:13:46.370+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:13:46.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:14:16.684+0000] {processor.py:161} INFO - Started process (PID=513) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:16.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:14:16.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:16.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:16.697+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:16.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:16.717+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:14:16.726+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:16.725+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:14:16.737+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T03:14:46.992+0000] {processor.py:161} INFO - Started process (PID=516) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:46.993+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:14:46.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:46.994+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:47.009+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:14:47.035+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:47.035+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:14:47.045+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:14:47.045+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:14:47.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T03:15:17.350+0000] {processor.py:161} INFO - Started process (PID=519) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:17.351+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:15:17.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:17.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:17.365+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:17.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:17.392+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:15:17.402+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:17.402+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:15:17.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T03:15:47.606+0000] {processor.py:161} INFO - Started process (PID=522) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:47.606+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:15:47.607+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:47.607+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:47.619+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:15:47.638+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:47.638+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:15:47.646+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:15:47.646+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:15:47.659+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:16:17.941+0000] {processor.py:161} INFO - Started process (PID=525) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:17.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:16:17.943+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:17.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:17.955+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:17.978+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:17.978+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:16:17.988+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:17.988+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:16:18.003+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T03:16:48.243+0000] {processor.py:161} INFO - Started process (PID=528) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:48.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:16:48.245+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:48.245+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:48.258+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:16:48.281+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:48.280+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:16:48.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:16:48.291+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:16:48.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T03:17:18.606+0000] {processor.py:161} INFO - Started process (PID=531) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:18.607+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:17:18.608+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:18.607+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:18.619+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:18.640+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:18.640+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:17:18.650+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:18.649+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:17:18.663+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T03:17:48.885+0000] {processor.py:161} INFO - Started process (PID=534) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:48.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:17:48.887+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:48.887+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:48.898+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:17:48.919+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:48.918+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:17:48.927+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:17:48.927+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:17:48.939+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:18:19.085+0000] {processor.py:161} INFO - Started process (PID=537) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:19.086+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:18:19.087+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:19.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:19.100+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:19.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:19.123+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:18:19.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:19.134+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:18:19.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T03:18:49.296+0000] {processor.py:161} INFO - Started process (PID=540) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:49.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:18:49.298+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:49.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:49.312+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:18:49.333+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:49.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:18:49.343+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:18:49.342+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:18:49.357+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T03:19:19.495+0000] {processor.py:161} INFO - Started process (PID=543) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:19.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:19:19.497+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:19.496+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:19.509+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:19.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:19.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:19:19.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:19.539+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:19:19.553+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:19:49.692+0000] {processor.py:161} INFO - Started process (PID=546) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:49.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:19:49.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:49.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:49.706+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:19:49.727+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:49.726+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:19:49.736+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:19:49.736+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:19:49.749+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T03:20:19.895+0000] {processor.py:161} INFO - Started process (PID=549) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:19.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:20:19.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:19.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:19.912+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:19.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:19.936+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:20:19.947+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:19.947+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:20:19.962+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T03:20:50.096+0000] {processor.py:161} INFO - Started process (PID=552) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:50.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:20:50.097+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:50.097+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:50.110+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:20:50.133+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:50.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:20:50.141+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:20:50.141+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:20:50.154+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:21:20.289+0000] {processor.py:161} INFO - Started process (PID=555) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:20.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:21:20.290+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:20.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:20.304+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:20.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:20.326+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:21:20.335+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:20.335+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:21:20.349+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T03:21:50.485+0000] {processor.py:161} INFO - Started process (PID=558) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:50.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:21:50.487+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:50.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:50.500+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:21:50.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:50.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:21:50.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:21:50.529+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:21:50.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T03:22:20.674+0000] {processor.py:161} INFO - Started process (PID=561) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:20.675+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:22:20.676+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:20.676+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:20.691+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:20.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:20.710+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:22:20.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:20.719+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:22:20.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T03:22:50.916+0000] {processor.py:161} INFO - Started process (PID=564) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:50.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:22:50.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:50.917+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:50.928+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:22:50.948+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:50.948+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:22:50.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:22:50.955+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:22:50.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.057 seconds
[2025-09-09T03:23:21.206+0000] {processor.py:161} INFO - Started process (PID=567) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:21.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:23:21.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:21.207+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:21.219+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:21.239+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:21.238+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:23:21.248+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:21.247+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:23:21.261+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:23:51.493+0000] {processor.py:161} INFO - Started process (PID=570) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:51.494+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:23:51.495+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:51.494+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:51.508+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:23:51.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:51.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:23:51.537+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:23:51.537+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:23:51.551+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T03:24:21.792+0000] {processor.py:161} INFO - Started process (PID=573) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:21.792+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:24:21.793+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:21.793+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:21.806+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:21.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:21.826+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:24:21.834+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:21.834+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:24:21.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T03:24:52.060+0000] {processor.py:161} INFO - Started process (PID=576) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:52.060+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:24:52.061+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:52.061+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:52.072+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:24:52.093+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:52.093+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:24:52.106+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:24:52.106+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:24:52.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T03:25:22.411+0000] {processor.py:161} INFO - Started process (PID=579) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:22.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:25:22.413+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:22.413+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:22.424+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:22.443+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:22.443+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:25:22.452+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:22.452+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:25:22.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T03:25:52.695+0000] {processor.py:161} INFO - Started process (PID=582) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:52.696+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:25:52.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:52.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:52.708+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:25:52.728+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:52.727+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:25:52.736+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:25:52.735+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:25:52.748+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T03:26:23.027+0000] {processor.py:161} INFO - Started process (PID=584) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:23.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:26:23.029+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:23.028+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:23.041+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:23.062+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:23.062+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:26:23.072+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:23.071+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:26:23.084+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T03:26:53.327+0000] {processor.py:161} INFO - Started process (PID=586) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:53.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:26:53.329+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:53.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:53.346+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:26:53.369+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:53.369+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:26:53.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:26:53.378+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:26:53.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T03:27:23.682+0000] {processor.py:161} INFO - Started process (PID=589) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:23.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:27:23.683+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:23.683+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:23.696+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:23.716+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:23.716+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:27:23.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:23.725+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:27:23.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T03:27:53.978+0000] {processor.py:161} INFO - Started process (PID=592) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:53.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T03:27:53.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:53.980+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:53.994+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T03:27:54.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:54.020+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T03:27:54.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T03:27:54.029+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T03:27:54.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T04:04:48.675+0000] {processor.py:161} INFO - Started process (PID=596) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:04:48.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:04:48.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:04:48.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:04:48.814+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:04:48.977+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:04:48.977+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:04:49.043+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:04:49.043+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:04:49.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.528 seconds
[2025-09-09T04:05:19.851+0000] {processor.py:161} INFO - Started process (PID=598) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:19.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:05:19.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:19.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:19.868+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:19.895+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:19.895+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:05:19.905+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:19.905+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:05:19.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T04:05:50.078+0000] {processor.py:161} INFO - Started process (PID=601) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:50.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:05:50.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:50.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:50.093+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:05:50.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:50.114+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:05:50.124+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:05:50.124+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:05:50.141+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T04:06:20.290+0000] {processor.py:161} INFO - Started process (PID=604) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:20.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:06:20.292+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:20.292+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:20.305+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:20.329+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:20.329+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:06:20.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:20.340+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:06:20.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T04:06:50.520+0000] {processor.py:161} INFO - Started process (PID=607) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:50.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:06:50.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:50.521+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:50.535+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:06:50.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:50.560+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:06:50.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:06:50.569+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:06:50.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T04:07:20.747+0000] {processor.py:161} INFO - Started process (PID=610) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:20.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:07:20.749+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:20.749+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:20.762+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:20.787+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:20.787+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:07:20.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:20.797+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:07:20.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T04:07:50.977+0000] {processor.py:161} INFO - Started process (PID=613) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:50.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:07:50.979+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:50.978+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:50.993+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:07:51.017+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:51.017+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:07:51.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:07:51.027+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:07:51.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T04:08:21.215+0000] {processor.py:161} INFO - Started process (PID=616) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:21.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:08:21.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:21.216+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:21.231+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:21.256+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:21.256+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:08:21.266+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:21.265+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:08:21.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T04:08:51.450+0000] {processor.py:161} INFO - Started process (PID=619) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:51.451+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:08:51.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:51.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:51.465+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:08:51.490+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:51.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:08:51.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:08:51.499+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:08:51.515+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T04:09:21.669+0000] {processor.py:161} INFO - Started process (PID=622) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:21.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:09:21.670+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:21.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:21.683+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:21.708+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:21.708+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:09:21.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:21.719+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:09:21.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T04:09:51.913+0000] {processor.py:161} INFO - Started process (PID=625) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:51.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:09:51.915+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:51.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:51.936+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:09:51.961+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:51.961+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:09:51.970+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:09:51.969+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:09:51.982+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T04:10:22.147+0000] {processor.py:161} INFO - Started process (PID=628) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:22.148+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:10:22.149+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:22.149+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:22.163+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:22.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:22.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:10:22.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:22.195+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:10:22.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T04:10:52.359+0000] {processor.py:161} INFO - Started process (PID=631) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:52.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:10:52.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:52.360+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:52.374+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:10:52.397+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:52.397+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:10:52.407+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:10:52.406+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:10:52.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T04:11:22.589+0000] {processor.py:161} INFO - Started process (PID=634) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:22.590+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:11:22.590+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:22.590+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:22.605+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:22.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:22.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:11:22.641+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:22.640+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:11:22.658+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T04:11:52.837+0000] {processor.py:161} INFO - Started process (PID=637) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:52.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:11:52.839+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:52.839+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:52.854+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:11:52.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:52.882+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:11:52.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:11:52.893+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:11:52.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T04:12:23.078+0000] {processor.py:161} INFO - Started process (PID=640) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:23.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:12:23.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:23.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:23.093+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:23.118+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:23.117+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:12:23.129+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:23.129+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:12:23.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T04:12:53.345+0000] {processor.py:161} INFO - Started process (PID=643) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:53.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:12:53.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:53.346+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:53.359+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:12:53.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:53.379+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:12:53.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:12:53.388+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:12:53.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T04:13:23.665+0000] {processor.py:161} INFO - Started process (PID=646) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:23.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:13:23.666+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:23.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:23.683+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:23.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:23.728+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:13:23.751+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:23.750+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:13:23.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.113 seconds
[2025-09-09T04:13:53.936+0000] {processor.py:161} INFO - Started process (PID=757) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:53.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:13:53.938+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:53.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:53.951+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:13:53.975+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:53.974+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:13:53.984+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:13:53.984+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:13:53.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T04:14:24.185+0000] {processor.py:161} INFO - Started process (PID=760) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:24.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:14:24.187+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:24.187+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:24.202+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:24.226+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:24.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:14:24.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:24.236+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:14:24.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T04:14:54.457+0000] {processor.py:161} INFO - Started process (PID=763) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:54.458+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:14:54.459+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:54.459+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:54.476+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:14:54.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:54.499+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:14:54.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:14:54.509+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:14:54.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T04:15:24.724+0000] {processor.py:161} INFO - Started process (PID=766) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:24.725+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:15:24.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:24.725+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:24.739+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:24.760+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:24.759+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:15:24.768+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:24.768+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:15:24.780+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T04:15:54.956+0000] {processor.py:161} INFO - Started process (PID=769) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:54.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:15:54.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:54.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:54.972+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:15:54.993+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:54.993+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:15:55.003+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:15:55.002+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:15:55.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T04:16:25.193+0000] {processor.py:161} INFO - Started process (PID=772) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:25.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:16:25.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:25.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:25.208+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:25.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:25.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:16:25.237+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:25.237+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:16:25.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T04:16:55.420+0000] {processor.py:161} INFO - Started process (PID=775) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:55.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:16:55.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:55.422+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:55.437+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:16:55.461+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:55.460+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:16:55.470+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:16:55.470+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:16:55.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T04:17:00.498+0000] {processor.py:161} INFO - Started process (PID=776) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:00.499+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:17:00.500+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:00.499+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:00.520+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:00.661+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:00.661+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:17:00.667+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:00.667+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:17:00.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.190 seconds
[2025-09-09T04:17:21.651+0000] {processor.py:161} INFO - Started process (PID=777) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:21.652+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:17:21.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:21.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:21.676+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:21.691+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:21.691+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:17:21.702+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:21.702+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:17:21.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T04:17:51.913+0000] {processor.py:161} INFO - Started process (PID=780) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:51.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:17:51.915+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:51.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:51.927+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:17:51.947+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:51.947+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:17:51.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:17:51.956+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:17:51.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T04:18:22.166+0000] {processor.py:161} INFO - Started process (PID=783) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:22.167+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:18:22.167+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:22.167+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:22.181+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:22.204+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:22.204+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:18:22.213+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:22.213+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:18:22.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T04:18:52.437+0000] {processor.py:161} INFO - Started process (PID=786) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:52.437+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:18:52.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:52.438+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:52.450+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:18:52.473+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:52.473+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:18:52.484+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:18:52.484+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:18:52.501+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T04:19:22.682+0000] {processor.py:161} INFO - Started process (PID=789) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:22.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:19:22.684+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:22.683+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:22.702+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:22.727+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:22.727+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:19:22.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:22.737+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:19:22.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T04:19:52.966+0000] {processor.py:161} INFO - Started process (PID=792) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:52.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:19:52.968+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:52.967+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:52.979+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:19:53.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:53.000+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:19:53.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:19:53.010+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:19:53.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T04:20:23.247+0000] {processor.py:161} INFO - Started process (PID=795) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:23.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:20:23.248+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:23.248+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:23.260+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:23.280+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:23.280+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:20:23.289+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:23.289+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:20:23.303+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T04:20:53.550+0000] {processor.py:161} INFO - Started process (PID=798) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:53.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:20:53.551+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:53.551+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:53.562+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:20:53.582+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:53.582+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:20:53.590+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:20:53.590+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:20:53.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T04:21:23.809+0000] {processor.py:161} INFO - Started process (PID=801) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:23.810+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:21:23.811+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:23.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:23.823+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:23.845+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:23.844+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:21:23.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:23.853+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:21:23.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T04:21:54.074+0000] {processor.py:161} INFO - Started process (PID=804) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:54.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:21:54.075+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:54.075+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:54.087+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:21:54.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:54.107+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:21:54.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:21:54.115+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:21:54.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T04:22:24.338+0000] {processor.py:161} INFO - Started process (PID=807) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:24.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:22:24.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:24.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:24.355+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:24.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:24.382+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:22:24.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:24.392+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:22:24.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T04:22:54.631+0000] {processor.py:161} INFO - Started process (PID=810) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:54.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:22:54.637+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:54.636+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:54.672+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:22:54.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:54.756+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:22:54.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:22:54.778+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:22:54.826+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.210 seconds
[2025-09-09T04:23:24.886+0000] {processor.py:161} INFO - Started process (PID=813) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:24.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:23:24.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:23:24.888+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:24.899+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:24.919+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:23:24.919+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T04:23:24.928+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:23:24.927+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T04:23:24.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T04:23:51.091+0000] {processor.py:161} INFO - Started process (PID=816) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:51.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:23:51.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:23:51.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:51.111+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:23:51.104+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark_jobs.transform.silver_topcv_transform import run_silver_topcv_transform
ModuleNotFoundError: No module named 'spark_jobs'
[2025-09-09T04:23:51.112+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:23:51.133+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.047 seconds
[2025-09-09T04:24:21.314+0000] {processor.py:161} INFO - Started process (PID=819) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:21.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:24:21.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:24:21.316+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:21.327+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:24:21.324+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark_jobs.transform.silver_topcv_transform import run_silver_topcv_transform
ModuleNotFoundError: No module named 'spark_jobs'
[2025-09-09T04:24:21.328+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:21.349+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.041 seconds
[2025-09-09T04:24:51.497+0000] {processor.py:161} INFO - Started process (PID=822) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:51.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:24:51.498+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:24:51.498+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:51.505+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:24:51.503+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark_jobs.transform.silver_topcv_transform import run_silver_topcv_transform
ModuleNotFoundError: No module named 'spark_jobs'
[2025-09-09T04:24:51.506+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:24:51.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.033 seconds
[2025-09-09T04:25:21.723+0000] {processor.py:161} INFO - Started process (PID=825) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:21.724+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:25:21.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:25:21.725+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:21.734+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:25:21.732+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark_jobs.transform.silver_topcv_transform import run_silver_topcv_transform
ModuleNotFoundError: No module named 'spark_jobs'
[2025-09-09T04:25:21.734+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:21.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.034 seconds
[2025-09-09T04:25:52.053+0000] {processor.py:161} INFO - Started process (PID=828) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:52.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:25:52.055+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:25:52.054+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:59.447+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:25:59.396+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/topcv/topcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:25:59.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:25:59.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 7.498 seconds
[2025-09-09T04:26:04.417+0000] {processor.py:161} INFO - Started process (PID=957) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:04.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:26:04.418+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:26:04.418+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:12.946+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:26:12.896+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/topcv/topcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:26:12.996+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:13.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 8.688 seconds
[2025-09-09T04:26:43.361+0000] {processor.py:161} INFO - Started process (PID=1089) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:43.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:26:43.364+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:26:43.364+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:49.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:26:49.882+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 8, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/topcv/topcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:26:49.984+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:26:50.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 6.655 seconds
[2025-09-09T04:27:13.230+0000] {processor.py:161} INFO - Started process (PID=1216) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:13.231+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:27:13.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:27:13.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:22.103+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:27:22.047+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/topcv/topcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:27:22.153+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:22.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 8.965 seconds
[2025-09-09T04:27:52.381+0000] {processor.py:161} INFO - Started process (PID=1349) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:52.381+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:27:52.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:27:52.382+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:58.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:27:58.166+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/topcv/topcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:27:58.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:27:58.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.942 seconds
[2025-09-09T04:28:28.641+0000] {processor.py:161} INFO - Started process (PID=1477) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:28:28.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:28:28.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:28:28.643+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:28:34.231+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:28:34.177+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 38, in <module>
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o29.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:28:34.280+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:28:34.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.664 seconds
[2025-09-09T04:29:04.473+0000] {processor.py:161} INFO - Started process (PID=1605) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:04.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:29:04.475+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:29:04.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:04.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:29:04.552+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:29:04.558+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:04.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.130 seconds
[2025-09-09T04:29:34.789+0000] {processor.py:161} INFO - Started process (PID=1608) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:34.789+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:29:34.790+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:29:34.790+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:34.870+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:29:34.865+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:29:34.872+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:29:34.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T04:30:05.135+0000] {processor.py:161} INFO - Started process (PID=1611) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:05.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:30:05.137+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:05.136+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:05.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:05.204+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:30:05.210+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:05.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T04:30:35.391+0000] {processor.py:161} INFO - Started process (PID=1614) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:35.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:30:35.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:35.392+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:35.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:35.465+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:30:35.470+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:35.494+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T04:30:40.552+0000] {processor.py:161} INFO - Started process (PID=1615) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:40.553+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:30:40.554+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:40.554+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:40.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:40.630+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:30:40.634+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:40.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.102 seconds
[2025-09-09T04:30:43.701+0000] {processor.py:161} INFO - Started process (PID=1616) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:43.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:30:43.702+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:43.702+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:43.785+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:30:43.781+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 5, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:30:43.786+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:30:43.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T04:31:06.941+0000] {processor.py:161} INFO - Started process (PID=1619) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:06.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:31:06.943+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:06.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:12.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:12.841+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 11, in <module>
    from spark.src.elt.transform import silver_vietnamworkcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_vietnamworkcv_transform.py", line 35, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/vietnamworkcv/vietnamworkcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o28.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:31:12.941+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:12.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 6.024 seconds
[2025-09-09T04:31:43.146+0000] {processor.py:161} INFO - Started process (PID=1746) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:43.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:31:43.148+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:43.147+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:48.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:48.379+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 11, in <module>
    from spark.src.elt.transform import silver_vietnamworkcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_vietnamworkcv_transform.py", line 35, in <module>
    df = spark.read.parquet(f"s3a://{MINIO_BUCKET}/bronze/vietnamworkcv/vietnamworkcv_jobs.parquet")
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/sql/readwriter.py", line 642, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/java_gateway.py", line 1362, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py", line 282, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/py4j/protocol.py", line 327, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o28.parquet.
: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
	at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
	at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
	at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
	at java.base/java.lang.Thread.run(Thread.java:840)
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2737)
		at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
		at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
		at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
		at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
		at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
		at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
		at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
		at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
		at scala.collection.immutable.List.map(List.scala:247)
		at scala.collection.immutable.List.map(List.scala:79)
		at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
		at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
		at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
		at scala.Option.getOrElse(Option.scala:201)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
		at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
		at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
		at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
		at scala.collection.immutable.List.foldLeft(List.scala:79)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
		at scala.collection.immutable.List.foreach(List.scala:334)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
		at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
		at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
		at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
		at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
		... 23 more
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2641)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2735)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3569)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3612)
	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:777)
	at scala.collection.immutable.List.map(List.scala:247)
	at scala.collection.immutable.List.map(List.scala:79)
	at org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:775)
	at org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:575)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:419)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
	at scala.Option.getOrElse(Option.scala:201)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
	at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
	at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
	at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
	at scala.collection.immutable.List.foldLeft(List.scala:79)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
	at scala.collection.immutable.List.foreach(List.scala:334)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
	at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
	at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
	at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
	... 23 more
[2025-09-09T04:31:48.479+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:48.502+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.361 seconds
[2025-09-09T04:31:54.728+0000] {processor.py:161} INFO - Started process (PID=1874) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:54.729+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:31:54.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:54.729+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:54.809+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:54.805+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:31:54.811+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:54.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T04:31:56.864+0000] {processor.py:161} INFO - Started process (PID=1875) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:56.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:31:56.865+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:56.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:56.952+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:31:56.947+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:31:56.953+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:31:56.968+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T04:32:18.006+0000] {processor.py:161} INFO - Started process (PID=1876) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:18.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:32:18.008+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:18.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:18.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:18.103+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:32:18.109+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:18.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.126 seconds
[2025-09-09T04:32:19.081+0000] {processor.py:161} INFO - Started process (PID=1877) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:19.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:32:19.082+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:19.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:19.165+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:19.161+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:32:19.166+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:19.181+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.106 seconds
[2025-09-09T04:32:49.355+0000] {processor.py:161} INFO - Started process (PID=1880) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:49.356+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T04:32:49.357+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:49.356+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:49.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T04:32:49.435+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T04:32:49.440+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T04:32:49.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T06:47:15.356+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:15.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:47:15.362+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:47:15.362+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:15.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:47:15.575+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:47:15.582+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:15.610+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.261 seconds
[2025-09-09T06:47:45.698+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:45.699+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:47:45.702+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:47:45.701+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:45.913+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:47:45.909+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:47:45.914+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:47:45.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.241 seconds
[2025-09-09T06:48:16.143+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:16.144+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:48:16.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:48:16.145+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:16.343+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:48:16.338+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:48:16.344+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:16.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.223 seconds
[2025-09-09T06:48:46.560+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:46.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:48:46.562+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:48:46.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:46.740+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:48:46.737+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:48:46.741+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:48:46.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.203 seconds
[2025-09-09T06:49:17.018+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:17.019+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:49:17.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:49:17.020+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:17.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:49:17.216+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:49:17.222+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:17.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.222 seconds
[2025-09-09T06:49:47.462+0000] {processor.py:161} INFO - Started process (PID=36) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:47.463+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:49:47.465+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:49:47.465+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:47.642+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:49:47.637+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:49:47.643+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:49:47.658+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.200 seconds
[2025-09-09T06:50:17.864+0000] {processor.py:161} INFO - Started process (PID=39) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:17.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:50:17.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:50:17.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:18.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:50:18.035+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:50:18.040+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:18.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.194 seconds
[2025-09-09T06:50:48.344+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:48.345+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:50:48.348+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:50:48.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:48.861+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:50:48.853+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:50:48.866+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:50:48.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.571 seconds
[2025-09-09T06:51:19.009+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:19.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:51:19.012+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:51:19.012+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:19.238+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:51:19.232+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:51:19.240+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:19.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.252 seconds
[2025-09-09T06:51:49.428+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:49.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:51:49.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:51:49.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:49.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:51:49.671+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:51:49.676+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:51:49.691+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.269 seconds
[2025-09-09T06:52:19.867+0000] {processor.py:161} INFO - Started process (PID=51) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:19.867+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:52:19.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:19.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:20.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:20.052+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:52:20.057+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:20.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.208 seconds
[2025-09-09T06:52:50.256+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:50.257+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:52:50.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:50.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:50.458+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:50.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:52:50.459+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:50.479+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.227 seconds
[2025-09-09T06:52:51.282+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:51.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:52:51.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:51.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:51.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:51.297+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:52:51.301+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:51.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.041 seconds
[2025-09-09T06:52:52.300+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:52.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:52:52.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:52.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:52.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:52:52.310+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:52:52.314+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:52:52.336+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.042 seconds
[2025-09-09T06:53:22.523+0000] {processor.py:161} INFO - Started process (PID=59) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:22.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:53:22.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:53:22.526+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:22.538+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:53:22.536+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:53:22.539+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:22.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.042 seconds
[2025-09-09T06:53:52.718+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:52.719+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:53:52.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:53:52.720+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:52.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:53:52.727+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:53:52.730+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:53:52.744+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.031 seconds
[2025-09-09T06:54:22.915+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:22.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:54:22.919+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:54:22.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:22.929+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:54:22.927+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:54:22.930+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:22.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.045 seconds
[2025-09-09T06:54:53.123+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:53.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:54:53.128+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:54:53.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:53.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:54:53.138+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:54:53.140+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:54:53.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.045 seconds
[2025-09-09T06:55:23.347+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:23.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:55:23.352+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:55:23.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:23.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:55:23.364+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:55:23.367+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:23.401+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T06:55:53.558+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:53.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:55:53.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:55:53.560+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:53.567+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:55:53.566+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:55:53.568+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:55:53.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.031 seconds
[2025-09-09T06:56:23.732+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:56:23.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:56:23.736+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:56:23.735+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:56:23.746+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:56:23.743+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:56:23.747+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:56:23.765+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.044 seconds
[2025-09-09T06:57:18.868+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:18.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:57:18.873+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:57:18.873+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:18.883+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:57:18.880+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:57:18.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:18.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T06:57:49.162+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:49.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:57:49.164+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:57:49.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:49.171+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:57:49.169+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:57:49.171+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:57:49.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.034 seconds
[2025-09-09T06:58:19.439+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:19.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:58:19.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:58:19.441+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:19.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:58:19.449+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:58:19.452+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:19.469+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.034 seconds
[2025-09-09T06:58:49.669+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:49.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:58:49.671+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:58:49.671+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:49.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:58:49.677+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:58:49.680+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:58:49.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.033 seconds
[2025-09-09T06:59:16.842+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:16.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:59:16.844+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:16.844+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:16.864+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:16.861+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 10, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:59:16.866+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:16.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.054 seconds
[2025-09-09T06:59:20.944+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:20.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:59:20.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:20.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:21.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:20.997+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:59:21.003+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:21.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.107 seconds
[2025-09-09T06:59:51.242+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:51.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T06:59:51.250+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:51.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:51.270+0000] {logging_mixin.py:188} INFO - [2025-09-09T06:59:51.267+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T06:59:51.271+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T06:59:51.300+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T07:00:21.454+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:21.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:00:21.457+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:00:21.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:21.464+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:00:21.463+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:00:21.465+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:21.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.038 seconds
[2025-09-09T07:00:51.647+0000] {processor.py:161} INFO - Started process (PID=43) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:51.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:00:51.650+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:00:51.649+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:51.664+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:00:51.661+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:00:51.665+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:00:51.693+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.054 seconds
[2025-09-09T07:01:21.866+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:21.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:01:21.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:01:21.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:21.881+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:01:21.879+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:01:21.882+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:21.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T07:01:52.060+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:52.061+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:01:52.062+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:01:52.062+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:52.074+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:01:52.071+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:01:52.074+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:01:52.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.046 seconds
[2025-09-09T07:02:22.259+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:22.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:02:22.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:22.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:22.271+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:22.269+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:02:22.272+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:22.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.040 seconds
[2025-09-09T07:02:50.188+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:50.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:02:50.191+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:50.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:50.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:50.205+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 9, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:02:50.209+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:50.226+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.047 seconds
[2025-09-09T07:02:52.221+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:52.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:02:52.223+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:52.223+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:52.235+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:52.233+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:02:52.235+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:52.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.035 seconds
[2025-09-09T07:02:53.284+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:53.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:02:53.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:53.286+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:53.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:53.298+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:02:53.300+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:53.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.046 seconds
[2025-09-09T07:02:59.356+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:59.357+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:02:59.360+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:59.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:59.381+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:02:59.378+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:02:59.381+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:02:59.401+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.052 seconds
[2025-09-09T07:03:29.567+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:29.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:03:29.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:29.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:29.580+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:29.577+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:03:29.580+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:29.601+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.041 seconds
[2025-09-09T07:03:34.608+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:34.608+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:03:34.610+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:34.609+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:34.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:34.865+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:03:34.870+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:34.884+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.281 seconds
[2025-09-09T07:03:43.968+0000] {processor.py:161} INFO - Started process (PID=65) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:43.969+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:03:43.971+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:43.970+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:44.182+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:44.178+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:03:44.184+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:44.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.236 seconds
[2025-09-09T07:03:44.986+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:44.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:03:44.988+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:44.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:45.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:03:45.279+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:03:45.287+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:03:45.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.329 seconds
[2025-09-09T07:04:15.474+0000] {processor.py:161} INFO - Started process (PID=69) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:15.475+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:04:15.477+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:04:15.477+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:15.573+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:04:15.567+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:04:15.574+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:15.597+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.131 seconds
[2025-09-09T07:04:45.673+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:45.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:04:45.676+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:04:45.675+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:45.777+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:04:45.773+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:04:45.779+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:04:45.799+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.133 seconds
[2025-09-09T07:05:16.004+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:16.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:05:16.008+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:05:16.008+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:16.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:05:16.097+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:05:16.103+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:16.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.130 seconds
[2025-09-09T07:05:46.323+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:46.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:05:46.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:05:46.325+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:46.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:05:46.436+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:05:46.443+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:05:46.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.150 seconds
[2025-09-09T07:06:16.648+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:16.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:06:16.654+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:06:16.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:16.736+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:06:16.732+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:06:16.737+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:16.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.128 seconds
[2025-09-09T07:06:46.952+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:46.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:06:46.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:06:46.954+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:47.032+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:06:47.028+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:06:47.033+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:06:47.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.106 seconds
[2025-09-09T07:07:17.242+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:17.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:07:17.249+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:07:17.248+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:17.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:07:17.494+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:07:17.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:17.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.337 seconds
[2025-09-09T07:07:47.768+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:47.769+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:07:47.771+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:07:47.770+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:47.852+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:07:47.847+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:07:47.854+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:07:47.873+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T07:08:18.030+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:18.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:08:18.033+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:08:18.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:18.117+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:08:18.113+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:08:18.118+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:18.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T07:08:48.318+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:48.319+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:08:48.321+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:08:48.321+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:48.408+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:08:48.403+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:08:48.409+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:08:48.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T07:09:18.560+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:18.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:09:18.564+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:09:18.563+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:18.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:09:18.661+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:09:18.666+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:18.686+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.136 seconds
[2025-09-09T07:09:48.980+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:48.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:09:48.982+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:09:48.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:49.078+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:09:49.073+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:09:49.080+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:09:49.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.133 seconds
[2025-09-09T07:10:19.346+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:19.346+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:10:19.348+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:10:19.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:19.431+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:10:19.427+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:10:19.433+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:19.451+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T07:10:49.652+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:49.654+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:10:49.661+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:10:49.661+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:49.798+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:10:49.794+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:10:49.799+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:10:49.817+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.178 seconds
[2025-09-09T07:11:20.035+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:20.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:11:20.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:11:20.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:20.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:11:20.176+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:11:20.182+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:20.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.173 seconds
[2025-09-09T07:11:50.420+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:50.421+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:11:50.423+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:11:50.423+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:50.512+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:11:50.508+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 7, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:11:50.514+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:11:50.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.132 seconds
[2025-09-09T07:12:17.098+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:17.099+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:12:17.104+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:17.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:17.120+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:17.117+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:12:17.121+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:17.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T07:12:18.100+0000] {processor.py:161} INFO - Started process (PID=116) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:18.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:12:18.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:18.102+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:18.116+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:18.114+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:12:18.116+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:18.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.040 seconds
[2025-09-09T07:12:48.305+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:48.306+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:12:48.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:48.308+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:48.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:12:48.315+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:12:48.318+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:12:48.340+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.044 seconds
[2025-09-09T07:13:18.515+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:13:18.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:13:18.517+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:13:18.517+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:13:18.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:13:18.523+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:13:18.526+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:13:18.542+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.033 seconds
[2025-09-09T07:14:08.018+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:08.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:14:08.026+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:14:08.026+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:10.220+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:14:10.208+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:14:10.224+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:10.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.259 seconds
[2025-09-09T07:14:40.325+0000] {processor.py:161} INFO - Started process (PID=25) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:40.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:14:40.327+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:14:40.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:40.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:14:40.831+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:14:40.839+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:14:40.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.536 seconds
[2025-09-09T07:15:11.523+0000] {processor.py:161} INFO - Started process (PID=28) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:11.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:15:11.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:15:11.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:12.046+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:15:12.037+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:15:12.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:12.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.547 seconds
[2025-09-09T07:15:42.811+0000] {processor.py:161} INFO - Started process (PID=31) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:42.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:15:42.813+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:15:42.813+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:43.302+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:15:43.298+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:15:43.304+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:15:43.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.512 seconds
[2025-09-09T07:16:13.997+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:13.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:16:13.999+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:16:13.998+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:14.462+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:16:14.457+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:16:14.464+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:14.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.486 seconds
[2025-09-09T07:16:45.246+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:45.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:16:45.247+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:16:45.247+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:45.715+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:16:45.710+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:16:45.717+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:16:45.730+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.489 seconds
[2025-09-09T07:17:16.559+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:16.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:17:16.564+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:17:16.564+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:18.715+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:17:18.676+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
  File "/opt/spark-jobs/elt/load/upload_jobs_to_minio.py", line 5, in <module>
    from spark.src.elt.etract.jobs_topcv import extract_topcv_jobs
ModuleNotFoundError: No module named 'spark'
[2025-09-09T07:17:18.725+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:18.837+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.290 seconds
[2025-09-09T07:17:48.056+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:48.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:17:48.061+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:17:48.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:48.072+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:17:48.070+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:17:48.073+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:17:48.119+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T07:18:18.326+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:18.329+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:18:18.338+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:18:18.337+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:18.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:18:18.356+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:18:18.363+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:18.420+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.125 seconds
[2025-09-09T07:18:48.643+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:48.644+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:18:48.646+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:18:48.645+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:48.655+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:18:48.653+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:18:48.656+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:18:48.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.043 seconds
[2025-09-09T07:19:18.877+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:18.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:19:18.880+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:18.879+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:18.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:18.887+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:19:18.889+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:18.908+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.035 seconds
[2025-09-09T07:19:33.020+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:33.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:19:33.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:33.021+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:33.297+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:33.292+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:19:33.299+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:33.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.297 seconds
[2025-09-09T07:19:34.036+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:34.037+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:19:34.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:34.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:34.258+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:19:34.254+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:19:34.259+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:19:34.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.242 seconds
[2025-09-09T07:20:04.451+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:04.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:20:04.453+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:04.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:04.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:04.659+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:20:04.666+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:04.681+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.234 seconds
[2025-09-09T07:20:34.838+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:34.838+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:20:34.840+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:34.839+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:35.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:35.023+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:20:35.029+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:35.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.211 seconds
[2025-09-09T07:20:50.154+0000] {processor.py:161} INFO - Started process (PID=41) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:50.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:20:50.157+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:50.156+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:50.353+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:50.349+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:20:50.354+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:50.369+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.220 seconds
[2025-09-09T07:20:51.174+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:51.176+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:20:51.178+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:51.178+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:51.401+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:20:51.396+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:20:51.402+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:20:51.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.249 seconds
[2025-09-09T07:21:07.582+0000] {processor.py:161} INFO - Started process (PID=45) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:07.583+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:21:07.585+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:07.584+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:07.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:07.794+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:21:07.799+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:07.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.249 seconds
[2025-09-09T07:21:08.594+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:08.595+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:21:08.596+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:08.596+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:08.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:08.788+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:21:08.792+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:08.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.221 seconds
[2025-09-09T07:21:39.009+0000] {processor.py:161} INFO - Started process (PID=49) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:39.010+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:21:39.011+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:39.011+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:39.210+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:21:39.206+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:21:39.212+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:21:39.229+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.225 seconds
[2025-09-09T07:22:09.480+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:09.481+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:22:09.483+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:22:09.482+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:09.658+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:22:09.654+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:22:09.660+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:09.675+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.203 seconds
[2025-09-09T07:22:39.938+0000] {processor.py:161} INFO - Started process (PID=55) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:39.939+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:22:39.942+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:22:39.941+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:40.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:22:40.121+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:22:40.127+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:22:40.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.212 seconds
[2025-09-09T07:23:10.372+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:10.373+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:23:10.375+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:23:10.375+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:10.595+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:23:10.590+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:23:10.597+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:10.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.250 seconds
[2025-09-09T07:23:40.870+0000] {processor.py:161} INFO - Started process (PID=61) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:40.871+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:23:40.872+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:23:40.872+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:41.047+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:23:41.043+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:23:41.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:23:41.064+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.199 seconds
[2025-09-09T07:24:11.301+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:11.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:24:11.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:24:11.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:11.505+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:24:11.502+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:24:11.507+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:11.522+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.226 seconds
[2025-09-09T07:24:41.715+0000] {processor.py:161} INFO - Started process (PID=67) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:41.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:24:41.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:24:41.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:41.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:24:41.885+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:24:41.891+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:24:41.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.197 seconds
[2025-09-09T07:25:12.214+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:12.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:25:12.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:25:12.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:12.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:25:12.287+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:25:12.292+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:12.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.099 seconds
[2025-09-09T07:25:42.486+0000] {processor.py:161} INFO - Started process (PID=73) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:42.487+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:25:42.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:25:42.488+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:42.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:25:42.556+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:25:42.561+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:25:42.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.094 seconds
[2025-09-09T07:26:12.816+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:12.818+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:26:12.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:26:12.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:12.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:26:12.888+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:26:12.892+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:12.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T07:26:43.179+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:43.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:26:43.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:26:43.181+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:43.261+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:26:43.257+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:26:43.262+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:26:43.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T07:27:13.472+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:13.473+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:27:13.476+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:27:13.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:13.580+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:27:13.576+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:27:13.582+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:13.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.142 seconds
[2025-09-09T07:27:43.798+0000] {processor.py:161} INFO - Started process (PID=103) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:43.799+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:27:43.800+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:27:43.800+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:43.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:27:43.878+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:27:43.883+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:27:43.897+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.104 seconds
[2025-09-09T07:28:02.957+0000] {processor.py:161} INFO - Started process (PID=104) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:02.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:28:02.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:02.959+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:03.059+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:03.054+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:28:03.061+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:03.079+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.128 seconds
[2025-09-09T07:28:04.016+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:04.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:28:04.018+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:04.018+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:04.109+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:04.105+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:28:04.110+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:04.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.115 seconds
[2025-09-09T07:28:34.334+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:34.335+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:28:34.337+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:34.336+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:34.420+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:28:34.417+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:28:34.421+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:28:34.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.108 seconds
[2025-09-09T07:29:04.641+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:04.642+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:29:04.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:29:04.643+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:04.733+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:29:04.729+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:29:04.734+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:04.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.117 seconds
[2025-09-09T07:29:34.927+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:34.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:29:34.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:29:34.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:35.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:29:35.016+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:29:35.022+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:29:35.041+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.127 seconds
[2025-09-09T07:30:05.227+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:05.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:30:05.230+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:05.230+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:05.315+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:05.312+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:30:05.317+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:05.334+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.113 seconds
[2025-09-09T07:30:35.527+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:35.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:30:35.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:35.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:35.615+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:35.611+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from spark.src.elt.transform.silver_topcv_transform import silver_topcv_transform
  File "/opt/airflow/spark/src/elt/transform/silver_topcv_transform.py", line 8, in <module>
    from elt.load.upload_jobs_to_minio import upload_jobs_cleaned_to_minio
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:30:35.616+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:35.635+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.113 seconds
[2025-09-09T07:30:37.550+0000] {processor.py:161} INFO - Started process (PID=121) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:37.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:30:37.552+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:37.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:37.565+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:30:37.563+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 6, in <module>
    from elt.transform.silver_topcv_transform import silver_topcv_transform
ModuleNotFoundError: No module named 'elt'
[2025-09-09T07:30:37.566+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:30:37.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.038 seconds
[2025-09-09T07:31:20.217+0000] {processor.py:161} INFO - Started process (PID=20) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:20.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:31:20.223+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:20.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:23.193+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:23.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:23.350+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:31:23.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:23.366+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:31:23.409+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.204 seconds
[2025-09-09T07:31:53.649+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:53.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:31:53.656+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:53.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:55.408+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:31:55.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:55.430+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:31:55.439+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:31:55.438+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:31:55.462+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.829 seconds
[2025-09-09T07:32:25.692+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:25.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:32:25.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:25.694+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:26.340+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:26.359+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:26.359+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:32:26.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:26.366+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:32:26.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.694 seconds
[2025-09-09T07:32:56.528+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:56.528+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:32:56.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:56.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:57.214+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:32:57.231+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:57.231+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:32:57.237+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:32:57.237+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:32:57.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.728 seconds
[2025-09-09T07:33:27.419+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:27.419+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:33:27.421+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:27.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:27.994+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:28.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:28.010+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:33:28.018+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:28.018+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:33:28.031+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.618 seconds
[2025-09-09T07:33:58.107+0000] {processor.py:161} INFO - Started process (PID=192) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:58.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:33:58.109+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:58.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:58.994+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:33:59.018+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:59.018+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:33:59.028+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:33:59.028+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:33:59.043+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.943 seconds
[2025-09-09T07:34:29.213+0000] {processor.py:161} INFO - Started process (PID=198) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:34:29.214+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:34:29.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:34:29.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:34:29.919+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:34:29.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:34:29.936+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:34:29.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:34:29.945+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:34:29.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.753 seconds
[2025-09-09T07:35:00.178+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:00.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:35:00.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:00.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:00.803+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:00.827+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:00.826+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:35:00.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:00.835+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:35:00.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.677 seconds
[2025-09-09T07:35:31.039+0000] {processor.py:161} INFO - Started process (PID=206) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:31.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:35:31.042+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:31.042+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:31.636+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:35:31.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:31.652+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:35:31.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:35:31.660+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:35:31.673+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.640 seconds
[2025-09-09T07:36:01.851+0000] {processor.py:161} INFO - Started process (PID=210) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:01.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:36:01.854+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:01.853+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:02.471+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:02.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:02.485+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:36:02.492+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:02.492+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:36:02.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.659 seconds
[2025-09-09T07:36:32.716+0000] {processor.py:161} INFO - Started process (PID=214) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:32.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:36:32.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:32.718+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:33.296+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:36:33.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:33.310+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:36:33.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:36:33.317+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:36:33.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.620 seconds
[2025-09-09T07:37:03.540+0000] {processor.py:161} INFO - Started process (PID=218) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:03.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:37:03.542+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:03.542+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:04.118+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:04.133+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:04.133+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:37:04.141+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:04.141+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:37:04.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.622 seconds
[2025-09-09T07:37:34.392+0000] {processor.py:161} INFO - Started process (PID=222) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:34.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:37:34.394+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:34.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:34.953+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:37:34.971+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:34.971+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:37:34.979+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:37:34.979+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:37:34.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.606 seconds
[2025-09-09T07:38:05.224+0000] {processor.py:161} INFO - Started process (PID=226) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:05.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:38:05.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:05.226+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:05.829+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:05.847+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:05.846+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:38:05.854+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:05.854+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:38:05.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.648 seconds
[2025-09-09T07:38:36.118+0000] {processor.py:161} INFO - Started process (PID=230) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:36.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:38:36.121+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:36.120+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:36.665+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:38:36.680+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:36.680+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:38:36.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:38:36.686+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:38:36.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.588 seconds
[2025-09-09T07:39:06.968+0000] {processor.py:161} INFO - Started process (PID=234) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:06.969+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:39:06.971+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:06.971+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:07.497+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:07.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:07.514+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:39:07.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:07.520+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:39:07.534+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.572 seconds
[2025-09-09T07:39:37.747+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:37.748+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:39:37.750+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:37.749+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:38.284+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:39:38.299+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:38.299+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:39:38.307+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:39:38.307+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:39:38.318+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.580 seconds
[2025-09-09T07:40:08.492+0000] {processor.py:161} INFO - Started process (PID=242) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:08.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:40:08.495+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:08.494+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:09.173+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:09.196+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:09.195+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:40:09.204+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:09.204+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:40:09.217+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.732 seconds
[2025-09-09T07:40:39.447+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:39.448+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:40:39.449+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:39.449+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:39.936+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:40:39.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:39.953+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:40:39.961+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:40:39.960+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:40:39.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.534 seconds
[2025-09-09T07:41:10.179+0000] {processor.py:161} INFO - Started process (PID=250) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:10.180+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:41:10.182+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:10.182+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:10.716+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:10.734+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:10.733+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:41:10.741+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:10.741+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:41:10.755+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.584 seconds
[2025-09-09T07:41:40.918+0000] {processor.py:161} INFO - Started process (PID=254) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:40.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:41:40.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:40.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:41.377+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:41:41.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:41.393+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:41:41.402+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:41:41.401+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:41:41.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.500 seconds
[2025-09-09T07:42:11.675+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:11.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:42:11.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:11.678+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:12.266+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:12.281+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:12.281+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:42:12.287+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:12.287+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:42:12.299+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.632 seconds
[2025-09-09T07:42:42.620+0000] {processor.py:161} INFO - Started process (PID=260) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:42.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:42:42.622+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:42.622+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:43.205+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:42:43.225+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:43.224+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:42:43.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:42:43.233+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:42:43.247+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.633 seconds
[2025-09-09T07:43:13.419+0000] {processor.py:161} INFO - Started process (PID=264) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:13.420+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:43:13.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:13.421+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:14.268+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:14.298+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:14.298+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:43:14.310+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:14.310+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:43:14.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.920 seconds
[2025-09-09T07:43:44.460+0000] {processor.py:161} INFO - Started process (PID=268) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:44.461+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:43:44.462+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:44.462+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:45.065+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:43:45.087+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:45.086+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:43:45.095+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:43:45.094+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:43:45.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.655 seconds
[2025-09-09T07:44:15.306+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:15.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:44:15.309+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:15.309+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:15.874+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:15.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:15.891+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:44:15.898+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:15.898+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:44:15.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.609 seconds
[2025-09-09T07:44:46.069+0000] {processor.py:161} INFO - Started process (PID=276) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:46.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:44:46.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:46.071+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:46.763+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:44:46.780+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:46.780+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:44:46.788+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:44:46.787+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:44:46.800+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.740 seconds
[2025-09-09T07:45:16.991+0000] {processor.py:161} INFO - Started process (PID=280) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:16.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:45:16.993+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:16.993+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:17.643+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:17.659+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:17.658+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:45:17.666+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:17.666+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:45:17.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.696 seconds
[2025-09-09T07:45:47.833+0000] {processor.py:161} INFO - Started process (PID=284) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:47.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:45:47.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:47.835+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:48.381+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:45:48.397+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:48.397+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:45:48.405+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:45:48.404+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:45:48.416+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.589 seconds
[2025-09-09T07:46:18.628+0000] {processor.py:161} INFO - Started process (PID=288) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:18.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:46:18.631+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:18.631+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:19.208+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:19.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:19.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:46:19.238+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:19.238+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:46:19.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.635 seconds
[2025-09-09T07:46:49.449+0000] {processor.py:161} INFO - Started process (PID=292) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:49.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:46:49.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:49.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:50.037+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:46:50.052+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:50.052+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:46:50.059+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:46:50.059+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:46:50.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.627 seconds
[2025-09-09T07:47:20.200+0000] {processor.py:161} INFO - Started process (PID=296) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:20.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:47:20.203+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:20.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:20.806+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:20.823+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:20.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:47:20.831+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:20.831+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:47:20.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.650 seconds
[2025-09-09T07:47:50.994+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:50.996+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:47:50.999+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:50.999+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:51.909+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:47:51.930+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:51.929+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:47:51.938+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:47:51.938+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:47:51.954+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.970 seconds
[2025-09-09T07:48:22.337+0000] {processor.py:161} INFO - Started process (PID=304) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:22.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:48:22.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:22.345+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:23.462+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:23.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:23.487+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:48:23.498+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:23.498+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:48:23.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.209 seconds
[2025-09-09T07:48:53.733+0000] {processor.py:161} INFO - Started process (PID=308) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:53.735+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:48:53.739+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:53.738+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:54.350+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:48:54.369+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:54.368+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:48:54.377+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:48:54.377+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:48:54.393+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.667 seconds
[2025-09-09T07:49:24.515+0000] {processor.py:161} INFO - Started process (PID=312) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:24.515+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:49:24.517+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:24.517+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:25.127+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:25.148+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:25.147+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:49:25.155+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:25.155+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:49:25.168+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.660 seconds
[2025-09-09T07:49:55.516+0000] {processor.py:161} INFO - Started process (PID=316) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:55.517+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:49:55.519+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:55.518+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:56.415+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:49:56.435+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:56.435+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:49:56.443+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:49:56.443+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:49:56.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.947 seconds
[2025-09-09T07:50:26.568+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:26.569+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:50:26.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:26.570+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:27.339+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:27.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:27.358+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:50:27.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:27.365+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:50:27.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.817 seconds
[2025-09-09T07:50:57.628+0000] {processor.py:161} INFO - Started process (PID=324) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:57.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:50:57.631+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:57.630+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:58.210+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:50:58.226+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:58.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:50:58.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:50:58.233+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:50:58.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.622 seconds
[2025-09-09T07:51:28.429+0000] {processor.py:161} INFO - Started process (PID=328) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:28.430+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:51:28.432+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:28.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:29.022+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:29.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:29.039+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:51:29.046+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:29.046+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:51:29.058+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.636 seconds
[2025-09-09T07:51:59.225+0000] {processor.py:161} INFO - Started process (PID=332) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:59.225+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:51:59.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:59.226+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:59.860+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:51:59.875+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:59.874+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:51:59.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:51:59.882+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:51:59.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.674 seconds
[2025-09-09T07:52:30.077+0000] {processor.py:161} INFO - Started process (PID=336) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:52:30.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:52:30.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:52:30.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:52:30.688+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:52:30.704+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:52:30.703+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:52:30.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:52:30.711+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:52:30.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.653 seconds
[2025-09-09T07:53:00.908+0000] {processor.py:161} INFO - Started process (PID=340) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:00.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:53:00.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:00.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:01.466+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:01.482+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:01.482+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:53:01.489+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:01.489+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:53:01.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.599 seconds
[2025-09-09T07:53:31.680+0000] {processor.py:161} INFO - Started process (PID=344) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:31.681+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:53:31.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:31.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:32.253+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:53:32.270+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:32.269+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:53:32.277+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:53:32.277+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:53:32.289+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.615 seconds
[2025-09-09T07:54:02.456+0000] {processor.py:161} INFO - Started process (PID=348) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:02.457+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:54:02.458+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:02.458+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:03.045+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:03.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:03.060+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:54:03.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:03.066+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:54:03.078+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.628 seconds
[2025-09-09T07:54:33.194+0000] {processor.py:161} INFO - Started process (PID=352) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:33.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:54:33.197+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:33.196+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:33.788+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:54:33.803+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:33.803+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:54:33.810+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:54:33.810+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:54:33.823+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.633 seconds
[2025-09-09T07:55:04.038+0000] {processor.py:161} INFO - Started process (PID=356) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:04.039+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:55:04.041+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:04.041+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:04.632+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:04.647+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:04.647+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:55:04.655+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:04.655+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:55:04.668+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.635 seconds
[2025-09-09T07:55:34.832+0000] {processor.py:161} INFO - Started process (PID=360) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:34.833+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:55:34.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:34.835+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:35.462+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:55:35.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:35.480+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:55:35.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:55:35.488+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:55:35.504+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.679 seconds
[2025-09-09T07:56:05.687+0000] {processor.py:161} INFO - Started process (PID=364) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:05.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:56:05.691+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:05.691+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:06.287+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:06.302+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:06.302+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:56:06.310+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:06.310+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:56:06.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.642 seconds
[2025-09-09T07:56:36.464+0000] {processor.py:161} INFO - Started process (PID=368) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:36.465+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:56:36.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:36.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:37.103+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:56:37.126+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:37.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:56:37.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:56:37.133+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:56:37.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.697 seconds
[2025-09-09T07:57:07.357+0000] {processor.py:161} INFO - Started process (PID=372) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:07.358+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:57:07.360+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:07.359+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:07.972+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:07.997+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:07.997+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:57:08.008+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:08.008+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:57:08.025+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.677 seconds
[2025-09-09T07:57:38.233+0000] {processor.py:161} INFO - Started process (PID=376) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:38.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:57:38.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:38.236+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:38.794+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:57:38.809+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:38.809+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:57:38.815+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:57:38.815+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:57:38.828+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.601 seconds
[2025-09-09T07:58:09.043+0000] {processor.py:161} INFO - Started process (PID=380) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:09.044+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:58:09.046+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:09.045+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:09.638+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:09.654+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:09.654+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:58:09.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:09.662+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:58:09.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.639 seconds
[2025-09-09T07:58:39.915+0000] {processor.py:161} INFO - Started process (PID=384) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:39.916+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:58:39.920+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:39.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:40.482+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:58:40.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:40.499+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:58:40.505+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:58:40.505+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:58:40.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.611 seconds
[2025-09-09T07:59:10.679+0000] {processor.py:161} INFO - Started process (PID=388) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:10.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:59:10.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:10.682+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:11.268+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:11.289+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:11.288+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:59:11.296+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:11.296+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:59:11.308+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.636 seconds
[2025-09-09T07:59:41.565+0000] {processor.py:161} INFO - Started process (PID=392) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:41.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T07:59:41.567+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:41.567+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:42.130+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T07:59:42.147+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:42.147+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T07:59:42.155+0000] {logging_mixin.py:188} INFO - [2025-09-09T07:59:42.155+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T07:59:42.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.608 seconds
[2025-09-09T08:00:12.325+0000] {processor.py:161} INFO - Started process (PID=396) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:12.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:00:12.328+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:12.327+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:12.892+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:12.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:12.910+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:00:12.918+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:12.918+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:00:12.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.613 seconds
[2025-09-09T08:00:43.146+0000] {processor.py:161} INFO - Started process (PID=400) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:43.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:00:43.149+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:43.149+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:43.690+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:00:43.706+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:43.705+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:00:43.713+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:00:43.713+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:00:43.726+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.587 seconds
[2025-09-09T08:01:13.868+0000] {processor.py:161} INFO - Started process (PID=404) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:13.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:01:13.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:13.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:14.448+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:14.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:14.466+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:01:14.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:14.474+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:01:14.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.625 seconds
[2025-09-09T08:01:44.760+0000] {processor.py:161} INFO - Started process (PID=408) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:44.761+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:01:44.763+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:44.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:45.344+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:01:45.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:45.365+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:01:45.374+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:01:45.374+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:01:45.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.632 seconds
[2025-09-09T08:02:15.617+0000] {processor.py:161} INFO - Started process (PID=412) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:15.618+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:02:15.619+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:15.619+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:16.205+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:16.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:16.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:02:16.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:16.228+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:02:16.239+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.630 seconds
[2025-09-09T08:02:46.523+0000] {processor.py:161} INFO - Started process (PID=416) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:46.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:02:46.526+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:46.525+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:47.077+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:02:47.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:47.092+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:02:47.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:02:47.098+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:02:47.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.592 seconds
[2025-09-09T08:03:17.359+0000] {processor.py:161} INFO - Started process (PID=420) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:17.360+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:03:17.363+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:17.363+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:17.977+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:17.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:17.994+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:03:18.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:18.001+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:03:18.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.665 seconds
[2025-09-09T08:03:48.207+0000] {processor.py:161} INFO - Started process (PID=424) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:48.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:03:48.210+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:48.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:48.784+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:03:48.799+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:48.799+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:03:48.807+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:03:48.807+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:03:48.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.619 seconds
[2025-09-09T08:04:19.024+0000] {processor.py:161} INFO - Started process (PID=428) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:19.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:04:19.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:19.026+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:19.565+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:19.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:19.581+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:04:19.590+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:19.590+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:04:19.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.584 seconds
[2025-09-09T08:04:49.720+0000] {processor.py:161} INFO - Started process (PID=432) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:49.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:04:49.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:49.722+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:50.357+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:04:50.374+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:50.374+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:04:50.381+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:04:50.381+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:04:50.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.678 seconds
[2025-09-09T08:05:20.529+0000] {processor.py:161} INFO - Started process (PID=436) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:20.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:05:20.531+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:20.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:21.121+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:21.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:21.137+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:05:21.144+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:21.144+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:05:21.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.635 seconds
[2025-09-09T08:05:51.278+0000] {processor.py:161} INFO - Started process (PID=440) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:51.278+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:05:51.280+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:51.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:51.854+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:05:51.870+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:51.869+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:05:51.878+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:05:51.878+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:05:51.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.620 seconds
[2025-09-09T08:06:22.046+0000] {processor.py:161} INFO - Started process (PID=444) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:22.046+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:06:22.048+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:22.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:22.656+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:22.678+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:22.677+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:06:22.687+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:22.687+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:06:22.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.664 seconds
[2025-09-09T08:06:52.842+0000] {processor.py:161} INFO - Started process (PID=448) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:52.843+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:06:52.844+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:52.844+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:53.498+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:06:53.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:53.514+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:06:53.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:06:53.521+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:06:53.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.698 seconds
[2025-09-09T08:07:23.632+0000] {processor.py:161} INFO - Started process (PID=452) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:23.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:07:23.634+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:23.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:24.270+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:24.290+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:24.289+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:07:24.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:24.300+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:07:24.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.687 seconds
[2025-09-09T08:07:54.428+0000] {processor.py:161} INFO - Started process (PID=456) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:54.429+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:07:54.431+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:54.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:54.986+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:07:55.003+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:55.003+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:07:55.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:07:55.010+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:07:55.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.600 seconds
[2025-09-09T08:08:25.208+0000] {processor.py:161} INFO - Started process (PID=460) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:25.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:08:25.211+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:25.210+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:25.762+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:25.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:25.778+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:08:25.785+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:25.785+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:08:25.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.593 seconds
[2025-09-09T08:08:55.987+0000] {processor.py:161} INFO - Started process (PID=464) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:55.988+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:08:55.989+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:55.989+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:56.554+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:08:56.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:56.569+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:08:56.576+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:08:56.576+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:08:56.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.607 seconds
[2025-09-09T08:09:26.757+0000] {processor.py:161} INFO - Started process (PID=468) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:26.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:09:26.759+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:26.759+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:27.328+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:27.342+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:27.342+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:09:27.349+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:27.349+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:09:27.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.610 seconds
[2025-09-09T08:09:57.560+0000] {processor.py:161} INFO - Started process (PID=472) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:57.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:09:57.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:57.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:58.236+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:09:58.255+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:58.255+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:09:58.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:09:58.265+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:09:58.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.725 seconds
[2025-09-09T08:10:28.439+0000] {processor.py:161} INFO - Started process (PID=476) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:28.440+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:10:28.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:28.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:29.075+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:29.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:29.091+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:10:29.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:29.099+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:10:29.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.678 seconds
[2025-09-09T08:10:59.282+0000] {processor.py:161} INFO - Started process (PID=480) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:59.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:10:59.285+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:59.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:59.916+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:10:59.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:59.932+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:10:59.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:10:59.939+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:10:59.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.674 seconds
[2025-09-09T08:11:30.111+0000] {processor.py:161} INFO - Started process (PID=484) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:11:30.111+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:11:30.113+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:11:30.112+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:11:30.796+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:11:30.818+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:11:30.817+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:11:30.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:11:30.829+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:11:30.845+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.746 seconds
[2025-09-09T08:12:01.028+0000] {processor.py:161} INFO - Started process (PID=488) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:01.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:12:01.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:01.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:01.592+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:01.608+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:01.607+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:12:01.615+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:01.615+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:12:01.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.608 seconds
[2025-09-09T08:12:31.772+0000] {processor.py:161} INFO - Started process (PID=492) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:31.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:12:31.774+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:31.774+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:32.320+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:12:32.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:32.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:12:32.347+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:12:32.347+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:12:32.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.599 seconds
[2025-09-09T08:13:02.518+0000] {processor.py:161} INFO - Started process (PID=496) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:02.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:13:02.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:02.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:03.113+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:03.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:03.129+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:13:03.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:03.138+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:13:03.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.640 seconds
[2025-09-09T08:13:33.303+0000] {processor.py:161} INFO - Started process (PID=500) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:33.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:13:33.305+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:33.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:33.848+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:13:33.864+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:33.864+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:13:33.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:13:33.871+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:13:33.883+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.586 seconds
[2025-09-09T08:14:04.078+0000] {processor.py:161} INFO - Started process (PID=504) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:04.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:14:04.081+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:04.081+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:04.656+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:04.677+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:04.677+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:14:04.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:04.685+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:14:04.698+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.628 seconds
[2025-09-09T08:14:34.812+0000] {processor.py:161} INFO - Started process (PID=508) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:34.813+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:14:34.815+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:34.814+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:35.451+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:14:35.467+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:35.467+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:14:35.473+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:14:35.473+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:14:35.485+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.680 seconds
[2025-09-09T08:15:05.649+0000] {processor.py:161} INFO - Started process (PID=512) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:05.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:15:05.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:05.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:06.258+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:06.273+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:06.273+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:15:06.280+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:06.280+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:15:06.291+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.648 seconds
[2025-09-09T08:15:36.390+0000] {processor.py:161} INFO - Started process (PID=516) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:36.391+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:15:36.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:36.392+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:37.043+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:15:37.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:37.067+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:15:37.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:15:37.079+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:15:37.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.720 seconds
[2025-09-09T08:16:07.270+0000] {processor.py:161} INFO - Started process (PID=520) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:07.271+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:16:07.273+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:07.273+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:08.009+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:08.050+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:08.049+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:16:08.070+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:08.069+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:16:08.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.844 seconds
[2025-09-09T08:16:38.202+0000] {processor.py:161} INFO - Started process (PID=524) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:38.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:16:38.205+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:38.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:38.843+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:16:38.863+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:38.863+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:16:38.872+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:16:38.872+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:16:38.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.693 seconds
[2025-09-09T08:17:09.027+0000] {processor.py:161} INFO - Started process (PID=528) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:09.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:17:09.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:09.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:09.664+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:09.681+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:09.681+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:17:09.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:09.689+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:17:09.704+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.683 seconds
[2025-09-09T08:17:39.828+0000] {processor.py:161} INFO - Started process (PID=532) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:39.829+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:17:39.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:39.831+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:40.495+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:17:40.513+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:40.513+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:17:40.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:17:40.521+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:17:40.536+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.714 seconds
[2025-09-09T08:18:10.708+0000] {processor.py:161} INFO - Started process (PID=536) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:10.709+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:18:10.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:10.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:11.360+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:11.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:11.375+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:18:11.383+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:11.383+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:18:11.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.692 seconds
[2025-09-09T08:18:41.627+0000] {processor.py:161} INFO - Started process (PID=540) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:41.628+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:18:41.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:41.629+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:42.204+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:18:42.219+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:42.218+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:18:42.226+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:18:42.226+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:18:42.237+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.618 seconds
[2025-09-09T08:19:12.395+0000] {processor.py:161} INFO - Started process (PID=544) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:12.396+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:19:12.397+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:12.397+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:12.990+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:13.005+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:13.005+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:19:13.012+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:13.012+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:19:13.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.638 seconds
[2025-09-09T08:19:43.185+0000] {processor.py:161} INFO - Started process (PID=548) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:43.186+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:19:43.188+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:43.187+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:43.761+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:19:43.780+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:43.779+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:19:43.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:19:43.786+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:19:43.797+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.619 seconds
[2025-09-09T08:20:13.934+0000] {processor.py:161} INFO - Started process (PID=552) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:13.934+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:20:13.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:13.936+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:14.497+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:14.512+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:14.512+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:20:14.519+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:14.519+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:20:14.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.605 seconds
[2025-09-09T08:20:44.693+0000] {processor.py:161} INFO - Started process (PID=556) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:44.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:20:44.695+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:44.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:45.254+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:20:45.272+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:45.272+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:20:45.279+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:20:45.279+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:20:45.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.605 seconds
[2025-09-09T08:21:15.451+0000] {processor.py:161} INFO - Started process (PID=560) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:15.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:21:15.454+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:15.453+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:16.028+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:16.043+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:16.043+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:21:16.050+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:16.049+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:21:16.060+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.617 seconds
[2025-09-09T08:21:46.223+0000] {processor.py:161} INFO - Started process (PID=564) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:46.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:21:46.225+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:46.225+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:46.814+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:21:46.830+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:46.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:21:46.838+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:21:46.838+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:21:46.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.634 seconds
[2025-09-09T08:22:16.982+0000] {processor.py:161} INFO - Started process (PID=568) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:16.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:22:16.986+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:16.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:17.656+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:17.672+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:17.671+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:22:17.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:17.679+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:22:17.692+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.717 seconds
[2025-09-09T08:22:47.805+0000] {processor.py:161} INFO - Started process (PID=572) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:47.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:22:47.808+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:47.808+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:48.440+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:22:48.475+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:48.475+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:22:48.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:22:48.486+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:22:48.508+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.710 seconds
[2025-09-09T08:23:18.626+0000] {processor.py:161} INFO - Started process (PID=576) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:23:18.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:23:18.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:23:18.628+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:23:19.352+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:23:19.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:23:19.371+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:23:19.380+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:23:19.380+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:23:19.394+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.775 seconds
[2025-09-09T08:43:50.604+0000] {processor.py:161} INFO - Started process (PID=20) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:43:50.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:43:50.609+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:43:50.608+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:43:52.042+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:43:52.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:43:52.313+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:43:52.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:43:52.325+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:43:52.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.755 seconds
[2025-09-09T08:44:22.509+0000] {processor.py:161} INFO - Started process (PID=26) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:22.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:44:22.512+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:22.511+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:23.215+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:23.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:23.285+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:44:23.293+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:23.293+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:44:23.307+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.804 seconds
[2025-09-09T08:44:53.510+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:53.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:44:53.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:53.514+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:54.135+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:44:54.152+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:54.152+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:44:54.160+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:44:54.159+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:44:54.171+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.667 seconds
[2025-09-09T08:45:24.358+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:24.359+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:45:24.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:24.361+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:25.045+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:25.066+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:25.066+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:45:25.074+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:25.073+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:45:25.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.735 seconds
[2025-09-09T08:45:55.188+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:55.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:45:55.190+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:55.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:55.728+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:45:55.743+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:55.743+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:45:55.750+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:45:55.750+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:45:55.761+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.578 seconds
[2025-09-09T08:46:25.880+0000] {processor.py:161} INFO - Started process (PID=40) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:25.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:46:25.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:25.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:26.728+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:26.752+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:26.751+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:46:26.763+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:26.763+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:46:26.787+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.914 seconds
[2025-09-09T08:46:56.957+0000] {processor.py:161} INFO - Started process (PID=46) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:56.958+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:46:56.960+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:56.959+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:57.641+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:46:57.657+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:57.656+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:46:57.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:46:57.663+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:46:57.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.721 seconds
[2025-09-09T08:47:27.885+0000] {processor.py:161} INFO - Started process (PID=50) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:27.886+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:47:27.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:27.887+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:28.752+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:28.779+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:28.778+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:47:28.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:28.791+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:47:28.807+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.929 seconds
[2025-09-09T08:47:59.217+0000] {processor.py:161} INFO - Started process (PID=54) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:59.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:47:59.219+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:59.219+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:59.747+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:47:59.762+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:59.762+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:47:59.769+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:47:59.769+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:47:59.781+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.569 seconds
[2025-09-09T08:48:29.927+0000] {processor.py:161} INFO - Started process (PID=58) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:48:29.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:48:29.930+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:48:29.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:48:30.668+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:48:30.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:48:30.689+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:48:30.697+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:48:30.697+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:48:30.711+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.789 seconds
[2025-09-09T08:49:00.891+0000] {processor.py:161} INFO - Started process (PID=62) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:00.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:49:00.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:00.893+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:01.565+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:01.585+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:01.584+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:49:01.595+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:01.594+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:49:01.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.723 seconds
[2025-09-09T08:49:31.762+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:31.763+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:49:31.764+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:31.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:32.492+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:49:32.510+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:32.509+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:49:32.518+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:49:32.518+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:49:32.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.792 seconds
[2025-09-09T08:50:02.720+0000] {processor.py:161} INFO - Started process (PID=70) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:02.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:50:02.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:02.722+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:03.324+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:03.342+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:03.342+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:50:03.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:03.350+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:50:03.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.650 seconds
[2025-09-09T08:50:33.517+0000] {processor.py:161} INFO - Started process (PID=74) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:33.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:50:33.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:33.521+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:34.195+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:50:34.212+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:34.212+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:50:34.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:50:34.222+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:50:34.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.725 seconds
[2025-09-09T08:51:04.437+0000] {processor.py:161} INFO - Started process (PID=79) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:04.438+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:51:04.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:04.440+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:05.241+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:05.264+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:05.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:51:05.275+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:05.275+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:51:05.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.861 seconds
[2025-09-09T08:51:35.459+0000] {processor.py:161} INFO - Started process (PID=216) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:35.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:51:35.463+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:35.462+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:36.218+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:51:36.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:36.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:51:36.277+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:51:36.277+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:51:36.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.865 seconds
[2025-09-09T08:52:06.604+0000] {processor.py:161} INFO - Started process (PID=220) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:06.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:52:06.606+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:06.606+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:07.085+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:07.104+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:07.103+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:52:07.113+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:07.113+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:52:07.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.640 seconds
[2025-09-09T08:52:37.496+0000] {processor.py:161} INFO - Started process (PID=356) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:37.497+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:52:37.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:37.499+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:38.072+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:52:38.089+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:38.088+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:52:38.097+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:52:38.097+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:52:38.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.624 seconds
[2025-09-09T08:53:08.672+0000] {processor.py:161} INFO - Started process (PID=360) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:08.672+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:53:08.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:08.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:09.201+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:09.218+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:09.217+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:53:09.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:09.227+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:53:09.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.574 seconds
[2025-09-09T08:53:39.444+0000] {processor.py:161} INFO - Started process (PID=364) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:39.445+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:53:39.446+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:39.446+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:39.981+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:53:40.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:39.999+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:53:40.009+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:53:40.009+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:53:40.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.587 seconds
[2025-09-09T08:54:10.219+0000] {processor.py:161} INFO - Started process (PID=368) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:10.220+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:54:10.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:10.221+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:10.806+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:10.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:10.825+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:54:10.834+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:10.833+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:54:10.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.662 seconds
[2025-09-09T08:54:41.081+0000] {processor.py:161} INFO - Started process (PID=372) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:41.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:54:41.084+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:41.084+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:41.586+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:54:41.603+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:41.603+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:54:41.610+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:54:41.610+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:54:41.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.547 seconds
[2025-09-09T08:55:28.288+0000] {processor.py:161} INFO - Started process (PID=22) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:55:28.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:55:28.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:55:28.299+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:55:32.421+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:55:32.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:55:32.562+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:55:32.582+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:55:32.582+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:55:32.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.341 seconds
[2025-09-09T08:56:02.832+0000] {processor.py:161} INFO - Started process (PID=26) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:02.835+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:56:02.840+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:02.839+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:03.643+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:03.658+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:03.658+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:56:03.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:03.665+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:56:03.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.858 seconds
[2025-09-09T08:56:33.851+0000] {processor.py:161} INFO - Started process (PID=35) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:33.852+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:56:33.855+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:33.855+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:35.156+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:56:35.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:35.206+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:56:35.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:56:35.227+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:56:35.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.449 seconds
[2025-09-09T08:57:05.432+0000] {processor.py:161} INFO - Started process (PID=176) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:05.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:57:05.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:05.437+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:06.199+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:06.218+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:06.217+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:57:06.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:06.227+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:57:06.240+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.815 seconds
[2025-09-09T08:57:36.424+0000] {processor.py:161} INFO - Started process (PID=182) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:36.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:57:36.432+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:36.430+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:37.074+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:57:37.089+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:37.089+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:57:37.096+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:57:37.096+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:57:37.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.701 seconds
[2025-09-09T08:58:07.314+0000] {processor.py:161} INFO - Started process (PID=186) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:07.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:58:07.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:07.318+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:07.964+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:07.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:07.981+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:58:07.989+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:07.989+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:58:08.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.702 seconds
[2025-09-09T08:58:38.188+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:38.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:58:38.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:38.192+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:38.958+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:58:38.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:38.980+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:58:38.992+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:58:38.992+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:58:39.008+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.828 seconds
[2025-09-09T08:59:09.195+0000] {processor.py:161} INFO - Started process (PID=194) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:09.197+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:59:09.199+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:09.199+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:09.864+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:09.886+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:09.885+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:59:09.898+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:09.898+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:59:09.913+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.725 seconds
[2025-09-09T08:59:40.103+0000] {processor.py:161} INFO - Started process (PID=198) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:40.104+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T08:59:40.106+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:40.105+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:40.733+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T08:59:40.748+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:40.748+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T08:59:40.755+0000] {logging_mixin.py:188} INFO - [2025-09-09T08:59:40.755+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T08:59:40.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.668 seconds
[2025-09-09T09:00:10.994+0000] {processor.py:161} INFO - Started process (PID=314) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:10.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:00:11.003+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:11.003+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:12.584+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:12.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:12.618+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:00:12.641+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:12.641+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:00:13.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.696 seconds
[2025-09-09T09:00:43.913+0000] {processor.py:161} INFO - Started process (PID=344) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:43.914+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:00:43.916+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:43.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:44.767+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:00:44.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:44.786+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:00:44.794+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:00:44.794+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:00:44.809+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.912 seconds
[2025-09-09T09:01:14.907+0000] {processor.py:161} INFO - Started process (PID=348) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:14.910+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:01:14.912+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:14.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:15.633+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:15.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:15.651+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:01:15.661+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:15.661+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:01:15.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.772 seconds
[2025-09-09T09:01:45.886+0000] {processor.py:161} INFO - Started process (PID=352) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:45.887+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:01:45.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:45.889+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:46.589+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:01:46.608+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:46.608+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:01:46.616+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:01:46.615+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:01:46.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.747 seconds
[2025-09-09T09:02:16.808+0000] {processor.py:161} INFO - Started process (PID=356) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:16.809+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:02:16.811+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:16.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:17.570+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:17.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:17.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:02:17.595+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:17.595+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:02:17.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.810 seconds
[2025-09-09T09:02:47.908+0000] {processor.py:161} INFO - Started process (PID=360) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:47.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:02:47.911+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:47.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:48.503+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:02:48.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:48.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:02:48.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:02:48.527+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:02:48.538+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.636 seconds
[2025-09-09T09:03:18.693+0000] {processor.py:161} INFO - Started process (PID=364) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:18.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:03:18.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:18.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:19.404+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:19.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:19.421+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:03:19.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:19.430+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:03:19.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.758 seconds
[2025-09-09T09:03:49.583+0000] {processor.py:161} INFO - Started process (PID=368) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:49.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:03:49.586+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:49.586+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:50.212+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:03:50.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:50.229+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:03:50.240+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:03:50.240+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:03:50.253+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.675 seconds
[2025-09-09T09:04:13.362+0000] {processor.py:161} INFO - Started process (PID=372) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:13.363+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:04:13.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:04:13.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:13.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:04:13.903+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 18, in <module>
    python_callable=silver_topcv_transform,
                    ^^^^^^^^^^^^^^^^^^^^^^
NameError: name 'silver_topcv_transform' is not defined
[2025-09-09T09:04:13.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:13.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.583 seconds
[2025-09-09T09:04:32.089+0000] {processor.py:161} INFO - Started process (PID=376) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:32.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:04:32.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:04:32.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:32.755+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:04:32.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:04:32.778+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:04:32.794+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:04:32.793+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:04:32.844+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.760 seconds
[2025-09-09T09:05:03.386+0000] {processor.py:161} INFO - Started process (PID=519) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:03.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:05:03.390+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:03.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:04.156+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:04.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:04.174+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:05:04.183+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:04.183+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:05:04.199+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.822 seconds
[2025-09-09T09:05:34.252+0000] {processor.py:161} INFO - Started process (PID=523) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:34.255+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:05:34.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:34.258+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:34.814+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:05:34.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:34.837+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:05:34.847+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:05:34.846+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:05:34.863+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.628 seconds
[2025-09-09T09:06:05.040+0000] {processor.py:161} INFO - Started process (PID=527) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:06:05.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:06:05.043+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:06:05.043+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:06:05.611+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:06:05.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:06:05.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:06:05.637+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:06:05.637+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:06:05.650+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.616 seconds
[2025-09-09T09:07:23.584+0000] {processor.py:161} INFO - Started process (PID=20) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:23.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:07:23.592+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:23.591+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:25.578+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:26.435+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:26.433+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:07:26.457+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:26.457+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:07:26.491+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.921 seconds
[2025-09-09T09:07:56.814+0000] {processor.py:161} INFO - Started process (PID=26) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:56.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:07:56.845+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:56.842+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:57.734+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:07:57.755+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:57.755+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:07:57.766+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:07:57.766+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:07:57.785+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.062 seconds
[2025-09-09T09:08:27.986+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:27.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:08:27.988+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:27.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:28.673+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:28.692+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:28.691+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:08:28.699+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:28.699+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:08:28.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.733 seconds
[2025-09-09T09:08:58.927+0000] {processor.py:161} INFO - Started process (PID=34) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:58.928+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:08:58.930+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:58.929+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:59.504+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:08:59.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:59.522+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:08:59.528+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:08:59.528+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:08:59.540+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.619 seconds
[2025-09-09T09:09:29.705+0000] {processor.py:161} INFO - Started process (PID=38) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:09:29.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:09:29.707+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:09:29.707+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:09:30.285+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:09:30.304+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:09:30.304+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:09:30.312+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:09:30.312+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:09:30.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.625 seconds
[2025-09-09T09:10:00.392+0000] {processor.py:161} INFO - Started process (PID=42) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:00.393+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:10:00.394+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:00.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:00.982+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:00.998+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:00.998+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:10:01.005+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:01.005+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:10:01.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.630 seconds
[2025-09-09T09:10:31.103+0000] {processor.py:161} INFO - Started process (PID=44) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:31.105+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:10:31.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:31.106+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:31.793+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:10:31.812+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:31.811+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:10:31.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:10:31.819+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:10:31.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.737 seconds
[2025-09-09T09:11:01.931+0000] {processor.py:161} INFO - Started process (PID=48) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:01.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:11:01.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:01.933+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:02.541+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:02.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:02.556+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:11:02.564+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:02.564+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:11:02.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.648 seconds
[2025-09-09T09:11:32.715+0000] {processor.py:161} INFO - Started process (PID=52) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:32.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:11:32.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:32.717+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:33.324+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:11:33.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:33.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:11:33.347+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:11:33.347+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:11:33.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.648 seconds
[2025-09-09T09:12:03.542+0000] {processor.py:161} INFO - Started process (PID=56) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:03.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:12:03.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:03.544+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:04.202+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:04.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:04.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:12:04.230+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:04.229+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:12:04.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.706 seconds
[2025-09-09T09:12:34.448+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:34.449+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:12:34.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:34.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:35.073+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:12:35.087+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:35.087+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:12:35.093+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:12:35.093+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:12:35.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.660 seconds
[2025-09-09T09:13:05.314+0000] {processor.py:161} INFO - Started process (PID=64) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:05.315+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:13:05.316+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:05.316+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:05.935+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:05.949+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:05.949+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:13:05.955+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:05.955+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:13:05.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.661 seconds
[2025-09-09T09:13:36.144+0000] {processor.py:161} INFO - Started process (PID=68) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:36.145+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:13:36.147+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:36.146+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:36.782+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:13:36.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:36.797+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:13:36.803+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:13:36.803+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:13:36.814+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.675 seconds
[2025-09-09T09:14:07.077+0000] {processor.py:161} INFO - Started process (PID=72) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:07.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:14:07.083+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:07.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:07.928+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:07.947+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:07.947+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:14:07.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:07.959+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:14:07.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.916 seconds
[2025-09-09T09:14:38.163+0000] {processor.py:161} INFO - Started process (PID=76) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:38.165+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:14:38.170+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:38.169+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:39.107+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:39.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:39.123+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:14:39.132+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:39.131+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:14:39.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.998 seconds
[2025-09-09T09:14:43.205+0000] {processor.py:161} INFO - Started process (PID=80) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:43.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:14:43.212+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:43.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:43.825+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:14:43.845+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:43.845+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:14:43.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:14:43.853+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:14:43.867+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.678 seconds
[2025-09-09T09:15:13.950+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:13.951+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:15:13.953+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:13.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:14.540+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:14.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:14.558+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:15:14.567+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:14.566+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:15:14.579+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.636 seconds
[2025-09-09T09:15:44.745+0000] {processor.py:161} INFO - Started process (PID=88) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:44.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:15:44.747+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:44.747+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:45.355+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:15:45.381+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:45.381+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:15:45.390+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:15:45.390+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:15:45.404+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.664 seconds
[2025-09-09T09:16:15.651+0000] {processor.py:161} INFO - Started process (PID=92) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:15.653+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:16:15.656+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:15.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:16.503+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:16.532+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:16.532+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:16:16.542+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:16.542+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:16:16.558+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.917 seconds
[2025-09-09T09:16:46.732+0000] {processor.py:161} INFO - Started process (PID=222) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:46.733+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:16:46.735+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:46.735+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:47.340+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:16:47.357+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:47.357+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:16:47.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:16:47.367+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:16:47.382+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.657 seconds
[2025-09-09T09:17:17.946+0000] {processor.py:161} INFO - Started process (PID=237) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:17.947+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:17:17.951+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:17.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:18.641+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:18.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:18.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:17:18.677+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:18.676+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:17:18.694+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.755 seconds
[2025-09-09T09:17:48.887+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:48.888+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:17:48.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:48.890+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:49.475+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:17:49.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:49.502+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:17:49.510+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:17:49.510+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:17:49.524+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.643 seconds
[2025-09-09T09:18:19.701+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:19.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:18:19.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:19.704+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:20.196+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:20.213+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:20.213+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:18:20.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:20.221+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:18:20.234+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.550 seconds
[2025-09-09T09:18:50.500+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:50.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:18:50.504+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:50.504+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:50.996+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:18:51.014+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:51.013+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:18:51.025+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:18:51.025+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:18:51.040+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.549 seconds
[2025-09-09T09:19:21.218+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:21.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:19:21.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:21.221+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:21.783+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:21.800+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:21.799+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:19:21.807+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:21.807+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:19:21.822+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.610 seconds
[2025-09-09T09:19:52.007+0000] {processor.py:161} INFO - Started process (PID=257) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:52.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:19:52.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:52.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:52.713+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:19:52.732+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:52.732+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:19:52.742+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:19:52.742+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:19:52.757+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.755 seconds
[2025-09-09T09:20:16.525+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:16.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:20:16.528+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:16.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:16.544+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:16.714+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:16.714+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:20:16.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:16.722+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:20:16.740+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.222 seconds
[2025-09-09T09:20:17.535+0000] {processor.py:161} INFO - Started process (PID=260) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:17.536+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:20:17.538+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:17.538+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:17.562+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:17.575+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:17.575+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:20:17.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:17.587+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:20:17.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T09:20:47.777+0000] {processor.py:161} INFO - Started process (PID=263) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:47.779+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:20:47.780+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:47.780+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:47.791+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:20:47.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:47.824+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:20:47.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:20:47.834+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:20:47.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T09:21:18.064+0000] {processor.py:161} INFO - Started process (PID=266) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:18.065+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:21:18.068+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:18.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:18.084+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:18.108+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:18.108+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:21:18.118+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:18.118+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:21:18.141+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T09:21:48.206+0000] {processor.py:161} INFO - Started process (PID=269) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:48.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:21:48.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:48.208+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:48.220+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:21:48.244+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:48.244+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:21:48.254+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:21:48.254+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:21:48.268+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T09:22:18.425+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:18.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:22:18.428+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:18.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:18.440+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:18.464+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:18.464+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:22:18.473+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:18.473+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:22:18.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T09:22:48.645+0000] {processor.py:161} INFO - Started process (PID=329) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:48.646+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:22:48.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:48.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:48.697+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:22:48.733+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:48.733+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:22:48.754+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:22:48.753+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:22:48.779+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.139 seconds
[2025-09-09T09:23:18.936+0000] {processor.py:161} INFO - Started process (PID=332) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:18.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:23:18.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:18.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:18.951+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:18.971+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:18.971+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:23:18.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:18.980+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:23:18.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T09:23:49.163+0000] {processor.py:161} INFO - Started process (PID=335) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:49.163+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:23:49.165+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:49.164+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:49.176+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:23:49.198+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:49.198+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:23:49.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:23:49.207+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:23:49.220+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T09:24:19.288+0000] {processor.py:161} INFO - Started process (PID=338) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:19.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:24:19.292+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:19.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:19.313+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:19.357+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:19.356+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:24:19.374+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:19.374+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:24:19.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.120 seconds
[2025-09-09T09:24:49.560+0000] {processor.py:161} INFO - Started process (PID=341) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:49.561+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:24:49.562+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:49.562+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:49.575+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:49.600+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:49.600+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:24:49.612+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:49.612+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:24:49.627+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T09:24:52.601+0000] {processor.py:161} INFO - Started process (PID=342) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:52.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:24:52.603+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:52.603+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:52.622+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:24:52.749+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:52.748+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:24:52.757+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:24:52.756+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:24:52.782+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.185 seconds
[2025-09-09T09:25:22.970+0000] {processor.py:161} INFO - Started process (PID=398) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:22.970+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:25:22.972+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:22.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:22.984+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:23.008+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:23.008+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:25:23.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:23.020+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:25:23.034+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T09:25:44.714+0000] {processor.py:161} INFO - Started process (PID=401) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:44.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:25:44.716+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:44.716+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:44.733+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:44.809+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:44.809+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:25:44.817+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:44.817+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:25:44.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.124 seconds
[2025-09-09T09:25:45.729+0000] {processor.py:161} INFO - Started process (PID=402) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:45.730+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:25:45.731+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:45.731+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:45.745+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:25:45.754+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:45.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:25:45.764+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:25:45.764+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:25:45.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.055 seconds
[2025-09-09T09:26:15.955+0000] {processor.py:161} INFO - Started process (PID=405) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:15.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:26:15.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:15.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:15.974+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:15.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:15.994+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:26:16.003+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:16.003+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:26:16.017+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T09:26:29.058+0000] {processor.py:161} INFO - Started process (PID=406) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:29.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:26:29.061+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:29.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:29.078+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:29.101+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:29.101+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:26:29.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:29.112+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:26:29.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T09:26:59.300+0000] {processor.py:161} INFO - Started process (PID=409) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:59.301+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:26:59.302+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:59.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:59.317+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:26:59.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:59.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:26:59.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:26:59.358+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:26:59.377+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T09:27:30.015+0000] {processor.py:161} INFO - Started process (PID=414) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:27:30.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:27:30.017+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:27:30.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:27:30.031+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:27:30.057+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:27:30.056+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:27:30.068+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:27:30.068+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:27:30.083+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T09:28:00.233+0000] {processor.py:161} INFO - Started process (PID=417) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:00.234+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:28:00.235+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:00.235+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:00.248+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:00.269+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:00.269+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:28:00.279+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:00.279+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:28:00.292+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T09:28:30.442+0000] {processor.py:161} INFO - Started process (PID=420) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:30.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:28:30.445+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:30.445+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:30.461+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:28:30.493+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:30.493+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:28:30.508+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:28:30.508+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:28:30.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.092 seconds
[2025-09-09T09:29:00.707+0000] {processor.py:161} INFO - Started process (PID=423) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:00.708+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:29:00.710+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:00.709+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:00.721+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:00.745+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:00.745+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:29:00.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:00.756+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:29:00.770+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T09:29:30.954+0000] {processor.py:161} INFO - Started process (PID=426) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:30.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:29:30.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:30.956+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:30.968+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:29:30.989+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:30.989+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:29:30.998+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:29:30.997+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:29:31.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T09:30:01.204+0000] {processor.py:161} INFO - Started process (PID=429) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:01.205+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:30:01.206+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:01.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:01.223+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:01.254+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:01.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:30:01.268+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:01.268+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:30:01.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T09:30:31.464+0000] {processor.py:161} INFO - Started process (PID=432) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:31.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:30:31.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:31.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:31.479+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:30:31.504+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:31.503+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:30:31.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:30:31.513+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:30:31.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T09:31:01.730+0000] {processor.py:161} INFO - Started process (PID=435) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:01.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:31:01.732+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:01.732+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:01.747+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:01.787+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:01.787+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:31:01.800+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:01.800+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:31:01.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T09:31:32.047+0000] {processor.py:161} INFO - Started process (PID=438) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:32.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:31:32.048+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:32.048+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:32.062+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:31:32.090+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:32.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:31:32.103+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:31:32.102+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:31:32.120+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T09:32:02.378+0000] {processor.py:161} INFO - Started process (PID=441) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:02.380+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:32:02.381+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:02.381+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:02.398+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:02.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:02.428+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:32:02.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:02.442+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:32:02.461+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T09:32:32.677+0000] {processor.py:161} INFO - Started process (PID=444) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:32.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:32:32.678+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:32.678+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:32.691+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:32:32.715+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:32.715+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:32:32.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:32:32.725+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:32:32.739+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T09:33:02.957+0000] {processor.py:161} INFO - Started process (PID=447) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:02.961+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:33:02.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:02.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:02.977+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:03.456+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:03.456+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:33:03.464+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:03.464+0000] {dag.py:3118} INFO - Creating ORM DAG for spark_transform_pipeline
[2025-09-09T09:33:03.465+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:03.465+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:33:03.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.535 seconds
[2025-09-09T09:33:33.717+0000] {processor.py:161} INFO - Started process (PID=450) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:33.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:33:33.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:33.722+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:33.749+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:33:33.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:33.791+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:33:33.805+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:33:33.804+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:33:33.824+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.130 seconds
[2025-09-09T09:34:04.197+0000] {processor.py:161} INFO - Started process (PID=453) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:04.203+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:34:04.209+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:04.206+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:04.342+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:04.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:04.705+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:34:04.798+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:04.797+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:34:04.972+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.892 seconds
[2025-09-09T09:34:33.797+0000] {processor.py:161} INFO - Started process (PID=22) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:33.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:34:33.803+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:33.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:33.828+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:34:33.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:33.866+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:34:33.915+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:34:33.915+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:34:33.990+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.199 seconds
[2025-09-09T09:35:04.983+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:04.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:35:04.993+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:04.992+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:05.054+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:05.352+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:05.351+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:35:05.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:05.367+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:35:05.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.425 seconds
[2025-09-09T09:35:35.646+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:35.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:35:35.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:35.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:35.670+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:35:35.706+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:35.706+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:35:35.720+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:35:35.720+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:35:35.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.099 seconds
[2025-09-09T09:36:05.941+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:05.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:36:05.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:05.945+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:05.960+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:05.986+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:05.986+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:36:05.997+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:05.997+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:36:06.012+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T09:36:36.240+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:36.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:36:36.242+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:36.242+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:36.254+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:36:36.276+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:36.275+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:36:36.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:36:36.284+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:36:36.298+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T09:37:06.523+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:06.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:37:06.526+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:06.526+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:06.544+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:06.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:06.570+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:37:06.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:06.580+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:37:06.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T09:37:36.817+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:36.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:37:36.822+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:36.821+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:36.846+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:37:36.894+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:36.894+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:37:36.914+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:37:36.913+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:37:36.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.128 seconds
[2025-09-09T09:38:07.118+0000] {processor.py:161} INFO - Started process (PID=60) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:07.119+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:38:07.121+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:07.121+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:07.136+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:07.164+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:07.164+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:38:07.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:07.175+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:38:07.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T09:38:37.375+0000] {processor.py:161} INFO - Started process (PID=63) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:37.376+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:38:37.378+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:37.377+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:37.391+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:38:37.413+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:37.413+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:38:37.423+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:38:37.422+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:38:37.437+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T09:39:07.650+0000] {processor.py:161} INFO - Started process (PID=66) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:07.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:39:07.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:07.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:07.664+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:07.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:07.685+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:39:07.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:07.694+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:39:07.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T09:39:37.912+0000] {processor.py:161} INFO - Started process (PID=75) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:37.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:39:37.914+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:37.914+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:37.928+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:39:37.952+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:37.951+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:39:37.961+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:39:37.961+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:39:37.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T09:40:08.138+0000] {processor.py:161} INFO - Started process (PID=78) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:08.140+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:40:08.141+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:08.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:08.155+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:08.183+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:08.182+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:40:08.332+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:08.332+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:40:08.345+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.213 seconds
[2025-09-09T09:40:38.516+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:38.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:40:38.518+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:38.517+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:38.529+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:40:38.659+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:38.659+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:40:38.667+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:40:38.666+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:40:38.679+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.169 seconds
[2025-09-09T09:41:08.875+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:08.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:41:08.878+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:08.877+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:09.024+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:09.044+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:09.044+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:41:09.053+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:09.053+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:41:09.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.208 seconds
[2025-09-09T09:41:39.458+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:39.459+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:41:39.460+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:39.460+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:39.476+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:41:39.504+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:39.504+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:41:39.515+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:41:39.515+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:41:39.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T09:42:09.722+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:09.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:42:09.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:09.725+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:09.737+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:09.761+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:09.760+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:42:09.770+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:09.769+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:42:09.783+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T09:42:40.005+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:40.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:42:40.034+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:40.031+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:40.046+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:42:40.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:40.071+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:42:40.082+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:42:40.082+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:42:40.096+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.099 seconds
[2025-09-09T09:43:10.317+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:10.317+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:43:10.319+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:10.319+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:10.336+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:10.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:10.358+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:43:10.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:10.367+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:43:10.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T09:43:40.622+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:40.623+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:43:40.625+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:40.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:40.638+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:43:40.661+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:40.661+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:43:40.670+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:43:40.670+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:43:40.733+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.116 seconds
[2025-09-09T09:44:10.928+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:10.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:44:10.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:10.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:10.945+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:10.972+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:10.972+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:44:10.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:10.981+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:44:10.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T09:44:41.259+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:41.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:44:41.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:41.262+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:41.277+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:44:41.328+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:41.327+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:44:41.352+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:44:41.352+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:44:41.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.137 seconds
[2025-09-09T09:45:11.654+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:11.656+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:45:11.658+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:11.658+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:11.674+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:11.704+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:11.704+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:45:11.716+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:11.715+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:45:11.732+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.091 seconds
[2025-09-09T09:45:41.926+0000] {processor.py:161} INFO - Started process (PID=111) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:41.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:45:41.928+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:41.928+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:41.942+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:45:41.963+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:41.963+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:45:41.974+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:45:41.974+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:45:41.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T09:46:12.212+0000] {processor.py:161} INFO - Started process (PID=114) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:12.213+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:46:12.214+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:12.214+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:12.226+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:12.252+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:12.252+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:46:12.263+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:12.263+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:46:12.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T09:46:42.520+0000] {processor.py:161} INFO - Started process (PID=117) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:42.522+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:46:42.524+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:42.524+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:42.538+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:46:42.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:42.566+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:46:42.577+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:46:42.576+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:46:42.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T09:47:12.815+0000] {processor.py:161} INFO - Started process (PID=120) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:12.816+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:47:12.818+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:12.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:12.831+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:12.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:12.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:47:12.868+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:12.868+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:47:12.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T09:47:43.111+0000] {processor.py:161} INFO - Started process (PID=123) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:43.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:47:43.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:43.115+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:43.130+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:47:43.158+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:43.158+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:47:43.174+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:47:43.174+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:47:43.191+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T09:48:13.406+0000] {processor.py:161} INFO - Started process (PID=126) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:13.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:48:13.408+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:13.408+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:13.420+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:13.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:13.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:48:13.450+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:13.450+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:48:13.463+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T09:48:43.701+0000] {processor.py:161} INFO - Started process (PID=129) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:43.702+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:48:43.703+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:43.703+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:43.715+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:48:43.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:43.737+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:48:43.746+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:48:43.746+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:48:43.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T09:49:13.955+0000] {processor.py:161} INFO - Started process (PID=132) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:13.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:49:13.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:13.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:13.969+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:13.992+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:13.992+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:49:14.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:14.001+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:49:14.015+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T09:49:44.222+0000] {processor.py:161} INFO - Started process (PID=135) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:44.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:49:44.224+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:44.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:44.237+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:49:44.258+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:44.258+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:49:44.266+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:49:44.266+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:49:44.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T09:50:14.525+0000] {processor.py:161} INFO - Started process (PID=138) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:14.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:50:14.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:14.527+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:14.540+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:14.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:14.562+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:50:14.571+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:14.571+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:50:14.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T09:50:44.822+0000] {processor.py:161} INFO - Started process (PID=141) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:44.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:50:44.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:44.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:44.839+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:50:44.868+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:44.868+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:50:44.881+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:50:44.881+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:50:44.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T09:51:15.065+0000] {processor.py:161} INFO - Started process (PID=144) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:15.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:51:15.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:15.067+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:15.079+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:15.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:15.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:51:15.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:15.115+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:51:15.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T09:51:45.324+0000] {processor.py:161} INFO - Started process (PID=147) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:45.325+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:51:45.327+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:45.326+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:45.338+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:51:45.362+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:45.362+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:51:45.371+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:51:45.371+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:51:45.386+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T09:52:15.588+0000] {processor.py:161} INFO - Started process (PID=150) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:15.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:52:15.590+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:15.590+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:15.602+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:15.624+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:15.624+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:52:15.634+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:15.634+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:52:15.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T09:52:45.853+0000] {processor.py:161} INFO - Started process (PID=153) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:45.854+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:52:45.856+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:45.855+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:45.869+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:52:45.894+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:45.894+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:52:45.903+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:52:45.903+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:52:45.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T09:53:16.091+0000] {processor.py:161} INFO - Started process (PID=156) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:16.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:53:16.094+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:16.093+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:16.107+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:16.132+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:16.132+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:53:16.142+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:16.141+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:53:16.156+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T09:53:46.387+0000] {processor.py:161} INFO - Started process (PID=159) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:46.388+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:53:46.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:46.389+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:46.402+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:53:46.423+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:46.423+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:53:46.431+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:53:46.431+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:53:46.443+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T09:54:16.694+0000] {processor.py:161} INFO - Started process (PID=162) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:16.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:54:16.697+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:16.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:16.710+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:16.734+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:16.734+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:54:16.744+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:16.744+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:54:16.759+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T09:54:46.982+0000] {processor.py:161} INFO - Started process (PID=165) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:46.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:54:46.984+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:46.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:46.996+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:54:47.021+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:47.020+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:54:47.031+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:54:47.031+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:54:47.049+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T09:55:17.309+0000] {processor.py:161} INFO - Started process (PID=168) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:17.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:55:17.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:17.311+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:17.323+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:17.345+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:17.345+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:55:17.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:17.358+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:55:17.373+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T09:55:47.654+0000] {processor.py:161} INFO - Started process (PID=171) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:47.655+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:55:47.656+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:47.656+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:47.668+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:55:47.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:47.689+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:55:47.698+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:55:47.697+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:55:47.710+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T09:56:17.983+0000] {processor.py:161} INFO - Started process (PID=174) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:17.983+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:56:17.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:17.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:17.997+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:18.017+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:18.017+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:56:18.025+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:18.025+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:56:18.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T09:56:48.251+0000] {processor.py:161} INFO - Started process (PID=177) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:48.252+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:56:48.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:48.253+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:48.264+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:56:48.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:48.283+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:56:48.292+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:56:48.292+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:56:48.305+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T09:57:18.661+0000] {processor.py:161} INFO - Started process (PID=180) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:18.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:57:18.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:18.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:18.674+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:18.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:18.694+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:57:18.702+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:18.701+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:57:18.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T09:57:49.000+0000] {processor.py:161} INFO - Started process (PID=183) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:49.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:57:49.002+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:49.002+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:49.012+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:57:49.032+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:49.032+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:57:49.041+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:57:49.040+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:57:49.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T09:58:19.234+0000] {processor.py:161} INFO - Started process (PID=185) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:19.235+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:58:19.237+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:19.237+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:19.253+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:19.277+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:19.277+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:58:19.288+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:19.287+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:58:19.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T09:58:49.621+0000] {processor.py:161} INFO - Started process (PID=188) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:49.622+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:58:49.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:49.626+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:49.642+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:58:49.676+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:49.676+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:58:49.695+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:58:49.694+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:58:49.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T09:59:19.940+0000] {processor.py:161} INFO - Started process (PID=190) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:19.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:59:19.946+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:19.946+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:19.966+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:20.012+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:20.012+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:59:20.026+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:20.026+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:59:20.053+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.133 seconds
[2025-09-09T09:59:50.335+0000] {processor.py:161} INFO - Started process (PID=193) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:50.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T09:59:50.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:50.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:50.357+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T09:59:50.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:50.393+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T09:59:50.405+0000] {logging_mixin.py:188} INFO - [2025-09-09T09:59:50.405+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T09:59:50.422+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T10:00:20.627+0000] {processor.py:161} INFO - Started process (PID=196) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:20.629+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:00:20.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:20.633+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:20.651+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:20.683+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:20.683+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:00:20.697+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:20.697+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:00:20.716+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.104 seconds
[2025-09-09T10:00:50.917+0000] {processor.py:161} INFO - Started process (PID=199) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:50.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:00:50.923+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:50.922+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:50.937+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:00:50.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:50.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:00:50.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:00:50.982+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:00:50.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.099 seconds
[2025-09-09T10:01:21.189+0000] {processor.py:161} INFO - Started process (PID=202) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:21.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:01:21.194+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:21.194+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:21.223+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:21.266+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:21.266+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:01:21.283+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:21.283+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:01:21.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.146 seconds
[2025-09-09T10:01:51.538+0000] {processor.py:161} INFO - Started process (PID=205) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:51.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:01:51.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:51.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:51.558+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:01:51.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:51.587+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:01:51.599+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:01:51.599+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:01:51.616+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T10:02:21.786+0000] {processor.py:161} INFO - Started process (PID=208) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:21.787+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:02:21.789+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:21.789+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:21.805+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:21.854+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:21.854+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:02:21.876+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:21.876+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:02:21.895+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.116 seconds
[2025-09-09T10:02:52.063+0000] {processor.py:161} INFO - Started process (PID=211) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:52.064+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:02:52.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:52.067+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:52.082+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:02:52.108+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:52.108+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:02:52.119+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:02:52.119+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:02:52.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T10:03:22.351+0000] {processor.py:161} INFO - Started process (PID=214) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:22.353+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:03:22.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:22.357+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:22.390+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:22.426+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:22.426+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:03:22.437+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:22.437+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:03:22.456+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.119 seconds
[2025-09-09T10:03:52.639+0000] {processor.py:161} INFO - Started process (PID=217) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:52.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:03:52.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:52.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:52.661+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:03:52.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:52.696+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:03:52.709+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:03:52.709+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:03:52.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.098 seconds
[2025-09-09T10:04:22.910+0000] {processor.py:161} INFO - Started process (PID=220) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:22.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:04:22.916+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:22.915+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:22.930+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:22.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:22.959+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:04:22.969+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:22.969+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:04:22.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T10:04:53.190+0000] {processor.py:161} INFO - Started process (PID=223) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:53.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:04:53.194+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:53.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:53.214+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:04:53.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:53.252+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:04:53.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:04:53.264+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:04:53.287+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.106 seconds
[2025-09-09T10:05:23.485+0000] {processor.py:161} INFO - Started process (PID=226) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:23.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:05:23.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:23.488+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:23.504+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:23.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:23.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:05:23.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:23.541+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:05:23.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T10:05:53.753+0000] {processor.py:161} INFO - Started process (PID=229) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:53.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:05:53.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:53.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:53.772+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:05:53.799+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:53.799+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:05:53.810+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:05:53.810+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:05:53.830+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T10:06:24.004+0000] {processor.py:161} INFO - Started process (PID=232) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:24.005+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:06:24.007+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:24.007+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:24.027+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:24.062+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:24.062+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:06:24.075+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:24.075+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:06:24.098+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.103 seconds
[2025-09-09T10:06:54.258+0000] {processor.py:161} INFO - Started process (PID=235) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:54.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:06:54.261+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:54.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:54.275+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:06:54.307+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:54.307+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:06:54.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:06:54.316+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:06:54.332+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T10:07:24.511+0000] {processor.py:161} INFO - Started process (PID=238) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:24.512+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:07:24.513+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:24.513+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:24.527+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:24.556+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:24.556+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:07:24.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:24.566+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:07:24.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T10:07:54.770+0000] {processor.py:161} INFO - Started process (PID=241) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:54.771+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:07:54.773+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:54.772+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:54.789+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:07:54.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:54.820+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:07:54.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:07:54.832+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:07:54.851+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T10:08:25.007+0000] {processor.py:161} INFO - Started process (PID=244) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:25.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:08:25.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:25.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:25.024+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:25.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:25.051+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:08:25.062+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:25.062+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:08:25.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T10:08:55.247+0000] {processor.py:161} INFO - Started process (PID=247) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:55.248+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:08:55.249+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:55.249+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:55.262+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:08:55.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:55.286+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:08:55.296+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:08:55.296+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:08:55.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T10:09:25.527+0000] {processor.py:161} INFO - Started process (PID=250) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:25.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:09:25.534+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:25.533+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:25.561+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:25.616+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:25.615+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:09:25.640+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:25.640+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:09:25.678+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.168 seconds
[2025-09-09T10:09:55.868+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:55.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:09:55.872+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:55.871+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:55.887+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:09:55.914+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:55.914+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:09:55.926+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:09:55.926+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:09:55.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T10:10:26.174+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:26.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:10:26.178+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:26.177+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:26.199+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:26.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:26.229+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:10:26.239+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:26.239+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:10:26.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T10:10:56.420+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:56.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:10:56.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:56.424+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:56.441+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:10:56.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:56.469+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:10:56.480+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:10:56.480+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:10:56.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T10:11:26.773+0000] {processor.py:161} INFO - Started process (PID=262) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:26.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:11:26.777+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:26.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:26.795+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:26.823+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:26.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:11:26.833+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:26.833+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:11:26.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T10:11:57.075+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:57.076+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:11:57.078+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:57.077+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:57.090+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:11:57.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:57.112+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:11:57.121+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:11:57.121+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:11:57.135+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T10:12:27.299+0000] {processor.py:161} INFO - Started process (PID=268) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:27.300+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:12:27.302+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:27.302+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:27.324+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:27.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:27.366+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:12:27.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:27.382+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:12:27.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.107 seconds
[2025-09-09T10:12:57.591+0000] {processor.py:161} INFO - Started process (PID=271) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:57.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:12:57.594+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:57.594+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:57.607+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:12:57.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:57.627+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:12:57.635+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:12:57.635+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:12:57.648+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:13:27.813+0000] {processor.py:161} INFO - Started process (PID=274) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:27.814+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:13:27.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:27.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:27.834+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:27.862+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:27.862+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:13:27.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:27.871+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:13:27.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T10:13:58.093+0000] {processor.py:161} INFO - Started process (PID=277) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:58.094+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:13:58.096+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:58.096+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:58.109+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:13:58.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:58.130+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:13:58.139+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:13:58.139+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:13:58.152+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T10:14:28.335+0000] {processor.py:161} INFO - Started process (PID=280) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:28.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:14:28.338+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:28.337+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:28.348+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:28.368+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:28.368+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:14:28.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:28.376+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:14:28.390+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T10:14:58.566+0000] {processor.py:161} INFO - Started process (PID=282) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:58.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:14:58.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:58.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:58.581+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:14:58.606+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:58.606+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:14:58.616+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:14:58.616+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:14:58.635+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T10:15:28.923+0000] {processor.py:161} INFO - Started process (PID=285) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:28.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:15:28.925+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:28.924+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:28.937+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:28.964+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:28.964+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:15:28.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:28.976+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:15:28.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T10:15:59.282+0000] {processor.py:161} INFO - Started process (PID=288) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:59.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:15:59.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:59.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:59.300+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:15:59.324+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:59.324+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:15:59.334+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:15:59.334+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:15:59.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T10:16:29.695+0000] {processor.py:161} INFO - Started process (PID=291) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:16:29.697+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:16:29.698+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:16:29.698+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:16:29.713+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:16:29.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:16:29.737+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:16:29.746+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:16:29.746+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:16:29.760+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T10:17:00.042+0000] {processor.py:161} INFO - Started process (PID=294) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:00.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:17:00.045+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:00.044+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:00.057+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:00.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:00.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:17:00.089+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:00.089+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:17:00.104+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T10:17:30.396+0000] {processor.py:161} INFO - Started process (PID=297) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:30.397+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:17:30.398+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:30.398+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:30.415+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:17:30.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:30.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:17:30.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:17:30.450+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:17:30.465+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T10:18:00.764+0000] {processor.py:161} INFO - Started process (PID=300) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:00.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:18:00.767+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:00.766+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:00.782+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:00.805+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:00.805+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:18:00.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:00.816+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:18:00.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T10:18:31.081+0000] {processor.py:161} INFO - Started process (PID=302) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:31.082+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:18:31.084+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:31.083+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:31.096+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:18:31.118+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:31.118+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:18:31.127+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:18:31.127+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:18:31.141+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T10:19:01.430+0000] {processor.py:161} INFO - Started process (PID=305) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:01.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:19:01.433+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:01.432+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:01.445+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:01.470+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:01.470+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:19:01.479+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:01.478+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:19:01.492+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T10:19:31.821+0000] {processor.py:161} INFO - Started process (PID=308) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:31.822+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:19:31.823+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:31.823+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:31.833+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:19:31.854+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:31.854+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:19:31.863+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:19:31.863+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:19:31.877+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:20:02.178+0000] {processor.py:161} INFO - Started process (PID=311) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:02.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:20:02.180+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:02.180+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:02.191+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:02.211+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:02.211+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:20:02.219+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:02.219+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:20:02.232+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T10:20:32.518+0000] {processor.py:161} INFO - Started process (PID=314) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:32.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:20:32.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:32.520+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:32.531+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:20:32.554+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:32.554+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:20:32.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:20:32.563+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:20:32.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T10:21:02.853+0000] {processor.py:161} INFO - Started process (PID=317) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:02.854+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:21:02.855+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:02.855+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:02.867+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:02.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:02.887+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:21:02.896+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:02.896+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:21:02.910+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:21:33.191+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:33.192+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:21:33.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:33.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:33.206+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:21:33.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:33.226+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:21:33.235+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:21:33.235+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:21:33.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T10:22:03.538+0000] {processor.py:161} INFO - Started process (PID=323) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:03.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:22:03.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:03.540+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:03.552+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:03.573+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:03.572+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:22:03.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:03.581+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:22:03.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:22:33.903+0000] {processor.py:161} INFO - Started process (PID=326) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:33.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:22:33.906+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:33.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:33.918+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:22:33.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:33.939+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:22:33.947+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:22:33.947+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:22:33.960+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:23:04.276+0000] {processor.py:161} INFO - Started process (PID=329) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:04.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:23:04.278+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:04.278+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:04.289+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:04.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:04.311+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:23:04.321+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:04.320+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:23:04.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T10:23:34.572+0000] {processor.py:161} INFO - Started process (PID=332) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:34.573+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:23:34.574+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:34.574+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:34.588+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:23:34.613+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:34.613+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:23:34.623+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:23:34.623+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:23:34.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T10:24:04.980+0000] {processor.py:161} INFO - Started process (PID=335) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:04.980+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:24:04.982+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:04.981+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:04.993+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:05.015+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:05.014+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:24:05.023+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:05.023+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:24:05.038+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T10:24:35.272+0000] {processor.py:161} INFO - Started process (PID=338) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:35.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:24:35.274+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:35.274+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:35.286+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:24:35.307+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:35.307+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:24:35.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:24:35.316+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:24:35.330+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T10:25:05.601+0000] {processor.py:161} INFO - Started process (PID=341) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:05.602+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:25:05.604+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:05.604+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:05.618+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:05.639+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:05.639+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:25:05.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:05.648+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:25:05.661+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T10:25:35.897+0000] {processor.py:161} INFO - Started process (PID=344) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:35.898+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:25:35.899+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:35.899+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:35.912+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:25:35.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:35.934+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:25:35.942+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:25:35.942+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:25:35.955+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T10:26:06.228+0000] {processor.py:161} INFO - Started process (PID=347) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:06.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:26:06.230+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:06.230+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:06.242+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:06.264+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:06.263+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:26:06.276+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:06.275+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:26:06.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T10:26:36.618+0000] {processor.py:161} INFO - Started process (PID=350) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:36.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:26:36.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:36.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:36.632+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:26:36.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:36.651+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:26:36.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:26:36.660+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:26:36.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T10:27:06.997+0000] {processor.py:161} INFO - Started process (PID=353) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:06.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:27:06.999+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:06.999+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:07.010+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:07.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:07.030+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:27:07.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:07.039+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:27:07.052+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T10:27:37.368+0000] {processor.py:161} INFO - Started process (PID=356) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:37.368+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:27:37.370+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:37.369+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:37.381+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:27:37.404+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:37.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:27:37.415+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:27:37.414+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:27:37.427+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T10:28:07.719+0000] {processor.py:161} INFO - Started process (PID=359) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:07.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:28:07.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:07.721+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:07.732+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:07.753+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:07.753+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:28:07.762+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:07.762+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:28:07.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T10:28:38.089+0000] {processor.py:161} INFO - Started process (PID=362) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:38.089+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:28:38.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:38.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:38.102+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:28:38.126+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:38.125+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:28:38.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:28:38.134+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:28:38.151+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T10:29:08.431+0000] {processor.py:161} INFO - Started process (PID=365) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:08.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:29:08.433+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:08.433+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:08.444+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:08.464+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:08.464+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:29:08.472+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:08.472+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:29:08.486+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T10:29:38.817+0000] {processor.py:161} INFO - Started process (PID=368) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:38.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T10:29:38.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:38.819+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:38.831+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T10:29:38.852+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:38.851+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T10:29:38.859+0000] {logging_mixin.py:188} INFO - [2025-09-09T10:29:38.859+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T10:29:38.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T12:38:10.795+0000] {processor.py:161} INFO - Started process (PID=372) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:10.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:38:10.809+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:10.808+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:10.847+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:11.004+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:11.003+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:38:11.026+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:11.025+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:38:11.090+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.317 seconds
[2025-09-09T12:38:41.320+0000] {processor.py:161} INFO - Started process (PID=374) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:41.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:38:41.323+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:41.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:41.340+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:38:41.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:41.365+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:38:41.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:38:41.376+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:38:41.392+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T12:39:11.569+0000] {processor.py:161} INFO - Started process (PID=377) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:11.571+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:39:11.573+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:11.572+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:11.588+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:11.615+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:11.615+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:39:11.628+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:11.627+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:39:11.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T12:39:41.814+0000] {processor.py:161} INFO - Started process (PID=380) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:41.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:39:41.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:41.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:41.827+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:39:41.849+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:41.849+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:39:41.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:39:41.858+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:39:41.872+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T12:40:12.106+0000] {processor.py:161} INFO - Started process (PID=383) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:12.107+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:40:12.109+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:12.108+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:12.124+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:12.153+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:12.153+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:40:12.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:12.163+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:40:12.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T12:40:42.454+0000] {processor.py:161} INFO - Started process (PID=386) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:42.455+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:40:42.456+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:42.456+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:42.468+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:40:42.490+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:42.490+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:40:42.498+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:40:42.498+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:40:42.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T12:41:12.770+0000] {processor.py:161} INFO - Started process (PID=389) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:12.771+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:41:12.773+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:12.772+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:12.794+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:12.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:12.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:41:12.862+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:12.862+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:41:12.883+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.119 seconds
[2025-09-09T12:41:43.081+0000] {processor.py:161} INFO - Started process (PID=392) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:43.081+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:41:43.083+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:43.082+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:43.094+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:41:43.116+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:43.116+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:41:43.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:41:43.125+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:41:43.137+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T12:42:13.427+0000] {processor.py:161} INFO - Started process (PID=395) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:13.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:42:13.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:13.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:13.441+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:13.463+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:13.463+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:42:13.472+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:13.472+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:42:13.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T12:42:43.741+0000] {processor.py:161} INFO - Started process (PID=398) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:43.741+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:42:43.743+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:43.743+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:43.755+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:42:43.777+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:43.777+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:42:43.787+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:42:43.787+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:42:43.801+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T12:43:14.066+0000] {processor.py:161} INFO - Started process (PID=401) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:14.067+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:43:14.068+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:14.068+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:14.080+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:14.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:14.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:43:14.110+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:14.110+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:43:14.123+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T12:43:44.321+0000] {processor.py:161} INFO - Started process (PID=404) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:44.322+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:43:44.324+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:44.323+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:44.336+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:43:44.357+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:44.357+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:43:44.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:43:44.365+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:43:44.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T12:44:14.593+0000] {processor.py:161} INFO - Started process (PID=407) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:14.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:44:14.595+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:14.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:14.608+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:14.628+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:14.628+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:44:14.636+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:14.636+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:44:14.649+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T12:44:44.961+0000] {processor.py:161} INFO - Started process (PID=410) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:44.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:44:44.963+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:44.963+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:44.977+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:44:45.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:45.000+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:44:45.011+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:44:45.011+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:44:45.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T12:45:15.329+0000] {processor.py:161} INFO - Started process (PID=413) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:15.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:45:15.331+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:15.331+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:15.343+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:15.363+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:15.362+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:45:15.371+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:15.371+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:45:15.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T12:45:45.634+0000] {processor.py:161} INFO - Started process (PID=416) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:45.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:45:45.636+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:45.635+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:45.647+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:45:45.667+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:45.667+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:45:45.676+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:45:45.676+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:45:45.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T12:46:15.903+0000] {processor.py:161} INFO - Started process (PID=419) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:15.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:46:15.906+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:15.906+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:15.921+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:15.953+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:15.952+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:46:15.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:15.962+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:46:15.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T12:46:46.217+0000] {processor.py:161} INFO - Started process (PID=422) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:46.218+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:46:46.219+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:46.219+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:46.232+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:46:46.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:46.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:46:46.261+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:46:46.261+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:46:46.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T12:47:16.526+0000] {processor.py:161} INFO - Started process (PID=425) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:16.527+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:47:16.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:16.528+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:16.541+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:16.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:16.563+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:47:16.573+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:16.573+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:47:16.588+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T12:47:46.807+0000] {processor.py:161} INFO - Started process (PID=428) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:46.808+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:47:46.810+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:46.810+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:46.824+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:47:46.855+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:46.854+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:47:46.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:47:46.866+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:47:46.882+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T12:48:17.116+0000] {processor.py:161} INFO - Started process (PID=431) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:17.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:48:17.119+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:17.118+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:17.131+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:17.155+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:17.154+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:48:17.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:17.163+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:48:17.177+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T12:48:47.378+0000] {processor.py:161} INFO - Started process (PID=434) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:47.379+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:48:47.380+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:47.380+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:47.395+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:48:47.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:47.422+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:48:47.434+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:48:47.434+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:48:47.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T12:49:17.687+0000] {processor.py:161} INFO - Started process (PID=437) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:17.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:49:17.690+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:17.689+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:17.701+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:17.723+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:17.723+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:49:17.732+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:17.732+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:49:17.746+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T12:49:47.939+0000] {processor.py:161} INFO - Started process (PID=440) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:47.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:49:47.942+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:47.941+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:47.955+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:49:47.978+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:47.978+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:49:47.986+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:49:47.986+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:49:48.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T12:50:18.207+0000] {processor.py:161} INFO - Started process (PID=443) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:18.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:50:18.209+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:18.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:18.225+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:18.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:18.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:50:18.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:18.262+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:50:18.279+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T12:50:48.459+0000] {processor.py:161} INFO - Started process (PID=446) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:48.460+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:50:48.461+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:48.461+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:48.473+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:50:48.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:48.494+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:50:48.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:50:48.503+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:50:48.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T12:51:18.693+0000] {processor.py:161} INFO - Started process (PID=449) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:18.694+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:51:18.695+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:18.695+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:18.708+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:18.731+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:18.731+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:51:18.740+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:18.740+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:51:18.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T12:51:48.935+0000] {processor.py:161} INFO - Started process (PID=452) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:48.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:51:48.938+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:48.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:48.954+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:51:48.986+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:48.986+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:51:48.996+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:51:48.995+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:51:49.022+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.094 seconds
[2025-09-09T12:52:19.222+0000] {processor.py:161} INFO - Started process (PID=455) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:19.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:52:19.224+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:19.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:19.239+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:19.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:19.265+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:52:19.276+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:19.276+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:52:19.293+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T12:52:49.406+0000] {processor.py:161} INFO - Started process (PID=458) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:49.407+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:52:49.409+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:49.409+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:49.425+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:52:49.464+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:49.464+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:52:49.478+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:52:49.478+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:52:49.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.101 seconds
[2025-09-09T12:53:19.683+0000] {processor.py:161} INFO - Started process (PID=461) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:19.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:53:19.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:19.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:19.701+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:19.730+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:19.730+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:53:19.741+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:19.741+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:53:19.755+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T12:53:49.901+0000] {processor.py:161} INFO - Started process (PID=464) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:49.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:53:49.904+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:49.904+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:49.920+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:53:49.949+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:49.949+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:53:49.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:53:49.959+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:53:49.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T12:54:20.117+0000] {processor.py:161} INFO - Started process (PID=467) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:20.118+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:54:20.119+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:20.119+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:20.132+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:20.157+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:20.157+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:54:20.166+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:20.165+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:54:20.179+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T12:54:50.320+0000] {processor.py:161} INFO - Started process (PID=470) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:50.321+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:54:50.322+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:50.322+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:50.337+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:54:50.362+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:50.362+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:54:50.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:54:50.372+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:54:50.387+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T12:55:20.578+0000] {processor.py:161} INFO - Started process (PID=473) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:20.579+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:55:20.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:20.581+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:20.596+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:20.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:20.618+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:55:20.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:20.627+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:55:20.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T12:55:50.800+0000] {processor.py:161} INFO - Started process (PID=476) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:50.801+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:55:50.802+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:50.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:50.813+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:55:50.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:50.841+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:55:50.852+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:55:50.852+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:55:50.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T12:56:21.052+0000] {processor.py:161} INFO - Started process (PID=479) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:21.053+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:56:21.054+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:21.054+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:21.068+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:21.094+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:21.094+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:56:21.105+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:21.105+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:56:21.121+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T12:56:51.275+0000] {processor.py:161} INFO - Started process (PID=482) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:51.276+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:56:51.277+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:51.277+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:51.291+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:56:51.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:51.317+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:56:51.327+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:56:51.327+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:56:51.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T12:57:21.518+0000] {processor.py:161} INFO - Started process (PID=485) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:21.520+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:57:21.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:21.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:21.552+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:21.595+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:21.595+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:57:21.615+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:21.614+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:57:21.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.129 seconds
[2025-09-09T12:57:51.827+0000] {processor.py:161} INFO - Started process (PID=488) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:51.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:57:51.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:51.829+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:51.841+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:57:51.862+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:51.862+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:57:51.872+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:57:51.872+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:57:51.886+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T12:58:22.078+0000] {processor.py:161} INFO - Started process (PID=491) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:22.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:58:22.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:22.080+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:22.093+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:22.116+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:22.115+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:58:22.124+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:22.124+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:58:22.139+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T12:58:52.246+0000] {processor.py:161} INFO - Started process (PID=493) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:52.247+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:58:52.248+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:52.248+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:52.262+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:58:52.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:52.285+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:58:52.295+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:58:52.295+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:58:52.310+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T12:59:22.482+0000] {processor.py:161} INFO - Started process (PID=495) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:22.483+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:59:22.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:22.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:22.498+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:22.519+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:22.519+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:59:22.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:22.529+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:59:22.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T12:59:52.750+0000] {processor.py:161} INFO - Started process (PID=498) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:52.751+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T12:59:52.753+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:52.753+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:52.766+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T12:59:52.788+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:52.788+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T12:59:52.799+0000] {logging_mixin.py:188} INFO - [2025-09-09T12:59:52.799+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T12:59:52.813+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T13:00:23.026+0000] {processor.py:161} INFO - Started process (PID=501) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:23.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:00:23.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:23.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:23.047+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:23.070+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:23.070+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:00:23.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:23.079+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:00:23.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T13:00:53.290+0000] {processor.py:161} INFO - Started process (PID=504) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:53.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:00:53.293+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:53.293+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:53.312+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:00:53.336+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:53.336+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:00:53.347+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:00:53.347+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:00:53.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T13:01:23.561+0000] {processor.py:161} INFO - Started process (PID=507) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:23.562+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:01:23.564+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:23.563+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:23.575+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:23.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:23.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:01:23.606+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:23.605+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:01:23.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T13:01:53.803+0000] {processor.py:161} INFO - Started process (PID=510) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:53.804+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:01:53.805+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:53.805+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:53.818+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:01:53.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:53.840+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:01:53.850+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:01:53.850+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:01:53.864+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T13:02:24.057+0000] {processor.py:161} INFO - Started process (PID=513) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:24.058+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:02:24.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:24.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:24.073+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:24.096+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:24.096+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:02:24.106+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:24.106+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:02:24.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T13:02:54.315+0000] {processor.py:161} INFO - Started process (PID=516) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:54.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:02:54.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:54.318+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:54.342+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:02:54.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:54.372+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:02:54.383+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:02:54.383+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:02:54.398+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.089 seconds
[2025-09-09T13:03:24.675+0000] {processor.py:161} INFO - Started process (PID=519) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:24.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:03:24.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:24.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:24.696+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:24.728+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:24.728+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:03:24.740+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:24.740+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:03:24.754+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T13:03:54.948+0000] {processor.py:161} INFO - Started process (PID=522) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:54.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:03:54.950+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:54.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:54.963+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:03:54.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:54.983+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:03:54.992+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:03:54.992+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:03:55.005+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T13:04:25.290+0000] {processor.py:161} INFO - Started process (PID=525) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:25.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:04:25.293+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:25.293+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:25.306+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:25.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:25.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:04:25.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:25.350+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:04:25.367+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T13:04:55.619+0000] {processor.py:161} INFO - Started process (PID=528) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:55.620+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:04:55.622+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:55.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:55.634+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:04:55.654+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:55.654+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:04:55.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:04:55.663+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:04:55.676+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T13:05:25.983+0000] {processor.py:161} INFO - Started process (PID=531) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:25.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:05:25.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:25.985+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:25.997+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:26.025+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:26.024+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:05:26.040+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:26.040+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:05:26.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T13:05:56.283+0000] {processor.py:161} INFO - Started process (PID=534) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:56.284+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:05:56.285+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:56.285+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:56.300+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:05:56.333+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:56.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:05:56.343+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:05:56.343+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:05:56.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T13:06:26.581+0000] {processor.py:161} INFO - Started process (PID=537) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:26.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:06:26.583+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:26.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:26.596+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:26.617+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:26.617+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:06:26.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:26.627+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:06:26.642+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T13:06:56.821+0000] {processor.py:161} INFO - Started process (PID=540) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:56.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:06:56.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:56.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:56.849+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:06:56.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:56.879+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:06:56.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:06:56.889+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:06:56.904+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T13:07:27.071+0000] {processor.py:161} INFO - Started process (PID=543) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:27.072+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:07:27.074+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:27.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:27.086+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:27.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:27.107+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:07:27.117+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:27.117+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:07:27.130+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T13:07:57.340+0000] {processor.py:161} INFO - Started process (PID=546) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:57.342+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:07:57.344+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:57.344+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:57.362+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:07:57.390+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:57.390+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:07:57.401+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:07:57.401+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:07:57.419+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T13:08:27.878+0000] {processor.py:161} INFO - Started process (PID=551) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:27.879+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:08:27.880+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:27.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:27.896+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:27.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:27.921+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:08:27.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:27.931+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:08:27.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:08:58.128+0000] {processor.py:161} INFO - Started process (PID=554) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:58.129+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:08:58.131+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:58.130+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:58.145+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:08:58.172+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:58.171+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:08:58.182+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:08:58.182+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:08:58.198+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T13:09:28.589+0000] {processor.py:161} INFO - Started process (PID=557) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:28.592+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:09:28.594+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:28.593+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:28.709+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:29.182+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:29.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:09:29.325+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:29.324+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:09:29.402+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.827 seconds
[2025-09-09T13:09:59.467+0000] {processor.py:161} INFO - Started process (PID=560) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:59.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:09:59.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:59.469+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:59.488+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:09:59.518+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:59.518+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:09:59.531+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:09:59.530+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:09:59.549+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T13:10:29.764+0000] {processor.py:161} INFO - Started process (PID=563) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:10:29.765+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:10:29.766+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:10:29.766+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:10:29.778+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:10:29.798+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:10:29.798+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:10:29.807+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:10:29.807+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:10:29.821+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T13:10:59.989+0000] {processor.py:161} INFO - Started process (PID=566) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:10:59.990+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:10:59.991+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:10:59.991+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:00.010+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:00.045+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:00.045+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:11:00.058+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:00.058+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:11:00.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.092 seconds
[2025-09-09T13:11:22.173+0000] {processor.py:161} INFO - Started process (PID=567) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:22.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:11:22.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:22.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:22.199+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:22.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:22.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:11:22.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:22.233+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:11:22.252+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T13:11:52.473+0000] {processor.py:161} INFO - Started process (PID=570) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:52.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:11:52.475+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:52.475+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:52.490+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:11:52.516+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:52.516+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:11:52.528+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:11:52.527+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:11:52.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T13:12:22.771+0000] {processor.py:161} INFO - Started process (PID=573) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:22.772+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:12:22.773+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:22.772+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:22.787+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:22.811+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:22.811+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:12:22.822+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:22.822+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:12:22.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T13:12:53.049+0000] {processor.py:161} INFO - Started process (PID=576) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:53.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:12:53.050+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:53.050+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:53.064+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:12:53.088+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:53.088+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:12:53.098+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:12:53.098+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:12:53.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:13:23.331+0000] {processor.py:161} INFO - Started process (PID=579) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:13:23.332+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:13:23.333+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:13:23.332+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:13:23.346+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:13:23.370+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:13:23.370+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:13:23.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:13:23.382+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:13:23.399+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:26:07.556+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:07.558+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:26:07.561+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:07.560+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:07.613+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:07.673+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:07.672+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:26:07.689+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:07.689+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:26:07.713+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.165 seconds
[2025-09-09T13:26:37.863+0000] {processor.py:161} INFO - Started process (PID=24) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:37.864+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:26:37.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:37.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:37.884+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:26:37.966+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:37.966+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:26:37.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:26:37.976+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:26:37.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.135 seconds
[2025-09-09T13:27:08.168+0000] {processor.py:161} INFO - Started process (PID=27) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:08.169+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:27:08.171+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:08.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:08.185+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:08.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:08.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:27:08.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:08.217+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:27:08.230+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T13:27:38.412+0000] {processor.py:161} INFO - Started process (PID=30) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:38.413+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:27:38.414+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:38.414+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:38.429+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:27:38.453+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:38.452+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:27:38.462+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:27:38.462+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:27:38.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T13:28:08.649+0000] {processor.py:161} INFO - Started process (PID=33) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:08.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:28:08.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:08.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:08.663+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:08.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:08.686+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:28:08.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:08.694+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:28:08.708+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T13:28:38.901+0000] {processor.py:161} INFO - Started process (PID=37) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:38.902+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:28:38.905+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:38.904+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:38.925+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:28:38.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:38.962+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:28:38.975+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:28:38.975+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:28:39.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T13:29:09.183+0000] {processor.py:161} INFO - Started process (PID=57) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:09.184+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:29:09.185+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:09.185+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:09.198+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:09.220+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:09.220+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:29:09.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:09.229+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:29:09.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T13:29:39.417+0000] {processor.py:161} INFO - Started process (PID=71) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:39.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:29:39.421+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:39.420+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:39.443+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:29:39.479+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:39.479+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:29:39.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:29:39.493+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:29:39.529+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.126 seconds
[2025-09-09T13:30:09.715+0000] {processor.py:161} INFO - Started process (PID=81) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:09.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:30:09.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:09.717+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:09.729+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:09.752+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:09.751+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:30:09.761+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:09.760+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:30:09.775+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T13:30:39.952+0000] {processor.py:161} INFO - Started process (PID=84) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:39.953+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:30:39.955+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:39.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:39.970+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:30:40.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:40.000+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:30:40.015+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:30:40.015+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:30:40.036+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T13:31:10.206+0000] {processor.py:161} INFO - Started process (PID=87) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:10.207+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:31:10.211+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:10.211+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:10.232+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:10.269+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:10.269+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:31:10.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:10.283+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:31:10.316+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.118 seconds
[2025-09-09T13:31:40.478+0000] {processor.py:161} INFO - Started process (PID=90) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:40.479+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:31:40.480+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:40.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:40.493+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:31:40.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:40.514+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:31:40.641+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:31:40.641+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:31:40.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.184 seconds
[2025-09-09T13:32:10.861+0000] {processor.py:161} INFO - Started process (PID=93) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:10.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:32:10.864+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:32:10.863+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:10.875+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:11.011+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:32:11.011+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:32:11.019+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:32:11.018+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:32:11.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.173 seconds
[2025-09-09T13:32:39.227+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:39.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:32:39.229+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:32:39.229+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:39.343+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:32:39.243+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 46, in <module>
    silver_group >> gold_transform >> load_jobs_to_motherduck
                    ^^^^^^^^^^^^^^
NameError: name 'gold_transform' is not defined
[2025-09-09T13:32:39.344+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:32:39.360+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.138 seconds
[2025-09-09T13:33:09.640+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:09.641+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:33:09.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:33:09.642+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:09.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:33:09.650+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 46, in <module>
    silver_group >> gold_transform >> load_jobs_to_motherduck
                    ^^^^^^^^^^^^^^
NameError: name 'gold_transform' is not defined
[2025-09-09T13:33:09.654+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:09.671+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.037 seconds
[2025-09-09T13:33:39.822+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:39.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:33:39.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:33:39.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:39.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:33:39.834+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 46, in <module>
    silver_group >> gold_transform >> load_jobs_to_motherduck
                    ^^^^^^^^^^^^^^
NameError: name 'gold_transform' is not defined
[2025-09-09T13:33:39.838+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:33:39.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.044 seconds
[2025-09-09T13:34:10.032+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:10.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:34:10.034+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:10.033+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:10.047+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:10.045+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 46, in <module>
    silver_group >> gold_transform >> load_jobs_to_motherduck
                    ^^^^^^^^^^^^^^
NameError: name 'gold_transform' is not defined
[2025-09-09T13:34:10.048+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:10.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.038 seconds
[2025-09-09T13:34:25.121+0000] {processor.py:161} INFO - Started process (PID=106) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:25.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:34:25.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:25.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:25.145+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:25.353+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:25.353+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:34:25.363+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:25.363+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:34:25.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.275 seconds
[2025-09-09T13:34:55.580+0000] {processor.py:161} INFO - Started process (PID=109) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:55.581+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:34:55.582+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:55.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:55.597+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:34:55.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:55.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:34:55.642+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:34:55.642+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:34:55.657+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T13:35:25.833+0000] {processor.py:161} INFO - Started process (PID=112) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:25.834+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:35:25.836+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:25.834+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:25.851+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:25.883+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:25.883+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:35:25.894+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:25.893+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:35:25.915+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T13:35:56.092+0000] {processor.py:161} INFO - Started process (PID=115) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:56.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:35:56.093+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:56.093+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:56.108+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:35:56.136+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:56.136+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:35:56.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:35:56.146+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:35:56.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T13:36:09.281+0000] {processor.py:161} INFO - Started process (PID=118) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:09.282+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:36:09.282+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:09.282+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:09.301+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:09.386+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:09.386+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:36:09.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:09.393+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:36:09.408+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.133 seconds
[2025-09-09T13:36:11.306+0000] {processor.py:161} INFO - Started process (PID=119) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:11.307+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:36:11.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:11.307+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:11.325+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:11.336+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:11.335+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:36:11.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:11.345+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:36:11.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T13:36:41.550+0000] {processor.py:161} INFO - Started process (PID=122) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:41.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:36:41.552+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:41.551+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:41.563+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:36:41.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:41.587+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:36:41.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:36:41.597+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:36:41.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T13:37:11.790+0000] {processor.py:161} INFO - Started process (PID=125) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:11.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:37:11.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:11.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:11.806+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:11.830+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:11.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:37:11.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:11.841+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:37:11.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:37:42.021+0000] {processor.py:161} INFO - Started process (PID=128) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:42.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:37:42.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:42.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:42.034+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:37:42.055+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:42.055+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:37:42.064+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:37:42.063+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:37:42.076+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T13:38:12.259+0000] {processor.py:161} INFO - Started process (PID=131) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:12.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:38:12.261+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:12.260+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:12.274+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:12.301+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:12.301+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:38:12.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:12.310+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:38:12.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:38:42.494+0000] {processor.py:161} INFO - Started process (PID=134) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:42.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:38:42.496+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:42.495+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:42.508+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:38:42.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:42.529+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:38:42.538+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:38:42.538+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:38:42.554+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T13:39:12.713+0000] {processor.py:161} INFO - Started process (PID=242) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:12.715+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:39:12.718+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:12.716+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:12.741+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:12.776+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:12.776+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:39:12.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:12.791+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:39:12.876+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.173 seconds
[2025-09-09T13:39:31.999+0000] {processor.py:161} INFO - Started process (PID=245) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:32.000+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:39:32.001+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:32.000+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:32.020+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:32.114+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:32.113+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:39:32.121+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:32.121+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:39:32.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.145 seconds
[2025-09-09T13:39:33.020+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:33.021+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:39:33.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:33.022+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:33.046+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:39:33.061+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:33.060+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:39:33.073+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:39:33.073+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:39:33.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T13:40:03.301+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:03.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:40:03.304+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:03.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:03.324+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:03.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:03.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:40:03.356+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:03.356+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:40:03.372+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T13:40:33.565+0000] {processor.py:161} INFO - Started process (PID=252) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:33.566+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:40:33.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:33.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:33.579+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:40:33.659+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:33.659+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:40:33.668+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:40:33.668+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:40:33.683+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.124 seconds
[2025-09-09T13:41:03.872+0000] {processor.py:161} INFO - Started process (PID=255) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:03.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:41:03.874+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:03.874+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:03.887+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:03.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:03.921+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:41:03.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:03.931+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:41:03.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T13:41:34.120+0000] {processor.py:161} INFO - Started process (PID=310) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:34.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:41:34.122+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:34.122+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:34.138+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:41:34.169+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:34.168+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:41:34.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:41:34.192+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:41:34.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T13:42:04.386+0000] {processor.py:161} INFO - Started process (PID=313) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:04.386+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:42:04.388+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:04.388+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:04.402+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:04.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:04.429+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:42:04.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:04.440+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:42:04.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T13:42:34.626+0000] {processor.py:161} INFO - Started process (PID=316) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:34.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:42:34.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:34.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:34.639+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:34.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:34.660+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:42:34.669+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:34.669+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:42:34.686+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T13:42:52.712+0000] {processor.py:161} INFO - Started process (PID=319) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:52.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:42:52.714+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:52.714+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:52.739+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:52.768+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:52.768+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:42:52.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:52.786+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:42:52.832+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.127 seconds
[2025-09-09T13:42:53.810+0000] {processor.py:161} INFO - Started process (PID=320) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:53.811+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:42:53.812+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:53.812+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:53.828+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:42:53.855+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:53.855+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:42:53.868+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:42:53.868+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:42:53.892+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T13:43:24.102+0000] {processor.py:161} INFO - Started process (PID=376) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:24.103+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:43:24.103+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:24.103+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:24.119+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:24.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:24.146+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:43:24.157+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:24.157+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:43:24.183+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T13:43:54.264+0000] {processor.py:161} INFO - Started process (PID=377) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:54.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:43:54.266+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:54.266+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:54.279+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:54.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:54.302+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:43:54.314+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:54.314+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:43:54.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T13:43:55.890+0000] {processor.py:161} INFO - Started process (PID=380) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:55.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:43:55.892+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:55.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:55.912+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:55.946+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:55.946+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:43:55.965+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:55.965+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:43:55.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T13:43:56.903+0000] {processor.py:161} INFO - Started process (PID=381) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:56.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:43:56.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:56.907+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:56.935+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:43:56.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:56.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:43:56.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:43:56.980+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:43:57.002+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.107 seconds
[2025-09-09T13:44:27.208+0000] {processor.py:161} INFO - Started process (PID=384) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:27.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:44:27.210+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:27.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:27.223+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:27.250+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:27.249+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:44:27.261+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:27.260+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:44:27.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T13:44:57.515+0000] {processor.py:161} INFO - Started process (PID=453) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:57.516+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:44:57.517+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:57.516+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:57.532+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:44:57.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:57.560+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:44:57.574+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:44:57.574+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:44:57.590+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T13:45:27.799+0000] {processor.py:161} INFO - Started process (PID=454) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:27.800+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:45:27.801+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:27.801+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:27.818+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:27.850+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:27.850+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:45:27.861+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:27.861+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:45:27.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T13:45:58.039+0000] {processor.py:161} INFO - Started process (PID=457) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:58.041+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:45:58.042+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:58.042+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:58.061+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:45:58.098+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:58.098+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:45:58.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:45:58.112+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:45:58.129+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T13:46:28.354+0000] {processor.py:161} INFO - Started process (PID=460) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:28.355+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:46:28.355+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:28.355+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:28.367+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:28.387+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:28.387+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:46:28.396+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:28.396+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:46:28.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T13:46:58.593+0000] {processor.py:161} INFO - Started process (PID=463) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:58.594+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:46:58.594+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:58.594+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:58.606+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:46:58.631+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:58.631+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:46:58.642+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:46:58.641+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:46:58.656+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T13:47:28.857+0000] {processor.py:161} INFO - Started process (PID=466) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:47:28.859+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:47:28.860+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:47:28.860+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:47:28.874+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:47:28.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:47:28.897+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:47:28.909+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:47:28.909+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:47:28.931+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.089 seconds
[2025-09-09T13:48:20.810+0000] {processor.py:161} INFO - Started process (PID=20) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:20.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:48:20.827+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:20.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:20.899+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:20.995+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:20.993+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:48:21.019+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:21.019+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:48:21.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.270 seconds
[2025-09-09T13:48:51.463+0000] {processor.py:161} INFO - Started process (PID=77) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:51.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:48:51.482+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:51.480+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:51.581+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:48:51.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:51.823+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:48:51.840+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:48:51.840+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:48:51.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.463 seconds
[2025-09-09T13:49:22.684+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:22.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:49:22.690+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:22.689+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:22.710+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:22.744+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:22.744+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:49:22.757+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:22.757+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:49:22.820+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.146 seconds
[2025-09-09T13:49:53.015+0000] {processor.py:161} INFO - Started process (PID=99) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:53.016+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:49:53.017+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:53.017+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:53.028+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:49:53.053+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:53.053+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:49:53.062+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:49:53.062+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:49:53.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T13:50:23.250+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:23.253+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:50:23.256+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:23.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:23.278+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:23.314+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:23.314+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:50:23.330+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:23.330+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:50:23.350+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.112 seconds
[2025-09-09T13:50:53.518+0000] {processor.py:161} INFO - Started process (PID=105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:53.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:50:53.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:53.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:53.542+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:50:53.583+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:53.582+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:50:53.599+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:50:53.599+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:50:53.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.109 seconds
[2025-09-09T13:51:23.799+0000] {processor.py:161} INFO - Started process (PID=108) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:51:23.800+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:51:23.802+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:51:23.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:51:23.814+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:51:23.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:51:23.834+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:51:23.843+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:51:23.843+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:51:23.857+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T13:52:02.741+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:02.750+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:52:02.765+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:02.762+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:02.804+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:02.836+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:02.836+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:52:02.849+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:02.849+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:52:02.878+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.190 seconds
[2025-09-09T13:52:33.152+0000] {processor.py:161} INFO - Started process (PID=23) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:33.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:52:33.159+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:33.158+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:33.185+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:52:33.405+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:33.405+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:52:33.420+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:52:33.420+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:52:33.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.308 seconds
[2025-09-09T13:53:03.491+0000] {processor.py:161} INFO - Started process (PID=25) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:03.493+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:53:03.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:03.493+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:03.513+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:03.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:03.539+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:53:03.550+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:03.550+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:53:03.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T13:53:33.977+0000] {processor.py:161} INFO - Started process (PID=96) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:33.979+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:53:33.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:33.980+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:34.004+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:53:34.035+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:34.035+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:53:34.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:53:34.051+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:53:34.068+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.107 seconds
[2025-09-09T13:54:04.234+0000] {processor.py:161} INFO - Started process (PID=98) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:04.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:54:04.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:04.236+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:04.251+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:04.271+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:04.271+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:54:04.281+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:04.281+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:54:04.294+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T13:54:34.448+0000] {processor.py:161} INFO - Started process (PID=100) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:34.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:54:34.450+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:34.450+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:34.465+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:54:34.488+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:34.488+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:54:34.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:54:34.499+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:54:34.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T13:55:04.970+0000] {processor.py:161} INFO - Started process (PID=102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:04.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:55:04.984+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:04.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:05.112+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:05.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:05.193+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:55:05.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:05.251+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:55:05.315+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.448 seconds
[2025-09-09T13:55:32.040+0000] {processor.py:161} INFO - Started process (PID=21) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:32.043+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:55:32.045+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:32.045+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:32.076+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:55:32.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:32.145+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:55:32.172+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:55:32.172+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:55:32.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.166 seconds
[2025-09-09T13:56:02.466+0000] {processor.py:161} INFO - Started process (PID=82) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:02.468+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:56:02.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:02.473+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:02.515+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:02.700+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:02.700+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:56:02.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:02.711+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:56:02.741+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.291 seconds
[2025-09-09T13:56:32.924+0000] {processor.py:161} INFO - Started process (PID=94) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:32.925+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:56:32.927+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:32.927+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:32.942+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:32.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:32.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:56:32.977+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:32.977+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:56:32.991+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T13:56:42.136+0000] {processor.py:161} INFO - Started process (PID=97) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:42.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:56:42.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:42.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:42.167+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:56:42.242+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:42.242+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:56:42.248+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:56:42.248+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:56:42.276+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.145 seconds
[2025-09-09T13:57:12.346+0000] {processor.py:161} INFO - Started process (PID=243) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:12.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:57:12.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:12.349+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:12.361+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:12.384+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:12.384+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:57:12.395+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:12.395+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:57:12.410+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T13:57:42.584+0000] {processor.py:161} INFO - Started process (PID=246) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:42.585+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:57:42.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:42.586+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:42.598+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:57:42.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:42.620+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:57:42.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:57:42.633+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:57:42.651+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T13:58:12.816+0000] {processor.py:161} INFO - Started process (PID=249) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:12.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:58:12.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:12.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:12.832+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:12.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:12.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:58:12.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:12.869+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:58:12.887+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T13:58:43.028+0000] {processor.py:161} INFO - Started process (PID=252) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:43.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:58:43.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:43.030+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:43.046+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:58:43.069+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:43.069+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:58:43.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:58:43.079+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:58:43.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T13:59:13.195+0000] {processor.py:161} INFO - Started process (PID=253) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:13.196+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:59:13.197+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:13.197+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:13.209+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:13.234+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:13.234+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:59:13.243+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:13.243+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:59:13.258+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T13:59:43.462+0000] {processor.py:161} INFO - Started process (PID=256) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:43.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T13:59:43.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:43.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:43.483+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T13:59:43.506+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:43.506+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T13:59:43.516+0000] {logging_mixin.py:188} INFO - [2025-09-09T13:59:43.516+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T13:59:43.532+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T14:00:13.681+0000] {processor.py:161} INFO - Started process (PID=259) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:13.682+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:00:13.683+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:13.683+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:13.698+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:13.720+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:13.720+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:00:13.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:13.728+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:00:13.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:00:43.904+0000] {processor.py:161} INFO - Started process (PID=262) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:43.905+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:00:43.913+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:43.913+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:43.943+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:00:43.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:43.976+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:00:43.987+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:00:43.987+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:00:44.122+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.224 seconds
[2025-09-09T14:01:14.301+0000] {processor.py:161} INFO - Started process (PID=265) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:14.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:01:14.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:14.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:14.315+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:14.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:14.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:01:14.460+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:14.460+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:01:14.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.179 seconds
[2025-09-09T14:01:44.649+0000] {processor.py:161} INFO - Started process (PID=268) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:44.649+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:01:44.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:44.651+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:44.664+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:01:44.792+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:44.792+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:01:44.799+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:01:44.798+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:01:44.811+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.168 seconds
[2025-09-09T14:02:02.941+0000] {processor.py:161} INFO - Started process (PID=271) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:02.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:02:02.944+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:02.944+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:02.963+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:03.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:03.233+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:02:03.240+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:03.240+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:02:03.254+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.319 seconds
[2025-09-09T14:02:03.953+0000] {processor.py:161} INFO - Started process (PID=272) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:03.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:02:03.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:03.955+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:04.092+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:04.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:04.101+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:02:04.114+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:04.114+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:02:04.174+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.227 seconds
[2025-09-09T14:02:34.485+0000] {processor.py:161} INFO - Started process (PID=347) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:34.489+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:02:34.491+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:34.490+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:34.508+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:02:34.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:34.544+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:02:34.561+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:02:34.560+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:02:34.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T14:03:04.784+0000] {processor.py:161} INFO - Started process (PID=446) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:04.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:03:04.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:04.786+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:04.798+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:04.822+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:04.822+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:03:04.833+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:04.833+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:03:04.850+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T14:03:35.026+0000] {processor.py:161} INFO - Started process (PID=449) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:35.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:03:35.029+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:35.028+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:35.042+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:03:35.065+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:35.065+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:03:35.077+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:03:35.076+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:03:35.095+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T14:04:03.289+0000] {processor.py:161} INFO - Started process (PID=452) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:03.290+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:04:03.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:03.291+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:03.310+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:03.420+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:03.419+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:04:03.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:03.427+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:04:03.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.166 seconds
[2025-09-09T14:04:33.707+0000] {processor.py:161} INFO - Started process (PID=606) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:33.721+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:04:33.731+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:33.730+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:33.782+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:04:33.844+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:33.844+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:04:33.886+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:04:33.886+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:04:33.971+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.317 seconds
[2025-09-09T14:05:04.282+0000] {processor.py:161} INFO - Started process (PID=609) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:04.283+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:05:04.285+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:04.284+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:04.299+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:04.323+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:04.323+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:05:04.336+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:04.336+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:05:04.353+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T14:05:34.520+0000] {processor.py:161} INFO - Started process (PID=612) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:34.521+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:05:34.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:34.522+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:34.535+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:05:34.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:34.560+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:05:34.572+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:05:34.572+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:05:34.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T14:06:04.736+0000] {processor.py:161} INFO - Started process (PID=615) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:04.737+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:06:04.738+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:04.738+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:04.750+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:04.772+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:04.772+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:06:04.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:04.782+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:06:04.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:06:34.910+0000] {processor.py:161} INFO - Started process (PID=616) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:34.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:06:34.915+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:34.914+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:34.930+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:06:34.956+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:34.956+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:06:34.968+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:06:34.968+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:06:34.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T14:07:05.192+0000] {processor.py:161} INFO - Started process (PID=619) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:05.193+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:07:05.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:05.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:05.208+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:05.231+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:05.231+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:07:05.241+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:05.241+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:07:05.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T14:07:35.456+0000] {processor.py:161} INFO - Started process (PID=622) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:35.457+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:07:35.459+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:35.458+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:35.471+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:07:35.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:35.494+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:07:35.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:07:35.503+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:07:35.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T14:08:05.688+0000] {processor.py:161} INFO - Started process (PID=625) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:05.689+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:08:05.690+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:05.690+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:05.701+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:05.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:05.721+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:08:05.730+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:05.730+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:08:05.743+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T14:08:35.934+0000] {processor.py:161} INFO - Started process (PID=628) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:35.935+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:08:35.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:35.936+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:35.947+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:08:35.968+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:35.968+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:08:35.978+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:08:35.978+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:08:35.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T14:09:06.187+0000] {processor.py:161} INFO - Started process (PID=631) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:06.188+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:09:06.189+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:06.189+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:06.202+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:06.224+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:06.223+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:09:06.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:06.232+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:09:06.245+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T14:09:36.404+0000] {processor.py:161} INFO - Started process (PID=634) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:36.405+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:09:36.407+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:36.407+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:36.422+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:09:36.448+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:36.448+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:09:36.460+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:09:36.460+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:09:36.478+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T14:10:06.641+0000] {processor.py:161} INFO - Started process (PID=637) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:06.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:10:06.645+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:06.645+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:06.667+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:06.720+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:06.720+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:10:06.735+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:06.735+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:10:06.766+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.134 seconds
[2025-09-09T14:10:36.993+0000] {processor.py:161} INFO - Started process (PID=640) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:36.995+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:10:36.998+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:36.997+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:37.022+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:10:37.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:37.056+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:10:37.069+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:10:37.068+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:10:37.088+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T14:11:07.267+0000] {processor.py:161} INFO - Started process (PID=643) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:07.269+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:11:07.271+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:07.270+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:07.285+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:07.309+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:07.309+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:11:07.319+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:07.319+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:11:07.335+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T14:11:37.635+0000] {processor.py:161} INFO - Started process (PID=646) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:37.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:11:37.641+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:37.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:37.653+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:11:37.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:37.682+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:11:37.699+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:11:37.699+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:11:37.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.100 seconds
[2025-09-09T14:12:07.902+0000] {processor.py:161} INFO - Started process (PID=649) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:07.903+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:12:07.904+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:07.904+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:07.918+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:07.943+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:07.942+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:12:07.952+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:07.952+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:12:07.966+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T14:12:38.132+0000] {processor.py:161} INFO - Started process (PID=652) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:38.133+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:12:38.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:38.134+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:38.151+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:12:38.187+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:38.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:12:38.202+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:12:38.201+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:12:38.223+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T14:13:08.391+0000] {processor.py:161} INFO - Started process (PID=655) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:08.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:13:08.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:08.393+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:08.406+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:08.431+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:08.430+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:13:08.440+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:08.440+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:13:08.454+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T14:13:38.659+0000] {processor.py:161} INFO - Started process (PID=658) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:38.660+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:13:38.661+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:38.661+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:38.675+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:13:38.697+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:38.697+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:13:38.707+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:13:38.707+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:13:38.724+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T14:14:08.932+0000] {processor.py:161} INFO - Started process (PID=661) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:08.932+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:14:08.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:08.933+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:08.944+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:08.969+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:08.968+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:14:08.982+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:08.981+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:14:08.996+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T14:14:39.218+0000] {processor.py:161} INFO - Started process (PID=664) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:39.219+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:14:39.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:39.220+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:39.234+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:14:39.255+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:39.255+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:14:39.263+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:14:39.263+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:14:39.277+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:15:09.481+0000] {processor.py:161} INFO - Started process (PID=667) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:09.482+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:15:09.484+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:09.484+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:09.496+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:09.517+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:09.517+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:15:09.526+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:09.526+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:15:09.539+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:15:39.761+0000] {processor.py:161} INFO - Started process (PID=670) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:39.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:15:39.765+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:39.764+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:39.782+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:15:39.813+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:39.813+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:15:39.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:15:39.826+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:15:39.847+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T14:16:10.016+0000] {processor.py:161} INFO - Started process (PID=673) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:10.017+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:16:10.019+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:10.019+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:10.030+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:10.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:10.051+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:16:10.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:10.060+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:16:10.073+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:16:40.225+0000] {processor.py:161} INFO - Started process (PID=676) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:40.226+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:16:40.227+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:40.227+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:40.238+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:16:40.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:40.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:16:40.268+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:16:40.268+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:16:40.280+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T14:17:10.425+0000] {processor.py:161} INFO - Started process (PID=679) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:10.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:17:10.428+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:10.428+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:10.440+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:10.462+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:10.461+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:17:10.471+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:10.471+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:17:10.487+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T14:17:40.649+0000] {processor.py:161} INFO - Started process (PID=682) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:40.650+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:17:40.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:40.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:40.664+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:17:40.684+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:40.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:17:40.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:17:40.693+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:17:40.706+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:18:10.855+0000] {processor.py:161} INFO - Started process (PID=685) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:10.856+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:18:10.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:10.857+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:10.869+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:10.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:10.890+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:18:10.901+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:10.901+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:18:10.917+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:18:41.069+0000] {processor.py:161} INFO - Started process (PID=688) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:41.069+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:18:41.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:41.070+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:41.082+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:18:41.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:41.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:18:41.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:18:41.112+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:18:41.127+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T14:19:11.295+0000] {processor.py:161} INFO - Started process (PID=691) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:11.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:19:11.297+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:11.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:11.308+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:11.334+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:11.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:19:11.344+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:11.343+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:19:11.359+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T14:19:41.517+0000] {processor.py:161} INFO - Started process (PID=694) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:41.518+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:19:41.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:41.519+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:41.533+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:19:41.554+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:41.554+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:19:41.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:19:41.563+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:19:41.578+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T14:20:11.717+0000] {processor.py:161} INFO - Started process (PID=697) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:11.717+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:20:11.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:11.718+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:11.730+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:11.750+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:11.750+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:20:11.759+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:11.759+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:20:11.772+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T14:20:41.929+0000] {processor.py:161} INFO - Started process (PID=700) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:41.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:20:41.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:41.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:41.946+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:20:41.970+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:41.970+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:20:41.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:20:41.980+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:20:41.995+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T14:21:12.137+0000] {processor.py:161} INFO - Started process (PID=703) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:12.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:21:12.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:12.139+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:12.152+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:12.173+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:12.173+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:21:12.182+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:12.182+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:21:12.195+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T14:21:42.348+0000] {processor.py:161} INFO - Started process (PID=706) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:42.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:21:42.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:42.350+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:42.362+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:21:42.384+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:42.383+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:21:42.393+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:21:42.393+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:21:42.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T14:22:12.501+0000] {processor.py:161} INFO - Started process (PID=707) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:12.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:22:12.504+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:12.503+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:12.517+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:12.542+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:12.542+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:22:12.553+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:12.553+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:22:12.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T14:22:42.752+0000] {processor.py:161} INFO - Started process (PID=710) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:42.753+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:22:42.754+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:42.754+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:42.766+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:22:42.788+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:42.788+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:22:42.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:22:42.797+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:22:42.811+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:23:12.993+0000] {processor.py:161} INFO - Started process (PID=713) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:12.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:23:12.995+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:12.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:13.007+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:13.030+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:13.030+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:23:13.040+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:13.040+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:23:13.054+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:23:43.297+0000] {processor.py:161} INFO - Started process (PID=716) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:43.298+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:23:43.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:43.300+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:43.315+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:23:43.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:43.340+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:23:43.353+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:23:43.353+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:23:43.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T14:24:13.585+0000] {processor.py:161} INFO - Started process (PID=719) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:13.585+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:24:13.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:13.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:13.597+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:13.622+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:13.622+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:24:13.632+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:13.632+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:24:13.647+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T14:24:43.844+0000] {processor.py:161} INFO - Started process (PID=722) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:43.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:24:43.847+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:43.847+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:43.865+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:24:43.890+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:43.889+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:24:43.901+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:24:43.901+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:24:43.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T14:25:14.092+0000] {processor.py:161} INFO - Started process (PID=725) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:14.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:25:14.094+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:14.094+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:14.106+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:14.126+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:14.126+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:25:14.135+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:14.135+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:25:14.150+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T14:25:44.368+0000] {processor.py:161} INFO - Started process (PID=728) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:44.369+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:25:44.370+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:44.370+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:44.385+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:25:44.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:44.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:25:44.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:25:44.435+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:25:44.448+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T14:26:14.615+0000] {processor.py:161} INFO - Started process (PID=731) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:14.616+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:26:14.617+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:14.617+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:14.631+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:14.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:14.650+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:26:14.659+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:14.659+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:26:14.672+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:26:44.846+0000] {processor.py:161} INFO - Started process (PID=734) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:44.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:26:44.848+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:44.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:44.860+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:26:44.880+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:44.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:26:44.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:26:44.888+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:26:44.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T14:27:15.112+0000] {processor.py:161} INFO - Started process (PID=737) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:15.113+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:27:15.114+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:15.114+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:15.126+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:15.147+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:15.147+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:27:15.156+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:15.155+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:27:15.169+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:27:45.333+0000] {processor.py:161} INFO - Started process (PID=740) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:45.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:27:45.335+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:45.334+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:45.346+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:27:45.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:45.366+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:27:45.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:27:45.375+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:27:45.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T14:28:15.537+0000] {processor.py:161} INFO - Started process (PID=743) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:15.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:28:15.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:15.539+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:15.550+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:15.583+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:15.583+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:28:15.593+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:15.593+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:28:15.607+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T14:28:45.784+0000] {processor.py:161} INFO - Started process (PID=746) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:45.784+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:28:45.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:45.786+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:45.797+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:28:45.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:45.816+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:28:45.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:28:45.826+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:28:45.841+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:29:15.993+0000] {processor.py:161} INFO - Started process (PID=749) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:15.994+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:29:15.995+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:15.995+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:16.007+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:16.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:16.027+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:29:16.036+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:16.036+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:29:16.048+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T14:29:46.211+0000] {processor.py:161} INFO - Started process (PID=752) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:46.212+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:29:46.213+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:46.213+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:46.227+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:29:46.250+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:46.250+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:29:46.260+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:29:46.260+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:29:46.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T14:30:16.455+0000] {processor.py:161} INFO - Started process (PID=755) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:16.456+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:30:16.458+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:16.457+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:16.469+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:16.490+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:16.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:30:16.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:16.499+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:30:16.518+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:30:46.709+0000] {processor.py:161} INFO - Started process (PID=758) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:46.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:30:46.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:46.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:46.721+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:30:46.744+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:46.744+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:30:46.753+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:30:46.753+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:30:46.767+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T14:31:16.923+0000] {processor.py:161} INFO - Started process (PID=761) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:16.923+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:31:16.925+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:16.924+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:16.937+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:16.960+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:16.960+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:31:16.970+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:16.970+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:31:16.985+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:31:47.188+0000] {processor.py:161} INFO - Started process (PID=764) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:47.189+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:31:47.190+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:47.190+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:47.200+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:31:47.220+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:47.219+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:31:47.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:31:47.228+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:31:47.241+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T14:32:17.425+0000] {processor.py:161} INFO - Started process (PID=767) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:17.426+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:32:17.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:17.427+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:17.439+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:17.460+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:17.460+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:32:17.470+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:17.470+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:32:17.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T14:32:47.646+0000] {processor.py:161} INFO - Started process (PID=770) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:47.647+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:32:47.649+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:47.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:47.664+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:32:47.690+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:47.689+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:32:47.700+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:32:47.700+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:32:47.715+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T14:33:17.883+0000] {processor.py:161} INFO - Started process (PID=773) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:17.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:33:17.886+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:17.886+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:17.903+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:17.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:17.932+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:33:17.948+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:17.947+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:33:17.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T14:33:48.173+0000] {processor.py:161} INFO - Started process (PID=776) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:48.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:33:48.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:48.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:48.193+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:33:48.218+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:48.218+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:33:48.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:33:48.228+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:33:48.242+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T14:34:18.392+0000] {processor.py:161} INFO - Started process (PID=779) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:18.392+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:34:18.394+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:18.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:18.406+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:18.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:18.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:34:18.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:18.436+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:34:18.449+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:34:48.631+0000] {processor.py:161} INFO - Started process (PID=782) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:48.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:34:48.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:48.632+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:48.643+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:34:48.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:48.663+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:34:48.671+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:34:48.671+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:34:48.684+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T14:35:18.868+0000] {processor.py:161} INFO - Started process (PID=934) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:18.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:35:18.875+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:18.874+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:18.923+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:18.977+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:18.977+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:35:18.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:18.994+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:35:19.021+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.176 seconds
[2025-09-09T14:35:49.201+0000] {processor.py:161} INFO - Started process (PID=937) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:49.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:35:49.204+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:49.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:49.216+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:35:49.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:49.236+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:35:49.245+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:35:49.244+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:35:49.258+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T14:36:19.441+0000] {processor.py:161} INFO - Started process (PID=940) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:19.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:36:19.443+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:19.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:19.454+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:19.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:19.474+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:36:19.485+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:19.484+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:36:19.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T14:36:49.658+0000] {processor.py:161} INFO - Started process (PID=943) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:49.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:36:49.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:49.660+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:49.672+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:36:49.692+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:49.692+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:36:49.701+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:36:49.701+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:36:49.714+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T14:37:19.869+0000] {processor.py:161} INFO - Started process (PID=944) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:19.870+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:37:19.872+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:19.872+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:19.887+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:19.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:19.910+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:37:19.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:19.921+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:37:19.936+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T14:37:50.108+0000] {processor.py:161} INFO - Started process (PID=947) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:50.109+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:37:50.110+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:50.110+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:50.123+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:37:50.150+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:50.149+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:37:50.161+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:37:50.161+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:37:50.176+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T14:38:20.363+0000] {processor.py:161} INFO - Started process (PID=950) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:20.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:38:20.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:20.366+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:20.378+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:20.400+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:20.400+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:38:20.410+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:20.410+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:38:20.424+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T14:38:50.631+0000] {processor.py:161} INFO - Started process (PID=953) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:50.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:38:50.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:50.632+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:50.645+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:38:50.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:50.674+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:38:50.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:38:50.693+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:38:50.718+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T14:39:20.895+0000] {processor.py:161} INFO - Started process (PID=956) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:20.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:39:20.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:20.897+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:20.910+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:20.935+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:20.935+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:39:20.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:20.944+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:39:20.957+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:39:51.201+0000] {processor.py:161} INFO - Started process (PID=959) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:51.202+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:39:51.204+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:51.203+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:51.217+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:39:51.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:51.339+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:39:51.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:39:51.350+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:39:51.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.176 seconds
[2025-09-09T14:40:21.677+0000] {processor.py:161} INFO - Started process (PID=962) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:21.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:40:21.680+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:21.680+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:21.697+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:21.739+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:21.738+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:40:21.751+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:21.750+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:40:21.768+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.102 seconds
[2025-09-09T14:40:51.928+0000] {processor.py:161} INFO - Started process (PID=965) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:51.929+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:40:51.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:51.931+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:51.970+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:40:52.015+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:52.014+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:40:52.027+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:40:52.027+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:40:52.044+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.125 seconds
[2025-09-09T14:41:22.199+0000] {processor.py:161} INFO - Started process (PID=968) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:22.200+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:41:22.202+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:22.202+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:22.215+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:22.239+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:22.238+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:41:22.250+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:22.250+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:41:22.264+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T14:41:37.954+0000] {processor.py:161} INFO - Started process (PID=971) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:37.955+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:41:37.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:37.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:37.975+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:41:38.131+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:38.131+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:41:38.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:41:38.137+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:41:38.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.208 seconds
[2025-09-09T14:42:08.456+0000] {processor.py:161} INFO - Started process (PID=1045) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:08.478+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:42:08.499+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:08.489+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:08.651+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:08.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:08.916+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:42:08.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:08.975+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:42:09.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.605 seconds
[2025-09-09T14:42:41.970+0000] {processor.py:161} INFO - Started process (PID=1125) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:41.998+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:42:42.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:42.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:42.764+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:42:43.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:43.631+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:42:44.129+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:42:44.127+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:42:46.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.209 seconds
[2025-09-09T14:43:17.251+0000] {processor.py:161} INFO - Started process (PID=1150) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:17.254+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:43:17.255+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:17.255+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:17.288+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:17.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:17.379+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:43:17.406+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:17.406+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:43:17.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.218 seconds
[2025-09-09T14:43:47.618+0000] {processor.py:161} INFO - Started process (PID=1153) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:47.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:43:47.621+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:47.621+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:47.636+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:43:47.681+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:47.680+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:43:47.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:43:47.692+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:43:47.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.097 seconds
[2025-09-09T14:44:18.132+0000] {processor.py:161} INFO - Started process (PID=1210) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:18.136+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:44:18.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:18.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:18.336+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:18.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:18.664+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:44:18.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:18.756+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:44:18.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.810 seconds
[2025-09-09T14:44:49.482+0000] {processor.py:161} INFO - Started process (PID=1306) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:49.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:44:49.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:49.595+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:49.889+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:44:50.220+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:50.220+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:44:50.336+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:44:50.335+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:44:50.480+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.214 seconds
[2025-09-09T14:45:20.674+0000] {processor.py:161} INFO - Started process (PID=1309) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:20.679+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:45:20.688+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:20.686+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:20.751+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:20.842+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:20.842+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:45:20.876+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:20.875+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:45:20.899+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.241 seconds
[2025-09-09T14:45:51.082+0000] {processor.py:161} INFO - Started process (PID=1312) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:51.083+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:45:51.084+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:51.084+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:51.098+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:45:51.129+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:51.129+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:45:51.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:45:51.140+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:45:51.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T14:46:21.315+0000] {processor.py:161} INFO - Started process (PID=1315) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:21.316+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:46:21.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:21.318+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:21.330+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:21.355+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:21.355+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:46:21.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:21.365+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:46:21.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T14:46:51.543+0000] {processor.py:161} INFO - Started process (PID=1450) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:51.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:46:51.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:51.566+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:51.674+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:46:51.784+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:51.784+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:46:51.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:46:51.824+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:46:51.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.357 seconds
[2025-09-09T14:47:22.719+0000] {processor.py:161} INFO - Started process (PID=1497) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:22.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:47:22.730+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:22.727+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:22.785+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:22.819+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:22.819+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:47:22.836+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:22.835+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:47:22.860+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.157 seconds
[2025-09-09T14:47:52.965+0000] {processor.py:161} INFO - Started process (PID=1500) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:52.967+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:47:52.971+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:52.970+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:52.984+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:47:53.009+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:53.009+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:47:53.020+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:47:53.020+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:47:53.035+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T14:48:23.194+0000] {processor.py:161} INFO - Started process (PID=1503) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:23.195+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:48:23.197+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:23.197+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:23.209+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:23.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:23.232+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:48:23.243+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:23.243+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:48:23.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T14:48:53.433+0000] {processor.py:161} INFO - Started process (PID=1506) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:53.434+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:48:53.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:53.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:53.448+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:48:53.472+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:53.472+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:48:53.482+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:48:53.482+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:48:53.495+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T14:49:25.195+0000] {processor.py:161} INFO - Started process (PID=1638) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:25.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:49:25.333+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:25.317+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:25.836+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:26.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:26.544+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:49:26.815+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:26.815+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:49:27.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.425 seconds
[2025-09-09T14:49:57.927+0000] {processor.py:161} INFO - Started process (PID=1644) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:57.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:49:57.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:57.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:58.170+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:49:58.723+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:58.723+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:49:58.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:49:58.826+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:49:58.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.210 seconds
[2025-09-09T14:50:29.226+0000] {processor.py:161} INFO - Started process (PID=1673) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:29.228+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:50:29.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:29.231+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:29.249+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:29.290+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:29.290+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:50:29.304+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:29.304+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:50:29.323+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.113 seconds
[2025-09-09T14:50:59.483+0000] {processor.py:161} INFO - Started process (PID=1676) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:59.484+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:50:59.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:59.486+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:59.502+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:50:59.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:59.542+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:50:59.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:50:59.557+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:50:59.581+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T14:51:29.661+0000] {processor.py:161} INFO - Started process (PID=1679) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:29.662+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:51:29.663+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:29.663+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:29.679+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:29.707+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:29.707+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:51:29.718+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:29.718+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:51:29.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T14:51:50.845+0000] {processor.py:161} INFO - Started process (PID=1680) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:50.846+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:51:50.848+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:50.848+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:50.869+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:51:51.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:51.022+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:51:51.029+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:51:51.029+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:51:51.046+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.207 seconds
[2025-09-09T14:52:21.941+0000] {processor.py:161} INFO - Started process (PID=1815) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:21.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:52:22.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:22.092+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:23.780+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:24.916+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:24.915+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:52:25.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:25.360+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:52:25.879+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.228 seconds
[2025-09-09T14:52:57.724+0000] {processor.py:161} INFO - Started process (PID=1848) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:57.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:52:57.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:57.873+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:58.739+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:52:59.461+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:59.461+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:52:59.572+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:52:59.572+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:52:59.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.416 seconds
[2025-09-09T14:53:09.933+0000] {processor.py:161} INFO - Started process (PID=1849) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:09.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:53:09.943+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:09.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:09.967+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:10.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:10.469+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:53:10.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:10.480+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:53:10.513+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.612 seconds
[2025-09-09T14:53:41.276+0000] {processor.py:161} INFO - Started process (PID=1852) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:41.277+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:53:41.280+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:41.280+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:41.302+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:53:41.342+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:41.342+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:53:41.354+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:53:41.354+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:53:41.371+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.103 seconds
[2025-09-09T14:54:11.523+0000] {processor.py:161} INFO - Started process (PID=1855) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:11.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:54:11.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:11.525+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:11.538+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:11.588+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:11.588+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:54:11.613+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:11.613+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:54:11.639+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.123 seconds
[2025-09-09T14:54:41.795+0000] {processor.py:161} INFO - Started process (PID=1858) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:41.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:54:41.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:41.797+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:41.811+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:54:41.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:41.832+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:54:41.842+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:54:41.841+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:54:41.856+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T14:55:12.019+0000] {processor.py:161} INFO - Started process (PID=1861) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:12.020+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:55:12.021+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:12.021+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:12.033+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:12.059+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:12.058+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:55:12.075+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:12.075+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:55:12.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T14:55:42.323+0000] {processor.py:161} INFO - Started process (PID=1864) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:42.324+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:55:42.326+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:42.326+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:42.340+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:55:42.380+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:42.380+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:55:42.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:55:42.392+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:55:42.412+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T14:56:12.585+0000] {processor.py:161} INFO - Started process (PID=1867) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:12.586+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:56:12.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:12.587+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:12.599+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:12.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:12.620+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:56:12.630+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:12.629+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:56:12.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T14:56:29.377+0000] {processor.py:161} INFO - Started process (PID=1870) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:29.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:56:29.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:29.379+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:29.398+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:29.501+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:29.501+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:56:29.508+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:29.508+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:56:29.598+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.227 seconds
[2025-09-09T14:56:59.771+0000] {processor.py:161} INFO - Started process (PID=1949) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:59.777+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:56:59.784+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:59.783+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:59.814+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:56:59.876+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:59.875+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:56:59.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:56:59.890+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:56:59.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.174 seconds
[2025-09-09T14:57:34.938+0000] {processor.py:161} INFO - Started process (PID=2016) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:57:35.007+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:57:35.384+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:57:35.277+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:57:36.937+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:57:39.230+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:57:39.228+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:57:41.209+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:57:41.206+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:57:43.874+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 10.969 seconds
[2025-09-09T14:58:16.666+0000] {processor.py:161} INFO - Started process (PID=2024) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:16.710+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:58:16.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:16.795+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:17.166+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:17.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:17.481+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:58:17.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:17.522+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:58:17.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.351 seconds
[2025-09-09T14:58:47.826+0000] {processor.py:161} INFO - Started process (PID=2050) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:47.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:58:47.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:47.829+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:47.844+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:47.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:47.871+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:58:47.884+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:47.884+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:58:47.902+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T14:58:52.958+0000] {processor.py:161} INFO - Started process (PID=2051) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:52.960+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:58:52.963+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:52.963+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:52.998+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:58:53.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:53.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:58:53.234+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:58:53.234+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:58:53.260+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.312 seconds
[2025-09-09T14:59:23.504+0000] {processor.py:161} INFO - Started process (PID=2127) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:23.507+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:59:23.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:23.509+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:23.526+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:23.554+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:23.554+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:59:23.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:23.566+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:59:23.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T14:59:53.775+0000] {processor.py:161} INFO - Started process (PID=2130) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:53.776+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T14:59:53.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:53.777+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:53.790+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T14:59:53.811+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:53.811+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T14:59:53.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T14:59:53.820+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T14:59:53.853+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T15:00:24.020+0000] {processor.py:161} INFO - Started process (PID=2133) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:24.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:00:24.024+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:24.024+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:24.042+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:24.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:24.070+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:00:24.082+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:24.082+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:00:24.102+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T15:00:54.493+0000] {processor.py:161} INFO - Started process (PID=2136) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:54.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:00:54.534+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:54.534+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:55.902+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:00:56.617+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:56.616+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:00:57.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:00:57.080+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:00:57.329+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.947 seconds
[2025-09-09T15:01:27.565+0000] {processor.py:161} INFO - Started process (PID=2137) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:27.567+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:01:27.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:27.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:27.584+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:27.612+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:27.611+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:01:27.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:27.620+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:01:27.638+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T15:01:57.827+0000] {processor.py:161} INFO - Started process (PID=2140) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:57.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:01:57.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:57.829+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:57.842+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:01:57.867+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:57.867+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:01:57.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:01:57.879+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:01:57.893+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T15:02:28.056+0000] {processor.py:161} INFO - Started process (PID=2143) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:28.057+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:02:28.058+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:28.058+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:28.070+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:28.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:28.091+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:02:28.100+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:28.100+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:02:28.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T15:02:58.296+0000] {processor.py:161} INFO - Started process (PID=2146) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:58.297+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:02:58.298+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:58.298+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:58.334+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:02:58.360+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:58.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:02:58.371+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:02:58.371+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:02:58.391+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.102 seconds
[2025-09-09T15:03:28.581+0000] {processor.py:161} INFO - Started process (PID=2149) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:28.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:03:28.584+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:28.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:28.597+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:28.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:28.619+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:03:28.630+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:28.630+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:03:28.643+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:03:58.799+0000] {processor.py:161} INFO - Started process (PID=2152) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:58.800+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:03:58.802+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:58.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:58.815+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:03:58.839+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:58.839+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:03:58.849+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:03:58.849+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:03:58.865+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T15:04:29.024+0000] {processor.py:161} INFO - Started process (PID=2155) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:29.025+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:04:29.026+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:29.026+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:29.039+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:29.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:29.060+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:04:29.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:29.071+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:04:29.085+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T15:04:59.263+0000] {processor.py:161} INFO - Started process (PID=2158) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:59.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:04:59.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:59.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:59.278+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:04:59.299+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:59.299+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:04:59.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:04:59.308+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:04:59.325+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:05:29.473+0000] {processor.py:161} INFO - Started process (PID=2161) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:29.474+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:05:29.476+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:29.476+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:29.489+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:29.511+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:29.511+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:05:29.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:29.520+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:05:29.535+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T15:05:59.676+0000] {processor.py:161} INFO - Started process (PID=2164) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:59.677+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:05:59.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:59.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:59.691+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:05:59.712+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:59.712+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:05:59.726+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:05:59.725+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:05:59.747+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T15:06:29.929+0000] {processor.py:161} INFO - Started process (PID=2167) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:06:29.931+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:06:29.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:06:29.934+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:06:29.949+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:06:29.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:06:29.985+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:06:30.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:06:30.000+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:06:30.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.101 seconds
[2025-09-09T15:07:00.206+0000] {processor.py:161} INFO - Started process (PID=2170) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:00.208+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:07:00.212+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:00.212+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:00.231+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:00.272+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:00.272+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:07:00.284+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:00.284+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:07:00.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.110 seconds
[2025-09-09T15:07:30.476+0000] {processor.py:161} INFO - Started process (PID=2175) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:30.477+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:07:30.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:30.481+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:30.498+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:30.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:30.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:07:30.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:30.540+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:07:30.558+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T15:07:46.910+0000] {processor.py:161} INFO - Started process (PID=2176) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:46.912+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:07:46.918+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:46.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:46.953+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:07:47.324+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:47.323+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:07:47.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:07:47.339+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:07:47.384+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.481 seconds
[2025-09-09T15:08:04.487+0000] {processor.py:161} INFO - Started process (PID=2179) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:04.488+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:08:04.489+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:04.489+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:04.509+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:04.520+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:04.520+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:08:04.532+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:04.532+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:08:04.595+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.115 seconds
[2025-09-09T15:08:05.495+0000] {processor.py:161} INFO - Started process (PID=2180) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:05.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:08:05.497+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:05.497+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:05.511+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:05.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:05.521+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:08:05.533+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:05.533+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:08:05.645+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.156 seconds
[2025-09-09T15:08:06.642+0000] {processor.py:161} INFO - Started process (PID=2181) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:06.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:08:06.646+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:06.646+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:06.673+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:06.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:06.694+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:08:06.708+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:06.708+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:08:06.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.133 seconds
[2025-09-09T15:08:37.014+0000] {processor.py:161} INFO - Started process (PID=2184) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:37.015+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:08:37.016+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:37.016+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:37.028+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:08:37.037+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:37.037+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:08:37.046+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:08:37.046+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:08:37.059+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.050 seconds
[2025-09-09T15:09:07.215+0000] {processor.py:161} INFO - Started process (PID=2187) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:07.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:09:07.218+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:07.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:07.230+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:07.321+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:07.321+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:09:07.343+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:07.343+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:09:07.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.154 seconds
[2025-09-09T15:09:37.731+0000] {processor.py:161} INFO - Started process (PID=2310) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:37.735+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:09:37.740+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:37.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:37.827+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:09:37.912+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:37.911+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:09:37.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:09:37.932+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:09:38.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.313 seconds
[2025-09-09T15:10:08.360+0000] {processor.py:161} INFO - Started process (PID=2350) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:08.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:10:08.368+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:08.367+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:08.515+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:08.958+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:08.957+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:10:09.065+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:09.065+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:10:09.138+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.799 seconds
[2025-09-09T15:10:39.524+0000] {processor.py:161} INFO - Started process (PID=2351) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:39.540+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:10:39.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:39.543+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:39.562+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:10:39.602+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:39.602+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:10:39.617+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:10:39.617+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:10:39.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.339 seconds
[2025-09-09T15:11:10.034+0000] {processor.py:161} INFO - Started process (PID=2354) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:10.035+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:11:10.037+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:10.036+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:10.052+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:10.081+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:10.081+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:11:10.094+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:10.093+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:11:10.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T15:11:32.784+0000] {processor.py:161} INFO - Started process (PID=2357) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:32.785+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:11:32.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:32.786+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:32.804+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:11:32.830+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:32.830+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:11:32.843+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:11:32.843+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:11:32.866+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.091 seconds
[2025-09-09T15:12:03.091+0000] {processor.py:161} INFO - Started process (PID=2447) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:03.092+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:12:03.095+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:03.094+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:03.113+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:03.151+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:03.150+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:12:03.167+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:03.167+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:12:03.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.107 seconds
[2025-09-09T15:12:33.567+0000] {processor.py:161} INFO - Started process (PID=2505) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:33.568+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:12:33.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:33.569+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:33.583+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:12:33.613+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:33.613+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:12:33.624+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:12:33.624+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:12:33.641+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T15:13:03.870+0000] {processor.py:161} INFO - Started process (PID=2508) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:03.873+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:13:03.878+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:03.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:03.895+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:03.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:03.933+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:13:03.948+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:03.947+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:13:03.978+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.126 seconds
[2025-09-09T15:13:34.159+0000] {processor.py:161} INFO - Started process (PID=2511) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:34.160+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:13:34.161+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:34.161+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:34.172+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:13:34.197+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:34.196+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:13:34.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:13:34.207+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:13:34.221+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:14:04.442+0000] {processor.py:161} INFO - Started process (PID=2514) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:04.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:14:04.445+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:04.444+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:04.458+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:04.481+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:04.481+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:14:04.491+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:04.491+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:14:04.505+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:14:34.678+0000] {processor.py:161} INFO - Started process (PID=2517) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:34.680+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:14:34.681+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:34.681+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:34.692+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:14:34.713+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:34.713+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:14:34.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:14:34.722+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:14:34.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T15:15:04.907+0000] {processor.py:161} INFO - Started process (PID=2520) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:04.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:15:04.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:04.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:04.924+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:04.947+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:04.947+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:15:04.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:04.959+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:15:04.979+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T15:15:35.160+0000] {processor.py:161} INFO - Started process (PID=2523) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:35.161+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:15:35.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:35.163+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:35.180+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:15:35.207+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:35.207+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:15:35.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:15:35.216+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:15:35.244+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T15:16:05.385+0000] {processor.py:161} INFO - Started process (PID=2526) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:05.386+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:16:05.388+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:05.387+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:05.400+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:05.424+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:05.424+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:16:05.434+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:05.433+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:16:05.447+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:16:35.626+0000] {processor.py:161} INFO - Started process (PID=2529) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:35.626+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:16:35.628+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:35.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:35.640+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:16:35.662+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:35.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:16:35.672+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:16:35.672+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:16:35.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T15:17:05.843+0000] {processor.py:161} INFO - Started process (PID=2532) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:05.845+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:17:05.846+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:05.846+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:05.861+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:05.883+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:05.883+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:17:05.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:05.893+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:17:05.907+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T15:17:13.910+0000] {processor.py:161} INFO - Started process (PID=2533) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:13.911+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:17:13.912+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:13.912+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:13.940+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:13.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:13.967+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:17:13.978+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:13.978+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:17:13.994+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T15:17:14.977+0000] {processor.py:161} INFO - Started process (PID=2534) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:14.978+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:17:14.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:14.979+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:14.999+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:15.024+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:15.024+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:17:15.033+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:15.033+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:17:15.070+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.098 seconds
[2025-09-09T15:17:45.270+0000] {processor.py:161} INFO - Started process (PID=2679) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:45.272+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:17:45.273+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:45.273+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:45.288+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:17:45.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:45.318+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:17:45.332+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:17:45.332+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:17:45.357+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.093 seconds
[2025-09-09T15:18:15.536+0000] {processor.py:161} INFO - Started process (PID=2682) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:15.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:18:15.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:15.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:15.555+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:15.576+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:15.576+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:18:15.586+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:15.586+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:18:15.601+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T15:18:22.754+0000] {processor.py:161} INFO - Started process (PID=2683) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:22.754+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:18:22.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:22.756+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:22.795+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:23.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:23.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:18:23.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:23.195+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:18:23.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.567 seconds
[2025-09-09T15:18:53.731+0000] {processor.py:161} INFO - Started process (PID=2747) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:53.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:18:53.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:53.776+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:53.911+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:18:54.112+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:54.111+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:18:54.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:18:54.175+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:18:54.256+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.633 seconds
[2025-09-09T15:19:25.029+0000] {processor.py:161} INFO - Started process (PID=2794) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:19:25.033+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:19:25.034+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:19:25.034+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:19:25.050+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:19:25.238+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:19:25.237+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:19:25.417+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:19:25.417+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:19:27.248+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.480 seconds
[2025-09-09T15:19:59.750+0000] {processor.py:161} INFO - Started process (PID=2819) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:19:59.762+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:19:59.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:19:59.933+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:20:00.891+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:20:01.955+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:20:01.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:20:02.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:20:02.376+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:20:02.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.456 seconds
[2025-09-09T15:20:33.754+0000] {processor.py:161} INFO - Started process (PID=2895) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:20:33.766+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:20:34.052+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:20:33.960+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:20:34.885+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:20:36.720+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:20:36.720+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:20:37.149+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:20:37.149+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:20:37.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.967 seconds
[2025-09-09T15:21:07.895+0000] {processor.py:161} INFO - Started process (PID=2898) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:07.900+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:21:07.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:07.939+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:08.140+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:08.401+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:08.401+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:21:08.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:08.509+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:21:08.576+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.772 seconds
[2025-09-09T15:21:28.821+0000] {processor.py:161} INFO - Started process (PID=2901) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:28.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:21:28.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:28.825+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:28.874+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:28.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:28.930+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:21:28.953+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:28.952+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:21:29.057+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.247 seconds
[2025-09-09T15:21:59.215+0000] {processor.py:161} INFO - Started process (PID=2904) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:59.215+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:21:59.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:59.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:59.229+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:21:59.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:59.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:21:59.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:21:59.262+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:21:59.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T15:22:29.474+0000] {processor.py:161} INFO - Started process (PID=2907) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:29.476+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:22:29.479+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:29.479+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:29.502+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:29.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:29.544+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:22:29.554+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:29.554+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:22:29.572+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T15:22:59.637+0000] {processor.py:161} INFO - Started process (PID=2910) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:59.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:22:59.640+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:59.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:59.655+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:22:59.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:59.682+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:22:59.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:22:59.692+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:22:59.710+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T15:23:29.868+0000] {processor.py:161} INFO - Started process (PID=2913) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:23:29.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:23:29.870+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:23:29.870+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:23:29.883+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:23:29.903+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:23:29.903+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:23:29.913+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:23:29.913+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:23:29.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T15:24:00.099+0000] {processor.py:161} INFO - Started process (PID=2916) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:00.100+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:24:00.101+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:00.101+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:00.113+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:00.135+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:00.134+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:24:00.146+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:00.146+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:24:00.160+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T15:24:30.308+0000] {processor.py:161} INFO - Started process (PID=2919) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:30.309+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:24:30.310+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:30.310+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:30.324+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:24:30.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:30.345+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:24:30.355+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:24:30.355+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:24:30.370+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T15:25:00.496+0000] {processor.py:161} INFO - Started process (PID=2920) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:00.496+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:25:00.498+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:00.498+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:00.513+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:00.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:00.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:25:00.550+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:00.550+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:25:00.566+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T15:25:30.737+0000] {processor.py:161} INFO - Started process (PID=2923) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:30.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:25:30.740+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:30.740+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:30.756+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:25:30.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:30.782+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:25:30.793+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:25:30.792+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:25:30.812+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T15:26:01.030+0000] {processor.py:161} INFO - Started process (PID=2926) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:01.031+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:26:01.033+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:01.032+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:01.047+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:01.069+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:01.069+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:26:01.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:01.080+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:26:01.094+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T15:26:31.293+0000] {processor.py:161} INFO - Started process (PID=2929) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:31.294+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:26:31.295+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:31.295+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:31.308+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:26:31.331+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:31.331+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:26:31.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:26:31.340+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:26:31.356+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T15:27:01.550+0000] {processor.py:161} INFO - Started process (PID=2932) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:01.551+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:27:01.552+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:01.552+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:01.567+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:01.594+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:01.594+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:27:01.605+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:01.605+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:27:01.622+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T15:27:31.793+0000] {processor.py:161} INFO - Started process (PID=2935) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:31.794+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:27:31.796+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:31.795+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:31.810+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:27:31.835+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:31.835+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:27:31.845+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:27:31.844+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:27:31.862+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T15:28:02.050+0000] {processor.py:161} INFO - Started process (PID=2938) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:02.051+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:28:02.052+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:02.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:02.065+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:02.093+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:02.093+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:28:02.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:02.102+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:28:02.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T15:28:32.290+0000] {processor.py:161} INFO - Started process (PID=2941) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:32.291+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:28:32.293+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:32.292+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:32.306+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:28:32.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:32.340+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:28:32.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:28:32.350+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:28:32.365+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T15:29:02.533+0000] {processor.py:161} INFO - Started process (PID=2944) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:02.533+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:29:02.535+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:02.535+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:02.547+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:02.579+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:02.579+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:29:02.589+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:02.589+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:29:02.604+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T15:29:32.777+0000] {processor.py:161} INFO - Started process (PID=2947) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:32.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:29:32.779+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:32.779+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:32.798+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:29:32.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:32.820+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:29:32.829+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:29:32.829+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:29:32.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T15:30:03.063+0000] {processor.py:161} INFO - Started process (PID=2950) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:03.066+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:30:03.073+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:03.072+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:03.106+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:03.145+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:03.144+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:30:03.166+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:03.165+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:30:03.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.157 seconds
[2025-09-09T15:30:33.388+0000] {processor.py:161} INFO - Started process (PID=2953) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:33.389+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:30:33.391+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:33.390+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:33.408+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:30:33.431+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:33.431+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:30:33.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:30:33.442+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:30:33.457+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T15:31:03.608+0000] {processor.py:161} INFO - Started process (PID=2956) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:03.609+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:31:03.610+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:03.610+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:03.622+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:03.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:03.643+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:31:03.651+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:03.651+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:31:03.665+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:31:33.822+0000] {processor.py:161} INFO - Started process (PID=2959) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:33.823+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:31:33.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:33.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:33.837+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:31:33.859+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:33.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:31:33.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:31:33.869+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:31:33.885+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T15:32:04.035+0000] {processor.py:161} INFO - Started process (PID=2962) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:04.036+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:32:04.037+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:04.037+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:04.051+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:04.074+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:04.074+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:32:04.084+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:04.084+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:32:04.099+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T15:32:34.288+0000] {processor.py:161} INFO - Started process (PID=2965) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:34.289+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:32:34.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:34.290+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:34.305+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:32:34.333+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:34.333+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:32:34.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:32:34.346+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:32:34.364+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T15:33:04.558+0000] {processor.py:161} INFO - Started process (PID=2968) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:04.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:33:04.560+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:04.560+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:04.572+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:04.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:04.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:33:04.607+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:04.607+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:33:04.621+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T15:33:34.862+0000] {processor.py:161} INFO - Started process (PID=2971) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:34.862+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:33:34.864+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:34.863+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:34.875+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:33:34.894+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:34.894+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:33:34.903+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:33:34.903+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:33:34.916+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:34:05.114+0000] {processor.py:161} INFO - Started process (PID=2974) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:05.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:34:05.116+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:05.116+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:05.129+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:05.159+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:05.158+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:34:05.168+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:05.168+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:34:05.184+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T15:34:35.428+0000] {processor.py:161} INFO - Started process (PID=2977) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:35.428+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:34:35.430+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:35.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:35.441+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:34:35.461+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:35.461+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:34:35.470+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:34:35.470+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:34:35.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:35:05.683+0000] {processor.py:161} INFO - Started process (PID=2980) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:05.684+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:35:05.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:05.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:05.697+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:05.719+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:05.719+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:35:05.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:05.728+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:35:05.742+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T15:35:36.201+0000] {processor.py:161} INFO - Started process (PID=2983) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:36.230+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:35:36.263+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:36.263+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:36.276+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:35:36.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:36.302+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:35:36.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:35:36.313+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:35:36.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.337 seconds
[2025-09-09T15:36:07.121+0000] {processor.py:161} INFO - Started process (PID=2986) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:07.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:36:07.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:07.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:07.135+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:07.157+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:07.157+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:36:07.166+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:07.166+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:36:07.180+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T15:36:37.441+0000] {processor.py:161} INFO - Started process (PID=2989) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:37.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:36:37.444+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:37.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:37.456+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:36:37.477+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:37.477+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:36:37.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:36:37.486+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:36:37.500+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T15:37:07.722+0000] {processor.py:161} INFO - Started process (PID=2992) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:07.722+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:37:07.724+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:07.723+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:07.735+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:07.757+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:07.756+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:37:07.765+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:07.765+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:37:07.778+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T15:37:37.970+0000] {processor.py:161} INFO - Started process (PID=2995) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:37.971+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:37:37.972+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:37.972+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:37.983+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:37:38.005+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:38.005+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:37:38.014+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:37:38.014+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:37:38.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T15:38:08.260+0000] {processor.py:161} INFO - Started process (PID=2998) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:08.260+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:38:08.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:08.261+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:08.274+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:08.294+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:08.293+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:38:08.307+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:08.306+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:38:08.320+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T15:38:38.563+0000] {processor.py:161} INFO - Started process (PID=3001) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:38.564+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:38:38.565+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:38.565+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:38.578+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:38:38.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:38.597+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:38:38.605+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:38:38.605+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:38:38.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:39:08.818+0000] {processor.py:161} INFO - Started process (PID=3004) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:08.819+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:39:08.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:08.820+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:08.834+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:08.857+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:08.857+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:39:08.867+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:08.867+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:39:08.890+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T15:39:39.074+0000] {processor.py:161} INFO - Started process (PID=3007) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:39.075+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:39:39.076+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:39.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:39.086+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:39:39.106+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:39.106+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:39:39.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:39:39.115+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:39:39.128+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T15:40:09.366+0000] {processor.py:161} INFO - Started process (PID=3010) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:09.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:40:09.369+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:09.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:09.380+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:09.400+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:09.400+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:40:09.408+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:09.408+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:40:09.421+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:40:39.650+0000] {processor.py:161} INFO - Started process (PID=3013) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:39.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:40:39.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:39.653+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:39.665+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:40:39.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:39.686+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:40:39.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:40:39.696+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:40:39.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.091 seconds
[2025-09-09T15:41:09.951+0000] {processor.py:161} INFO - Started process (PID=3016) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:09.952+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:41:09.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:09.953+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:09.965+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:09.984+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:09.984+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:41:09.993+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:09.993+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:41:10.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:41:40.170+0000] {processor.py:161} INFO - Started process (PID=3017) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:40.171+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:41:40.173+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:40.173+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:40.185+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:41:40.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:40.208+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:41:40.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:41:40.217+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:41:40.233+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T15:42:10.499+0000] {processor.py:161} INFO - Started process (PID=3020) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:10.500+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:42:10.502+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:10.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:10.515+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:10.535+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:10.534+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:42:10.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:10.544+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:42:10.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T15:42:40.846+0000] {processor.py:161} INFO - Started process (PID=3023) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:40.847+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:42:40.849+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:40.849+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:40.860+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:42:40.880+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:40.880+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:42:40.888+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:42:40.888+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:42:40.901+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:43:11.120+0000] {processor.py:161} INFO - Started process (PID=3026) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:11.121+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:43:11.122+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:11.122+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:11.133+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:11.153+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:11.153+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:43:11.162+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:11.162+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:43:11.178+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:43:41.427+0000] {processor.py:161} INFO - Started process (PID=3029) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:41.427+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:43:41.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:41.429+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:41.440+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:43:41.460+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:41.460+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:43:41.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:43:41.469+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:43:41.482+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:44:11.757+0000] {processor.py:161} INFO - Started process (PID=3032) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:11.758+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:44:11.760+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:11.759+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:11.771+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:11.792+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:11.792+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:44:11.801+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:11.801+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:44:11.814+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:44:42.058+0000] {processor.py:161} INFO - Started process (PID=3035) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:42.059+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:44:42.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:42.060+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:42.071+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:44:42.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:42.092+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:44:42.101+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:44:42.100+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:44:42.114+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T15:45:12.332+0000] {processor.py:161} INFO - Started process (PID=3038) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:12.333+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:45:12.335+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:12.334+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:12.345+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:12.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:12.367+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:45:12.377+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:12.376+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:45:12.389+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T15:45:42.633+0000] {processor.py:161} INFO - Started process (PID=3041) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:42.634+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:45:42.635+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:42.635+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:42.647+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:45:42.668+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:42.668+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:45:42.678+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:45:42.678+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:45:42.693+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T15:46:12.956+0000] {processor.py:161} INFO - Started process (PID=3044) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:12.957+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:46:12.958+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:12.958+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:12.968+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:12.989+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:12.988+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:46:12.997+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:12.997+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:46:13.010+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T15:46:43.312+0000] {processor.py:161} INFO - Started process (PID=3047) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:43.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:46:43.314+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:43.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:43.325+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:46:43.345+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:43.345+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:46:43.355+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:46:43.355+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:46:43.368+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T15:47:13.632+0000] {processor.py:161} INFO - Started process (PID=3050) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:13.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:47:13.634+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:13.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:13.645+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:13.666+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:13.665+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:47:13.675+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:13.675+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:47:13.688+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:47:43.914+0000] {processor.py:161} INFO - Started process (PID=3053) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:43.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:47:43.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:43.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:43.930+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:47:43.955+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:43.955+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:47:43.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:47:43.967+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:47:43.981+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T15:48:14.308+0000] {processor.py:161} INFO - Started process (PID=3056) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:14.310+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:48:14.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:14.312+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:14.330+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:14.354+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:14.353+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:48:14.363+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:14.363+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:48:14.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T15:48:55.532+0000] {processor.py:161} INFO - Started process (PID=3059) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:55.539+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:48:55.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:55.541+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:55.565+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:48:55.593+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:55.593+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:48:55.605+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:48:55.605+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:48:55.714+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.187 seconds
[2025-09-09T15:49:25.925+0000] {processor.py:161} INFO - Started process (PID=3062) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:25.927+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:49:25.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:25.932+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:25.952+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:25.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:25.994+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:49:26.009+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:26.009+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:49:26.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.117 seconds
[2025-09-09T15:49:56.171+0000] {processor.py:161} INFO - Started process (PID=3065) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:56.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:49:56.174+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:56.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:56.192+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:49:56.221+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:56.221+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:49:56.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:49:56.232+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:49:56.251+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T15:50:26.400+0000] {processor.py:161} INFO - Started process (PID=3068) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:26.400+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:50:26.402+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:26.401+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:26.414+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:26.441+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:26.440+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:50:26.450+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:26.450+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:50:26.464+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T15:50:57.064+0000] {processor.py:161} INFO - Started process (PID=3168) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:57.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:50:57.176+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:57.171+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:57.356+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:50:57.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:57.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:50:57.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:50:57.543+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:50:57.564+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.628 seconds
[2025-09-09T15:51:28.169+0000] {processor.py:161} INFO - Started process (PID=3213) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:28.174+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:51:28.194+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:28.191+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:28.289+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:28.382+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:28.382+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:51:28.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:28.427+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:51:28.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.361 seconds
[2025-09-09T15:51:58.891+0000] {processor.py:161} INFO - Started process (PID=3286) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:58.899+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:51:58.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:58.945+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:59.151+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:51:59.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:59.424+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:51:59.515+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:51:59.514+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:51:59.587+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.792 seconds
[2025-09-09T15:52:30.625+0000] {processor.py:161} INFO - Started process (PID=3289) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:52:30.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:52:30.640+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:52:30.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:52:30.781+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:52:31.093+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:52:31.092+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:52:31.188+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:52:31.188+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:52:31.309+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.699 seconds
[2025-09-09T15:53:01.519+0000] {processor.py:161} INFO - Started process (PID=3290) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:01.519+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:53:01.521+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:01.521+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:01.535+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:01.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:01.566+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:53:01.576+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:01.576+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:53:01.592+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T15:53:31.878+0000] {processor.py:161} INFO - Started process (PID=3293) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:31.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:53:31.880+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:31.880+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:31.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:31.897+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:53:31.909+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:31.932+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T15:53:53.092+0000] {processor.py:161} INFO - Started process (PID=3296) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:53.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:53:53.095+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:53.094+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:53.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:53:53.126+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:53:53.131+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:53:53.161+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T15:54:23.349+0000] {processor.py:161} INFO - Started process (PID=3299) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:23.350+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:54:23.352+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:54:23.351+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:23.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:54:23.361+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:54:23.366+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:23.385+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.044 seconds
[2025-09-09T15:54:53.553+0000] {processor.py:161} INFO - Started process (PID=3302) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:53.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:54:53.556+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:54:53.556+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:53.574+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:54:53.570+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:54:53.575+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:54:53.594+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.050 seconds
[2025-09-09T15:55:23.753+0000] {processor.py:161} INFO - Started process (PID=3305) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:23.755+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:55:23.756+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:55:23.755+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:23.769+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:55:23.766+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:55:23.770+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:23.790+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.045 seconds
[2025-09-09T15:55:53.986+0000] {processor.py:161} INFO - Started process (PID=3308) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:53.986+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:55:53.987+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:55:53.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:54.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:55:53.996+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:55:54.000+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:55:54.026+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.046 seconds
[2025-09-09T15:56:24.178+0000] {processor.py:161} INFO - Started process (PID=3311) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:24.179+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:56:24.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:24.181+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:24.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:24.191+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 30, in <module>
    silver_topcv_transform = SparkSubmitOperator(
                             ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 881, in __init__
    raise AirflowException(
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkSubmitOperator (task_id: silver_topcv_transform). Invalid arguments were:
**kwargs: {'silver_vietnamworkcv_transform': <Task(SparkSubmitOperator): silver_group.silver_vietnamworkcv_transform>}
[2025-09-09T15:56:24.196+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:24.214+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.042 seconds
[2025-09-09T15:56:31.263+0000] {processor.py:161} INFO - Started process (PID=3312) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:31.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:56:31.264+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:31.264+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:31.276+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:31.274+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 38
    silver_vietnamworkcv_transform = SparkSubmitOperator(
                                     
SyntaxError: invalid syntax. Perhaps you forgot a comma?
[2025-09-09T15:56:31.277+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:31.297+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.040 seconds
[2025-09-09T15:56:40.346+0000] {processor.py:161} INFO - Started process (PID=3313) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:40.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:56:40.348+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:40.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:40.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:56:40.359+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 38
    silver_vietnamworkcv_transform = SparkSubmitOperator(
                                     ^^^^^^^^^^^^^^^^^^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
[2025-09-09T15:56:40.362+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:56:40.380+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.042 seconds
[2025-09-09T15:57:10.603+0000] {processor.py:161} INFO - Started process (PID=3316) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:10.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:57:10.604+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:10.604+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:10.613+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:10.612+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 38
    silver_vietnamworkcv_transform = SparkSubmitOperator(
                                     ^^^^^^^^^^^^^^^^^^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
[2025-09-09T15:57:10.613+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:10.630+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.033 seconds
[2025-09-09T15:57:40.814+0000] {processor.py:161} INFO - Started process (PID=3319) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:40.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:57:40.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:40.815+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:40.826+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:40.825+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 38
    silver_vietnamworkcv_transform = SparkSubmitOperator(
                                     ^^^^^^^^^^^^^^^^^^^^
SyntaxError: invalid syntax. Perhaps you forgot a comma?
[2025-09-09T15:57:40.826+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:40.843+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.035 seconds
[2025-09-09T15:57:58.509+0000] {processor.py:161} INFO - Started process (PID=3322) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:58.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:57:58.510+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:58.510+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:58.530+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:57:58.730+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:58.730+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:57:58.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:57:58.737+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:57:58.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.255 seconds
[2025-09-09T15:58:00.525+0000] {processor.py:161} INFO - Started process (PID=3323) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:00.526+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:58:00.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:00.526+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:00.548+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:00.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:00.558+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:58:00.569+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:00.568+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:58:00.584+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T15:58:03.556+0000] {processor.py:161} INFO - Started process (PID=3324) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:03.556+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:58:03.557+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:03.557+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:03.575+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:03.586+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:03.586+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:58:03.596+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:03.595+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:58:03.613+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:58:33.825+0000] {processor.py:161} INFO - Started process (PID=3327) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:33.826+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:58:33.827+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:33.826+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:33.842+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:58:33.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:33.869+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:58:33.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:58:33.879+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:58:33.894+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T15:59:04.054+0000] {processor.py:161} INFO - Started process (PID=3330) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:04.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:59:04.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:04.056+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:04.069+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:04.090+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:04.090+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:59:04.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:04.099+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:59:04.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T15:59:34.284+0000] {processor.py:161} INFO - Started process (PID=3333) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:34.285+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T15:59:34.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:34.285+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:34.298+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T15:59:34.320+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:34.320+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T15:59:34.329+0000] {logging_mixin.py:188} INFO - [2025-09-09T15:59:34.329+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T15:59:34.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T16:00:04.505+0000] {processor.py:161} INFO - Started process (PID=3336) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:04.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:00:04.507+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:04.507+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:04.523+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:04.545+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:04.545+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:00:04.555+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:04.555+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:00:04.568+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T16:00:34.727+0000] {processor.py:161} INFO - Started process (PID=3339) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:34.728+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:00:34.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:34.729+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:34.743+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:00:34.768+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:34.767+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:00:34.779+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:00:34.778+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:00:34.792+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T16:01:04.955+0000] {processor.py:161} INFO - Started process (PID=3342) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:04.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:01:04.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:04.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:04.970+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:04.991+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:04.991+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:01:05.000+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:05.000+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:01:05.013+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T16:01:35.190+0000] {processor.py:161} INFO - Started process (PID=3345) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:35.191+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:01:35.192+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:35.192+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:35.208+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:01:35.232+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:35.231+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:01:35.241+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:01:35.241+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:01:35.255+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:02:05.464+0000] {processor.py:161} INFO - Started process (PID=3401) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:05.467+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:02:05.469+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:05.469+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:05.496+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:05.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:05.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:02:05.555+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:05.555+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:02:05.582+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.132 seconds
[2025-09-09T16:02:36.050+0000] {processor.py:161} INFO - Started process (PID=3450) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:36.052+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:02:36.053+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:36.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:36.069+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:02:36.097+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:36.097+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:02:36.107+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:02:36.107+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:02:36.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T16:03:08.209+0000] {processor.py:161} INFO - Started process (PID=3497) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:08.242+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:03:08.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:08.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:09.657+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:10.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:10.441+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:03:10.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:10.587+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:03:10.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.608 seconds
[2025-09-09T16:03:42.005+0000] {processor.py:161} INFO - Started process (PID=3592) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:42.040+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:03:42.098+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:42.075+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:43.246+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:03:44.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:44.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:03:45.172+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:03:45.171+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:03:45.556+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.109 seconds
[2025-09-09T16:04:16.583+0000] {processor.py:161} INFO - Started process (PID=3696) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:16.584+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:04:16.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:16.586+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:16.713+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:16.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:16.824+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:04:16.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:16.857+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:04:16.889+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.379 seconds
[2025-09-09T16:04:47.153+0000] {processor.py:161} INFO - Started process (PID=3771) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:47.155+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:04:47.156+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:47.156+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:47.177+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:04:47.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:47.217+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:04:47.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:04:47.236+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:04:47.274+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.128 seconds
[2025-09-09T16:05:17.390+0000] {processor.py:161} INFO - Started process (PID=3835) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:17.395+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:05:17.399+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:17.397+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:17.445+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:17.514+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:17.510+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:05:17.551+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:17.551+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:05:17.577+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.209 seconds
[2025-09-09T16:05:47.745+0000] {processor.py:161} INFO - Started process (PID=3838) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:47.746+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:05:47.747+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:47.747+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:47.759+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:05:47.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:47.782+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:05:47.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:05:47.791+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:05:47.808+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T16:06:18.126+0000] {processor.py:161} INFO - Started process (PID=3841) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:18.127+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:06:18.127+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:18.127+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:18.143+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:18.168+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:18.168+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:06:18.179+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:18.179+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:06:18.193+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T16:06:48.370+0000] {processor.py:161} INFO - Started process (PID=3844) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:48.371+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:06:48.372+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:48.371+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:48.384+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:06:48.409+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:48.409+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:06:48.420+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:06:48.420+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:06:48.434+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T16:07:18.619+0000] {processor.py:161} INFO - Started process (PID=3847) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:18.619+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:07:18.620+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:18.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:18.632+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:18.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:18.653+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:07:18.662+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:18.661+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:07:18.674+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T16:07:48.864+0000] {processor.py:161} INFO - Started process (PID=3850) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:48.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:07:48.865+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:48.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:48.879+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:07:48.900+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:48.900+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:07:48.910+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:07:48.910+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:07:48.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T16:08:19.069+0000] {processor.py:161} INFO - Started process (PID=3853) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:19.070+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:08:19.071+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:19.070+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:19.083+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:19.109+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:19.109+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:08:19.118+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:19.118+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:08:19.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T16:08:49.250+0000] {processor.py:161} INFO - Started process (PID=3854) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:49.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:08:49.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:49.251+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:49.268+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:08:49.297+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:49.297+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:08:49.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:08:49.308+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:08:49.324+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T16:09:19.494+0000] {processor.py:161} INFO - Started process (PID=3857) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:19.495+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:09:19.496+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:19.496+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:19.512+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:19.541+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:19.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:09:19.552+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:19.551+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:09:19.567+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T16:09:41.208+0000] {processor.py:161} INFO - Started process (PID=3860) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:41.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:09:41.210+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:41.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:41.230+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:41.399+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:41.399+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:09:41.406+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:41.406+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:09:41.425+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.223 seconds
[2025-09-09T16:09:46.243+0000] {processor.py:161} INFO - Started process (PID=3861) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:46.244+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:09:46.244+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:46.244+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:46.265+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:46.278+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:46.278+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:09:46.288+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:46.288+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:09:46.304+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T16:09:52.856+0000] {processor.py:161} INFO - Started process (PID=3864) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:52.857+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:09:52.858+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:52.858+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:52.880+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:09:52.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:52.893+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:09:52.904+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:09:52.904+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:09:52.923+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T16:10:11.961+0000] {processor.py:161} INFO - Started process (PID=3865) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:11.962+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:10:11.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:11.962+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:11.980+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:12.068+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:12.067+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:10:12.075+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:12.075+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:10:12.093+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.137 seconds
[2025-09-09T16:10:13.037+0000] {processor.py:161} INFO - Started process (PID=3866) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:13.038+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:10:13.038+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:13.038+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:13.059+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:13.070+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:13.070+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:10:13.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:13.080+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:10:13.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T16:10:42.816+0000] {processor.py:161} INFO - Started process (PID=3869) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:42.817+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:10:42.818+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:42.818+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:42.838+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:42.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:42.917+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:10:42.924+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:42.924+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:10:42.941+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.131 seconds
[2025-09-09T16:10:43.826+0000] {processor.py:161} INFO - Started process (PID=3870) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:43.827+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:10:43.827+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:43.827+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:43.846+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:43.856+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:43.856+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:10:43.865+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:43.865+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:10:43.880+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T16:10:47.867+0000] {processor.py:161} INFO - Started process (PID=3871) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:47.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:10:47.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:47.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:47.889+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:10:47.905+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:47.905+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:10:47.916+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:10:47.916+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:10:47.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T16:11:18.171+0000] {processor.py:161} INFO - Started process (PID=3973) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:18.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:11:18.174+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:18.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:18.202+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:18.271+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:18.271+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:11:18.304+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:18.304+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:11:18.339+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.183 seconds
[2025-09-09T16:11:48.986+0000] {processor.py:161} INFO - Started process (PID=4014) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:48.991+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:11:48.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:48.993+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:49.010+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:11:49.048+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:49.048+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:11:49.061+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:11:49.061+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:11:49.100+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.143 seconds
[2025-09-09T16:12:20.013+0000] {processor.py:161} INFO - Started process (PID=4078) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:20.079+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:12:20.140+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:20.108+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:21.075+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:23.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:23.196+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:12:23.765+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:23.763+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:12:24.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.495 seconds
[2025-09-09T16:12:55.040+0000] {processor.py:161} INFO - Started process (PID=4211) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:55.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:12:55.094+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:55.076+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:56.114+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:12:57.585+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:57.582+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:12:57.999+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:12:57.997+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:12:58.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.694 seconds
[2025-09-09T16:13:28.602+0000] {processor.py:161} INFO - Started process (PID=4214) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:28.605+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:13:28.605+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:28.605+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:28.711+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:29.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:29.130+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:13:29.144+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:29.144+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:13:29.162+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.627 seconds
[2025-09-09T16:13:59.410+0000] {processor.py:161} INFO - Started process (PID=4360) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:59.412+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:13:59.413+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:59.412+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:59.431+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:13:59.459+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:59.458+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:13:59.473+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:13:59.473+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:13:59.488+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T16:14:29.663+0000] {processor.py:161} INFO - Started process (PID=4438) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:29.664+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:14:29.664+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:29.664+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:29.677+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:29.701+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:29.701+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:14:29.710+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:29.710+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:14:29.751+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.100 seconds
[2025-09-09T16:14:59.935+0000] {processor.py:161} INFO - Started process (PID=4441) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:59.936+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:14:59.936+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:59.936+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:59.950+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:14:59.973+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:59.973+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:14:59.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:14:59.982+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:14:59.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T16:15:17.048+0000] {processor.py:161} INFO - Started process (PID=4442) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:17.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:15:17.050+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:17.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:17.070+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:17.199+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:17.199+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:15:17.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:17.208+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:15:17.228+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.187 seconds
[2025-09-09T16:15:48.604+0000] {processor.py:161} INFO - Started process (PID=4561) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:48.610+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:15:48.653+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:48.620+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:49.270+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:15:49.922+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:49.921+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:15:50.599+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:15:50.597+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:15:54.155+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.951 seconds
[2025-09-09T16:16:26.028+0000] {processor.py:161} INFO - Started process (PID=4684) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:16:26.093+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:16:26.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:16:26.126+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:16:27.302+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:16:28.522+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:16:28.521+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:16:29.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:16:29.021+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:16:29.472+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.064 seconds
[2025-09-09T16:17:00.121+0000] {processor.py:161} INFO - Started process (PID=4805) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:00.122+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:17:00.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:00.123+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:00.162+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:00.216+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:00.216+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:17:00.239+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:00.239+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:17:01.249+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.157 seconds
[2025-09-09T16:17:31.674+0000] {processor.py:161} INFO - Started process (PID=4934) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:31.676+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:17:31.677+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:31.677+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:31.707+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:17:31.735+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:31.735+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:17:31.745+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:17:31.745+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:17:31.762+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T16:18:02.947+0000] {processor.py:161} INFO - Started process (PID=5011) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:02.974+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:18:03.011+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:02.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:03.833+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:08.695+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:08.449+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:18:10.088+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:10.020+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:18:10.803+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 8.340 seconds
[2025-09-09T16:18:43.272+0000] {processor.py:161} INFO - Started process (PID=5062) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:43.326+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:18:43.380+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:43.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:44.516+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:18:45.761+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:45.760+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:18:46.226+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:18:46.226+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:18:47.925+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 5.052 seconds
[2025-09-09T16:19:18.083+0000] {processor.py:161} INFO - Started process (PID=5104) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:18.084+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:19:18.085+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:18.085+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:18.105+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:18.136+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:18.136+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:19:18.161+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:18.160+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:19:18.186+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.114 seconds
[2025-09-09T16:19:48.367+0000] {processor.py:161} INFO - Started process (PID=5107) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:48.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:19:48.368+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:48.368+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:48.384+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:19:48.414+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:48.414+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:19:48.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:19:48.425+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:19:48.441+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:20:18.616+0000] {processor.py:161} INFO - Started process (PID=5110) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:18.617+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:20:18.618+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:18.618+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:18.636+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:18.662+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:18.662+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:20:18.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:18.674+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:20:18.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T16:20:48.864+0000] {processor.py:161} INFO - Started process (PID=5113) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:48.865+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:20:48.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:48.866+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:48.881+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:20:48.901+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:48.901+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:20:48.911+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:20:48.911+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:20:48.924+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T16:21:19.027+0000] {processor.py:161} INFO - Started process (PID=5114) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:19.028+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:21:19.029+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:19.029+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:19.050+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:19.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:19.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:21:19.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:19.090+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:21:19.108+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.089 seconds
[2025-09-09T16:21:49.264+0000] {processor.py:161} INFO - Started process (PID=5117) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:49.265+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:21:49.266+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:49.266+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:49.281+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:21:49.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:49.313+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:21:49.324+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:21:49.323+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:21:49.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T16:22:19.554+0000] {processor.py:161} INFO - Started process (PID=5120) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:19.555+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:22:19.556+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:19.556+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:19.571+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:19.599+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:19.599+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:22:19.611+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:19.611+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:22:19.628+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T16:22:49.944+0000] {processor.py:161} INFO - Started process (PID=5221) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:49.954+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:22:49.959+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:49.957+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:50.023+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:22:50.120+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:50.120+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:22:50.155+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:22:50.154+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:22:50.184+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.306 seconds
[2025-09-09T16:23:20.398+0000] {processor.py:161} INFO - Started process (PID=5345) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:20.402+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:23:20.405+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:20.403+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:20.433+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:20.478+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:20.477+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:23:20.502+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:20.502+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:23:20.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.165 seconds
[2025-09-09T16:23:51.046+0000] {processor.py:161} INFO - Started process (PID=5498) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:51.054+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:23:51.072+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:51.064+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:51.309+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:23:51.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:51.527+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:23:51.639+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:23:51.639+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:23:53.920+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.978 seconds
[2025-09-09T16:24:25.088+0000] {processor.py:161} INFO - Started process (PID=5622) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:25.090+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:24:25.091+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:25.091+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:25.211+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:25.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:25.389+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:24:25.489+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:25.489+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:24:25.606+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.735 seconds
[2025-09-09T16:24:55.712+0000] {processor.py:161} INFO - Started process (PID=5744) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:56.190+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:24:56.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:56.193+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:56.737+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:24:57.850+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:57.850+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:24:57.891+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:24:57.891+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:24:59.693+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.990 seconds
[2025-09-09T16:25:30.022+0000] {processor.py:161} INFO - Started process (PID=5791) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:25:30.027+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:25:30.028+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:25:30.028+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:25:30.086+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:25:30.134+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:25:30.134+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:25:30.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:25:30.185+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:25:30.311+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.302 seconds
[2025-09-09T16:26:00.524+0000] {processor.py:161} INFO - Started process (PID=5794) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:00.525+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:26:00.526+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:00.526+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:00.539+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:00.561+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:00.561+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:26:00.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:00.570+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:26:00.583+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T16:26:30.784+0000] {processor.py:161} INFO - Started process (PID=5797) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:30.786+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:26:30.786+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:30.786+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:30.799+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:26:30.822+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:30.821+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:26:30.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:26:30.832+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:26:30.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T16:27:01.060+0000] {processor.py:161} INFO - Started process (PID=5800) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:01.062+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:27:01.063+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:01.062+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:01.077+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:01.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:01.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:27:01.113+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:01.113+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:27:01.126+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T16:27:31.295+0000] {processor.py:161} INFO - Started process (PID=5803) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:31.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:27:31.297+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:31.296+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:31.310+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:27:31.337+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:31.337+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:27:31.346+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:27:31.346+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:27:31.361+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T16:28:01.509+0000] {processor.py:161} INFO - Started process (PID=5806) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:01.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:28:01.511+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:01.511+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:01.524+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:01.549+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:01.548+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:28:01.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:01.559+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:28:01.574+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T16:28:31.713+0000] {processor.py:161} INFO - Started process (PID=5809) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:31.713+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:28:31.714+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:31.714+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:31.727+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:28:31.750+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:31.750+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:28:31.760+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:28:31.760+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:28:31.774+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T16:29:01.917+0000] {processor.py:161} INFO - Started process (PID=5812) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:01.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:29:01.918+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:01.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:01.932+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:01.958+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:01.958+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:29:01.968+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:01.968+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:29:01.983+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T16:29:32.148+0000] {processor.py:161} INFO - Started process (PID=5815) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:32.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:29:32.151+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:32.150+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:32.168+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:29:32.197+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:32.197+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:29:32.208+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:29:32.208+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:29:32.224+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T16:30:02.364+0000] {processor.py:161} INFO - Started process (PID=5818) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:02.364+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:30:02.365+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:02.365+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:02.380+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:02.404+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:02.404+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:30:02.415+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:02.415+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:30:02.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T16:30:32.581+0000] {processor.py:161} INFO - Started process (PID=5821) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:32.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:30:32.583+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:32.583+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:32.596+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:30:32.626+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:32.626+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:30:32.637+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:30:32.637+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:30:32.653+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T16:31:02.794+0000] {processor.py:161} INFO - Started process (PID=5824) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:02.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:31:02.796+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:02.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:02.809+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:02.838+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:02.838+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:31:02.849+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:02.849+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:31:02.865+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T16:31:33.008+0000] {processor.py:161} INFO - Started process (PID=5827) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:33.009+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:31:33.010+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:33.010+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:33.024+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:31:33.049+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:33.048+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:31:33.060+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:31:33.059+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:31:33.075+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T16:32:03.221+0000] {processor.py:161} INFO - Started process (PID=5830) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:03.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:32:03.223+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:03.223+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:03.237+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:03.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:03.259+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:32:03.268+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:03.268+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:32:03.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T16:32:33.421+0000] {processor.py:161} INFO - Started process (PID=5833) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:33.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:32:33.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:33.422+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:33.435+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:32:33.458+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:33.458+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:32:33.467+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:32:33.467+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:32:33.484+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T16:33:03.647+0000] {processor.py:161} INFO - Started process (PID=5836) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:03.648+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:33:03.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:03.648+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:03.669+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:03.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:03.694+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:33:03.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:03.705+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:33:03.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T16:33:33.941+0000] {processor.py:161} INFO - Started process (PID=5839) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:33.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:33:33.942+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:33.942+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:33.955+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:33:33.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:33.976+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:33:33.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:33:33.985+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:33:33.998+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T16:34:04.273+0000] {processor.py:161} INFO - Started process (PID=5842) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:04.274+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:34:04.275+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:04.275+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:04.287+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:04.309+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:04.309+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:34:04.318+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:04.318+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:34:04.331+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T16:34:34.529+0000] {processor.py:161} INFO - Started process (PID=5845) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:34.529+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:34:34.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:34.530+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:34.542+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:34:34.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:34.563+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:34:34.574+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:34:34.574+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:34:34.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T16:35:04.795+0000] {processor.py:161} INFO - Started process (PID=5848) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:04.795+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:35:04.796+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:04.796+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:04.810+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:04.832+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:04.832+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:35:04.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:04.841+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:35:04.854+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T16:35:35.055+0000] {processor.py:161} INFO - Started process (PID=5851) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:35.055+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:35:35.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:35.056+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:35.068+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:35:35.088+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:35.087+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:35:35.096+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:35:35.096+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:35:35.112+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T16:36:05.346+0000] {processor.py:161} INFO - Started process (PID=5854) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:05.347+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:36:05.348+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:05.348+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:05.366+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:05.391+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:05.391+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:36:05.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:05.473+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:36:05.599+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.259 seconds
[2025-09-09T16:36:35.787+0000] {processor.py:161} INFO - Started process (PID=5857) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:35.788+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:36:35.789+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:35.788+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:35.802+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:35.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:35.824+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:36:35.834+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:35.833+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:36:35.846+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T16:36:45.914+0000] {processor.py:161} INFO - Started process (PID=5858) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:45.915+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:36:45.916+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:45.916+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:45.935+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:36:45.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:45.962+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:36:45.972+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:36:45.972+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:36:45.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:37:05.678+0000] {processor.py:161} INFO - Started process (PID=5861) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:05.683+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:37:05.685+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:05.684+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:05.708+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:05.899+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:05.899+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:37:05.906+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:05.906+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:37:05.927+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.258 seconds
[2025-09-09T16:37:36.894+0000] {processor.py:161} INFO - Started process (PID=5964) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:36.896+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:37:36.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:36.896+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:36.912+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:37:36.966+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:36.966+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:37:36.977+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:37:36.977+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:37:36.992+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.104 seconds
[2025-09-09T16:38:07.076+0000] {processor.py:161} INFO - Started process (PID=6007) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:07.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:38:07.078+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:07.077+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:07.129+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:07.175+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:07.174+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:38:07.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:07.193+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:38:07.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.208 seconds
[2025-09-09T16:38:37.638+0000] {processor.py:161} INFO - Started process (PID=6194) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:37.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:38:37.645+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:37.644+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:38.369+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:38:39.358+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:39.358+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:38:39.706+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:38:39.705+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:38:39.977+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.352 seconds
[2025-09-09T16:39:11.049+0000] {processor.py:161} INFO - Started process (PID=6263) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:11.060+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:39:11.108+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:11.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:12.339+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:13.043+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:13.041+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:39:13.313+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:13.311+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:39:13.585+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.921 seconds
[2025-09-09T16:39:43.822+0000] {processor.py:161} INFO - Started process (PID=6412) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:43.824+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:39:43.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:43.824+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:43.854+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:39:43.895+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:43.895+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:39:43.907+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:39:43.906+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:39:43.989+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.213 seconds
[2025-09-09T16:40:15.258+0000] {processor.py:161} INFO - Started process (PID=6489) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:15.261+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:40:15.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:15.262+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:15.532+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:15.901+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:15.900+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:40:16.228+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:16.228+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:40:16.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.268 seconds
[2025-09-09T16:40:46.715+0000] {processor.py:161} INFO - Started process (PID=6803) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:46.718+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:40:46.722+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:46.719+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:46.777+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:40:46.855+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:46.854+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:40:46.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:40:46.879+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:40:46.923+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.255 seconds
[2025-09-09T16:41:17.328+0000] {processor.py:161} INFO - Started process (PID=6806) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:17.336+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:41:17.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:17.343+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:17.620+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:17.967+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:17.966+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:41:18.007+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:18.006+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:41:18.109+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.949 seconds
[2025-09-09T16:41:48.265+0000] {processor.py:161} INFO - Started process (PID=6809) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:48.266+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:41:48.267+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:48.267+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:48.282+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:41:48.307+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:48.307+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:41:48.320+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:41:48.320+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:41:48.337+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T16:42:18.535+0000] {processor.py:161} INFO - Started process (PID=6812) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:18.537+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:42:18.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:18.538+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:18.555+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:18.596+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:18.596+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:42:18.609+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:18.609+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:42:18.625+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.102 seconds
[2025-09-09T16:42:48.839+0000] {processor.py:161} INFO - Started process (PID=6815) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:48.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:42:48.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:48.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:48.861+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:42:48.885+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:48.885+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:42:48.894+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:42:48.894+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:42:48.906+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T16:43:19.113+0000] {processor.py:161} INFO - Started process (PID=6818) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:19.115+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:43:19.116+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:19.116+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:19.133+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:19.161+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:19.161+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:43:19.171+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:19.171+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:43:19.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.088 seconds
[2025-09-09T16:43:49.381+0000] {processor.py:161} INFO - Started process (PID=6821) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:49.382+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:43:49.383+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:49.382+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:49.395+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:43:49.426+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:49.426+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:43:49.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:43:49.436+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:43:49.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T16:44:19.663+0000] {processor.py:161} INFO - Started process (PID=6824) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:19.666+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:44:19.668+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:19.667+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:19.684+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:19.711+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:19.711+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:44:19.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:19.721+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:44:19.735+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:44:49.908+0000] {processor.py:161} INFO - Started process (PID=6827) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:49.909+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:44:49.911+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:49.910+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:49.926+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:44:49.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:49.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:44:49.966+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:44:49.965+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:44:49.982+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:45:20.238+0000] {processor.py:161} INFO - Started process (PID=6830) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:20.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:45:20.241+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:20.241+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:20.255+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:20.288+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:20.288+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:45:20.299+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:20.299+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:45:20.314+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T16:45:50.530+0000] {processor.py:161} INFO - Started process (PID=6833) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:50.531+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:45:50.531+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:50.531+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:50.544+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:45:50.566+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:50.566+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:45:50.575+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:45:50.575+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:45:50.593+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T16:46:20.795+0000] {processor.py:161} INFO - Started process (PID=6836) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:20.796+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:46:20.797+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:20.797+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:20.817+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:20.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:20.853+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:46:20.871+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:20.870+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:46:20.894+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.108 seconds
[2025-09-09T16:46:51.077+0000] {processor.py:161} INFO - Started process (PID=6839) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:51.078+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:46:51.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:51.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:51.093+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:46:51.119+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:51.119+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:46:51.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:46:51.129+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:46:51.145+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T16:47:21.348+0000] {processor.py:161} INFO - Started process (PID=6842) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:21.349+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:47:21.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:21.350+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:21.363+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:21.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:21.388+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:47:21.398+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:21.398+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:47:21.414+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T16:47:51.632+0000] {processor.py:161} INFO - Started process (PID=6845) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:51.632+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:47:51.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:51.633+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:51.645+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:47:51.668+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:51.668+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:47:51.681+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:47:51.681+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:47:51.696+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T16:48:21.891+0000] {processor.py:161} INFO - Started process (PID=6848) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:21.892+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:48:21.893+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:21.892+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:21.907+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:21.935+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:21.935+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:48:21.945+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:21.945+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:48:21.965+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T16:48:52.148+0000] {processor.py:161} INFO - Started process (PID=6851) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:52.149+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:48:52.149+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:52.149+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:52.163+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:48:52.185+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:52.185+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:48:52.194+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:48:52.194+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:48:52.208+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T16:49:22.393+0000] {processor.py:161} INFO - Started process (PID=6854) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:22.394+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:49:22.394+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:22.394+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:22.406+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:22.427+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:22.427+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:49:22.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:22.436+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:49:22.450+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T16:49:52.637+0000] {processor.py:161} INFO - Started process (PID=6857) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:52.638+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:49:52.638+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:52.638+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:52.651+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:49:52.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:52.673+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:49:52.682+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:49:52.682+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:49:52.695+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T16:50:22.880+0000] {processor.py:161} INFO - Started process (PID=6860) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:22.881+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:50:22.882+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:22.882+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:22.897+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:22.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:22.917+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:50:22.926+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:22.926+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:50:22.940+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T16:50:53.096+0000] {processor.py:161} INFO - Started process (PID=6863) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:53.097+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:50:53.099+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:53.098+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:53.121+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:50:53.144+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:53.144+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:50:53.153+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:50:53.153+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:50:53.167+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T16:51:23.301+0000] {processor.py:161} INFO - Started process (PID=6866) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:23.302+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:51:23.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:51:23.303+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:23.317+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:23.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:51:23.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:51:23.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:51:23.351+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:51:23.366+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T16:51:44.435+0000] {processor.py:161} INFO - Started process (PID=6867) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:44.436+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:51:44.436+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:51:44.436+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:44.477+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:51:44.453+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 67, in <module>
    gold_transform = SparkSubmitOperator(
                     ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 899, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 232, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'gold_transform' has already been added to the DAG
[2025-09-09T16:51:44.479+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:51:44.507+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T16:52:14.692+0000] {processor.py:161} INFO - Started process (PID=6870) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:14.693+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:52:14.694+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:14.693+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:14.708+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:14.703+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 67, in <module>
    gold_transform = SparkSubmitOperator(
                     ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 899, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 232, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'gold_transform' has already been added to the DAG
[2025-09-09T16:52:14.709+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:14.731+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.045 seconds
[2025-09-09T16:52:44.912+0000] {processor.py:161} INFO - Started process (PID=6873) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:44.913+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:52:44.914+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:44.913+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:44.927+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:44.924+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 67, in <module>
    gold_transform = SparkSubmitOperator(
                     ^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 123, in __init__
    super().__init__(**kwargs)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 484, in apply_defaults
    result = func(self, **kwargs, default_args=default_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 899, in __init__
    task_group.add(self)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/utils/task_group.py", line 232, in add
    raise DuplicateTaskIdFound(f"{node_type} id '{key}' has already been added to the DAG")
airflow.exceptions.DuplicateTaskIdFound: Task id 'gold_transform' has already been added to the DAG
[2025-09-09T16:52:44.928+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:44.945+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.039 seconds
[2025-09-09T16:52:50.985+0000] {processor.py:161} INFO - Started process (PID=6874) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:50.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:52:50.988+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:50.988+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:50.999+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:52:50.997+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 66
    C:\Users\msi\Desktop\jd\spark\src\elt\load\upload_jobs_to_dwh.py
       ^
SyntaxError: unexpected character after line continuation character
[2025-09-09T16:52:50.999+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:52:51.018+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.043 seconds
[2025-09-09T16:53:01.684+0000] {processor.py:161} INFO - Started process (PID=6877) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:01.685+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:53:01.686+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:01.685+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:01.708+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:01.856+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:01.856+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:53:01.863+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:01.863+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:53:01.888+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.210 seconds
[2025-09-09T16:53:02.704+0000] {processor.py:161} INFO - Started process (PID=6878) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:02.705+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:53:02.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:02.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:02.719+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:02.729+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:02.729+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:53:02.739+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:02.739+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:53:02.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.054 seconds
[2025-09-09T16:53:32.954+0000] {processor.py:161} INFO - Started process (PID=6980) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:32.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:53:32.958+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:32.958+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:32.976+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:53:33.006+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:33.006+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:53:33.019+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:53:33.018+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:53:33.033+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T16:54:03.702+0000] {processor.py:161} INFO - Started process (PID=7010) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:03.704+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:54:03.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:03.705+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:03.741+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:03.769+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:03.768+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:54:03.778+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:03.778+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:54:03.868+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.171 seconds
[2025-09-09T16:54:35.531+0000] {processor.py:161} INFO - Started process (PID=7087) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:35.550+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:54:35.597+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:35.571+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:36.271+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:54:37.563+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:37.561+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:54:38.287+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:54:38.286+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:54:38.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.891 seconds
[2025-09-09T16:55:09.111+0000] {processor.py:161} INFO - Started process (PID=7220) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:09.125+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:55:09.131+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:09.129+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:09.183+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:09.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:09.258+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:55:09.289+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:09.289+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:55:09.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.311 seconds
[2025-09-09T16:55:40.237+0000] {processor.py:161} INFO - Started process (PID=7247) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:40.240+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:55:40.242+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:40.241+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:40.557+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:55:40.821+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:40.821+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:55:42.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:55:42.114+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:55:47.667+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 7.513 seconds
[2025-09-09T16:56:17.911+0000] {processor.py:161} INFO - Started process (PID=7366) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:17.918+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:56:17.922+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:17.920+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:17.996+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:18.066+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:18.065+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:56:18.087+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:18.086+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:56:18.125+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.242 seconds
[2025-09-09T16:56:48.423+0000] {processor.py:161} INFO - Started process (PID=7490) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:48.425+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:56:48.426+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:48.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:48.492+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:56:48.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:48.633+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:56:48.915+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:56:48.915+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:56:53.121+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.721 seconds
[2025-09-09T16:57:23.294+0000] {processor.py:161} INFO - Started process (PID=7848) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:23.296+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:57:23.298+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:23.297+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:23.350+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:23.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:23.379+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:57:23.392+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:23.392+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:57:23.415+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.130 seconds
[2025-09-09T16:57:53.903+0000] {processor.py:161} INFO - Started process (PID=8077) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:53.906+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:57:53.907+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:53.907+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:53.925+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:57:53.949+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:53.949+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:57:53.958+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:57:53.958+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:57:53.973+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T16:58:24.136+0000] {processor.py:161} INFO - Started process (PID=8080) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:24.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:58:24.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:24.138+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:24.153+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:24.181+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:24.181+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:58:24.193+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:24.192+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:58:24.207+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T16:58:54.449+0000] {processor.py:161} INFO - Started process (PID=8083) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:54.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:58:54.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:54.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:54.467+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:58:54.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:54.494+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:58:54.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:58:54.503+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:58:54.517+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T16:59:24.710+0000] {processor.py:161} INFO - Started process (PID=8086) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:24.711+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:59:24.712+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:24.711+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:24.723+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:24.745+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:24.745+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:59:24.753+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:24.753+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:59:24.771+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T16:59:54.980+0000] {processor.py:161} INFO - Started process (PID=8089) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:54.981+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T16:59:54.982+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:54.982+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:54.996+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T16:59:55.021+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:55.021+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T16:59:55.033+0000] {logging_mixin.py:188} INFO - [2025-09-09T16:59:55.032+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T16:59:55.050+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T17:00:25.258+0000] {processor.py:161} INFO - Started process (PID=8092) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:25.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:00:25.259+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:25.259+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:25.271+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:25.292+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:25.292+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:00:25.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:25.300+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:00:25.312+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T17:00:55.491+0000] {processor.py:161} INFO - Started process (PID=8095) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:55.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:00:55.493+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:55.493+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:55.505+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:00:55.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:55.525+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:00:55.533+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:00:55.533+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:00:55.546+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T17:01:25.715+0000] {processor.py:161} INFO - Started process (PID=8096) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:25.716+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:01:25.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:25.717+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:25.731+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:25.758+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:25.758+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:01:25.769+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:25.769+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:01:25.784+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T17:01:56.023+0000] {processor.py:161} INFO - Started process (PID=8099) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:56.024+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:01:56.024+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:56.024+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:56.037+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:01:56.058+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:56.058+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:01:56.067+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:01:56.067+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:01:56.080+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:02:26.302+0000] {processor.py:161} INFO - Started process (PID=8102) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:26.303+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:02:26.304+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:26.304+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:26.318+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:26.341+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:26.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:02:26.351+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:26.351+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:02:26.363+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T17:02:56.630+0000] {processor.py:161} INFO - Started process (PID=8105) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:56.631+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:02:56.632+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:56.631+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:56.644+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:02:56.665+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:56.665+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:02:56.674+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:02:56.674+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:02:56.687+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:03:26.939+0000] {processor.py:161} INFO - Started process (PID=8108) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:26.940+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:03:26.940+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:26.940+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:26.954+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:26.975+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:26.975+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:03:26.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:26.983+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:03:26.997+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:03:57.257+0000] {processor.py:161} INFO - Started process (PID=8111) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:57.258+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:03:57.258+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:57.258+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:57.270+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:03:57.291+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:57.291+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:03:57.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:03:57.300+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:03:57.313+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:04:27.542+0000] {processor.py:161} INFO - Started process (PID=8114) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:27.543+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:04:27.544+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:27.544+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:27.556+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:27.577+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:27.577+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:04:27.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:27.587+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:04:27.600+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:04:57.805+0000] {processor.py:161} INFO - Started process (PID=8117) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:57.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:04:57.807+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:57.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:57.818+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:04:57.839+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:57.839+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:04:57.848+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:04:57.848+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:04:57.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T17:05:28.076+0000] {processor.py:161} INFO - Started process (PID=8120) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:28.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:05:28.078+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:28.078+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:28.092+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:28.115+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:28.115+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:05:28.125+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:28.124+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:05:28.136+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T17:05:58.365+0000] {processor.py:161} INFO - Started process (PID=8123) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:58.367+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:05:58.367+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:58.367+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:58.383+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:05:58.408+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:58.408+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:05:58.418+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:05:58.417+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:05:58.430+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T17:06:28.638+0000] {processor.py:161} INFO - Started process (PID=8126) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:28.639+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:06:28.640+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:28.640+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:28.653+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:28.675+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:28.675+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:06:28.683+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:28.683+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:06:28.697+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:06:58.917+0000] {processor.py:161} INFO - Started process (PID=8129) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:58.917+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:06:58.918+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:58.918+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:58.930+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:06:58.953+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:58.953+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:06:58.962+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:06:58.962+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:06:58.974+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:07:29.216+0000] {processor.py:161} INFO - Started process (PID=8132) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:29.216+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:07:29.217+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:29.217+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:29.229+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:29.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:29.251+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:07:29.260+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:29.260+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:07:29.273+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:07:59.523+0000] {processor.py:161} INFO - Started process (PID=8135) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:59.524+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:07:59.525+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:59.525+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:59.538+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:07:59.561+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:59.561+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:07:59.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:07:59.570+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:07:59.655+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.137 seconds
[2025-09-09T17:08:29.848+0000] {processor.py:161} INFO - Started process (PID=8138) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:08:29.849+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:08:29.850+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:08:29.850+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:08:29.864+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:08:29.892+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:08:29.892+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:08:29.903+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:08:29.903+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:08:29.919+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.076 seconds
[2025-09-09T17:09:00.137+0000] {processor.py:161} INFO - Started process (PID=8141) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:00.138+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:09:00.139+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:00.139+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:00.152+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:00.174+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:00.174+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:09:00.183+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:00.182+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:09:00.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T17:09:30.385+0000] {processor.py:161} INFO - Started process (PID=8144) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:30.386+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:09:30.387+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:30.387+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:30.401+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:09:30.428+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:30.428+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:09:30.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:09:30.437+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:09:30.452+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T17:10:00.636+0000] {processor.py:161} INFO - Started process (PID=8147) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:00.637+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:10:00.638+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:00.638+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:00.655+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:00.684+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:00.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:10:00.697+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:00.696+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:10:00.721+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.092 seconds
[2025-09-09T17:10:30.907+0000] {processor.py:161} INFO - Started process (PID=8150) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:30.908+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:10:30.908+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:30.908+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:30.926+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:10:30.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:30.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:10:30.966+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:10:30.966+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:10:30.986+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T17:11:01.146+0000] {processor.py:161} INFO - Started process (PID=8153) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:01.147+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:11:01.148+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:01.147+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:01.162+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:01.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:01.186+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:11:01.196+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:01.196+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:11:01.210+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T17:11:31.377+0000] {processor.py:161} INFO - Started process (PID=8156) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:31.378+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:11:31.379+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:31.379+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:31.394+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:11:31.425+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:31.424+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:11:31.438+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:11:31.437+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:11:31.466+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.096 seconds
[2025-09-09T17:12:01.650+0000] {processor.py:161} INFO - Started process (PID=8159) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:01.651+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:12:01.652+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:01.652+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:01.671+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:01.700+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:01.700+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:12:01.710+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:01.710+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:12:01.723+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T17:12:26.877+0000] {processor.py:161} INFO - Started process (PID=8162) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:26.878+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:12:26.878+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:26.878+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:26.903+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:26.896+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 77, in <module>
    silver_group >> gold_transform >> load_data_to_dwh
    ^^^^^^^^^^^^
NameError: name 'silver_group' is not defined
[2025-09-09T17:12:26.904+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:26.928+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.058 seconds
[2025-09-09T17:12:34.941+0000] {processor.py:161} INFO - Started process (PID=8163) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:34.942+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:12:34.943+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:34.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:34.962+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:35.137+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:35.137+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:12:35.147+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:35.147+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:12:35.166+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.232 seconds
[2025-09-09T17:12:36.007+0000] {processor.py:161} INFO - Started process (PID=8164) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:36.008+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:12:36.009+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:36.009+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:36.029+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:12:36.041+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:36.041+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:12:36.052+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:12:36.051+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:12:36.069+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T17:13:06.262+0000] {processor.py:161} INFO - Started process (PID=8264) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:06.263+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:13:06.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:06.264+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:06.280+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:06.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:06.311+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:13:06.324+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:06.323+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:13:06.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.092 seconds
[2025-09-09T17:13:36.503+0000] {processor.py:161} INFO - Started process (PID=8300) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:36.506+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:13:36.507+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:36.507+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:36.528+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:13:36.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:36.559+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:13:36.570+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:13:36.570+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:13:36.644+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.150 seconds
[2025-09-09T17:14:07.727+0000] {processor.py:161} INFO - Started process (PID=8401) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:07.731+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:14:07.733+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:07.732+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:08.707+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:09.859+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:09.858+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:14:10.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:10.261+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:14:10.589+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.902 seconds
[2025-09-09T17:14:40.782+0000] {processor.py:161} INFO - Started process (PID=8403) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:40.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:14:40.784+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:40.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:40.800+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:14:40.825+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:40.825+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:14:40.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:14:40.837+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:14:40.852+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T17:15:11.020+0000] {processor.py:161} INFO - Started process (PID=8406) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:11.022+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:15:11.024+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:11.024+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:11.040+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:11.066+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:11.066+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:15:11.080+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:11.080+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:15:11.097+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T17:15:41.438+0000] {processor.py:161} INFO - Started process (PID=8507) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:41.442+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:15:41.443+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:41.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:41.468+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:15:41.535+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:41.535+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:15:41.549+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:15:41.549+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:15:41.618+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.191 seconds
[2025-09-09T17:16:13.507+0000] {processor.py:161} INFO - Started process (PID=8510) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:13.688+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:16:13.744+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:13.719+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:14.900+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:17.016+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:17.012+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:16:17.316+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:17.316+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:16:17.512+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 4.587 seconds
[2025-09-09T17:16:49.098+0000] {processor.py:161} INFO - Started process (PID=8546) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:49.112+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:16:49.156+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:49.141+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:49.745+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:16:50.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:50.694+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:16:51.447+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:16:51.447+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:16:52.460+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 3.872 seconds
[2025-09-09T17:17:22.732+0000] {processor.py:161} INFO - Started process (PID=8743) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:22.734+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:17:22.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:22.736+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:22.763+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:22.808+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:22.808+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:17:22.823+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:22.823+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:17:22.849+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.127 seconds
[2025-09-09T17:17:53.048+0000] {processor.py:161} INFO - Started process (PID=8846) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:53.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:17:53.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:53.051+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:53.068+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:17:53.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:53.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:17:53.117+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:17:53.116+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:17:53.134+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.097 seconds
[2025-09-09T17:18:23.338+0000] {processor.py:161} INFO - Started process (PID=8849) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:23.339+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:18:23.339+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:23.339+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:23.352+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:23.376+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:23.376+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:18:23.386+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:23.385+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:18:23.400+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T17:18:53.620+0000] {processor.py:161} INFO - Started process (PID=8852) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:53.621+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:18:53.622+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:53.622+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:53.633+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:18:53.655+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:53.655+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:18:53.664+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:18:53.664+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:18:53.677+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:19:23.881+0000] {processor.py:161} INFO - Started process (PID=8855) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:23.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:19:23.884+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:23.883+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:23.897+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:23.921+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:23.921+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:19:23.933+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:23.932+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:19:23.949+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T17:19:54.186+0000] {processor.py:161} INFO - Started process (PID=8858) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:54.187+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:19:54.188+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:54.188+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:54.200+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:19:54.225+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:54.225+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:19:54.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:19:54.236+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:19:54.250+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T17:20:24.441+0000] {processor.py:161} INFO - Started process (PID=8861) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:24.441+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:20:24.442+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:24.442+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:24.453+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:24.474+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:24.473+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:20:24.483+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:24.483+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:20:24.498+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:20:54.723+0000] {processor.py:161} INFO - Started process (PID=8864) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:54.723+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:20:54.724+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:54.724+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:54.735+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:20:54.755+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:54.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:20:54.763+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:20:54.763+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:20:54.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.059 seconds
[2025-09-09T17:21:24.975+0000] {processor.py:161} INFO - Started process (PID=8867) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:24.975+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:21:24.976+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:24.976+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:24.988+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:25.007+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:25.007+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:21:25.017+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:25.017+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:21:25.030+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T17:21:49.174+0000] {processor.py:161} INFO - Started process (PID=8868) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:49.175+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:21:49.176+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:49.175+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:49.196+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:21:49.331+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:49.330+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:21:49.337+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:21:49.337+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:21:49.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.183 seconds
[2025-09-09T17:22:19.623+0000] {processor.py:161} INFO - Started process (PID=8871) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:19.624+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:22:19.625+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:19.624+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:19.642+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:19.672+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:19.672+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:22:19.684+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:19.684+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:22:19.701+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T17:22:49.936+0000] {processor.py:161} INFO - Started process (PID=8874) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:49.937+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:22:49.938+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:49.937+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:49.949+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:22:49.973+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:49.972+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:22:49.981+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:22:49.981+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:22:49.994+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:23:20.222+0000] {processor.py:161} INFO - Started process (PID=8877) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:20.223+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:23:20.224+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:20.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:20.239+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:20.262+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:20.262+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:23:20.272+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:20.272+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:23:20.285+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T17:23:50.509+0000] {processor.py:161} INFO - Started process (PID=8880) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:50.509+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:23:50.510+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:50.510+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:50.523+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:23:50.550+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:50.550+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:23:50.561+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:23:50.561+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:23:50.575+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T17:24:20.814+0000] {processor.py:161} INFO - Started process (PID=8883) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:20.815+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:24:20.816+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:20.816+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:20.830+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:20.853+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:20.853+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:24:20.862+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:20.862+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:24:20.875+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T17:24:51.085+0000] {processor.py:161} INFO - Started process (PID=8886) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:51.085+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:24:51.086+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:51.086+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:51.099+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:24:51.120+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:51.120+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:24:51.129+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:24:51.129+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:24:51.143+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:25:21.885+0000] {processor.py:161} INFO - Started process (PID=8957) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:21.891+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:25:21.895+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:21.894+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:22.061+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:22.233+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:22.232+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:25:22.292+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:22.292+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:25:22.346+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.695 seconds
[2025-09-09T17:25:52.579+0000] {processor.py:161} INFO - Started process (PID=9021) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:52.582+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:25:52.583+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:52.582+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:52.606+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:25:52.646+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:52.646+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:25:52.664+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:25:52.663+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:25:52.736+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.170 seconds
[2025-09-09T17:26:23.008+0000] {processor.py:161} INFO - Started process (PID=9247) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:26:23.012+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:26:23.014+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:26:23.013+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:26:23.085+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:26:23.123+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:26:23.123+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:26:23.139+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:26:23.139+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:26:31.283+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 8.319 seconds
[2025-09-09T17:27:01.508+0000] {processor.py:161} INFO - Started process (PID=9485) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:01.511+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:27:01.512+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:01.512+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:01.533+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:01.567+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:01.566+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:27:01.581+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:01.581+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:27:04.202+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.699 seconds
[2025-09-09T17:27:34.508+0000] {processor.py:161} INFO - Started process (PID=9928) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:34.510+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:27:34.511+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:34.511+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:34.534+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:27:34.574+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:34.574+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:27:34.591+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:27:34.590+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:27:34.609+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.111 seconds
[2025-09-09T17:28:04.847+0000] {processor.py:161} INFO - Started process (PID=10073) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:04.850+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:28:04.856+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:04.854+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:04.922+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:05.002+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:05.002+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:28:05.022+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:05.022+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:28:05.051+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.272 seconds
[2025-09-09T17:28:35.249+0000] {processor.py:161} INFO - Started process (PID=10266) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:35.250+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:28:35.251+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:35.251+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:35.267+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:28:35.293+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:35.292+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:28:35.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:28:35.303+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:28:35.321+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T17:29:05.484+0000] {processor.py:161} INFO - Started process (PID=10269) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:05.485+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:29:05.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:05.485+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:05.498+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:05.518+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:05.518+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:29:05.529+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:05.528+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:29:05.544+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T17:29:35.762+0000] {processor.py:161} INFO - Started process (PID=10272) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:35.763+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:29:35.764+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:35.763+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:35.777+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:29:35.798+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:35.797+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:29:35.807+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:29:35.807+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:29:35.819+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:30:06.003+0000] {processor.py:161} INFO - Started process (PID=10275) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:06.003+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:30:06.004+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:06.004+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:06.017+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:06.042+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:06.042+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:30:06.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:06.051+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:30:06.065+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T17:30:36.272+0000] {processor.py:161} INFO - Started process (PID=10278) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:36.273+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:30:36.274+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:36.274+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:36.287+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:30:36.308+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:36.308+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:30:36.317+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:30:36.316+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:30:36.333+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T17:31:06.546+0000] {processor.py:161} INFO - Started process (PID=10281) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:06.546+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:31:06.547+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:06.547+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:06.561+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:06.584+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:06.583+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:31:06.594+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:06.594+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:31:06.608+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T17:31:36.790+0000] {processor.py:161} INFO - Started process (PID=10284) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:36.791+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:31:36.791+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:36.791+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:36.810+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:31:36.836+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:36.835+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:31:36.847+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:31:36.847+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:31:36.861+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T17:32:06.991+0000] {processor.py:161} INFO - Started process (PID=10285) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:06.992+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:32:06.993+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:06.993+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:07.007+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:07.039+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:07.039+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:32:07.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:07.050+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:32:07.071+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T17:32:37.313+0000] {processor.py:161} INFO - Started process (PID=10288) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:37.313+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:32:37.314+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:37.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:37.329+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:32:37.355+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:37.355+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:32:37.366+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:32:37.366+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:32:37.379+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T17:33:07.642+0000] {processor.py:161} INFO - Started process (PID=10291) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:07.643+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:33:07.643+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:07.643+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:07.657+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:07.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:07.678+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:33:07.687+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:07.687+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:33:07.702+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T17:33:37.949+0000] {processor.py:161} INFO - Started process (PID=10294) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:37.949+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:33:37.950+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:37.950+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:37.961+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:33:37.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:37.983+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:33:37.991+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:33:37.991+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:33:38.004+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:34:08.307+0000] {processor.py:161} INFO - Started process (PID=10297) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:08.308+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:34:08.309+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:08.308+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:08.321+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:08.342+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:08.341+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:34:08.350+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:08.350+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:34:08.362+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:34:38.604+0000] {processor.py:161} INFO - Started process (PID=10300) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:38.604+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:34:38.605+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:38.605+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:38.618+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:34:38.639+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:38.639+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:34:38.648+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:34:38.648+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:34:38.662+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:35:08.920+0000] {processor.py:161} INFO - Started process (PID=10303) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:08.921+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:35:08.922+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:08.921+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:08.933+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:08.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:08.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:35:08.963+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:08.963+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:35:08.976+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:35:39.172+0000] {processor.py:161} INFO - Started process (PID=10306) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:39.173+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:35:39.174+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:39.174+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:39.187+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:35:39.212+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:39.212+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:35:39.222+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:35:39.222+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:35:39.235+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T17:36:09.489+0000] {processor.py:161} INFO - Started process (PID=10309) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:09.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:36:09.490+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:09.490+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:09.505+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:09.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:09.530+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:36:09.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:09.543+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:36:09.562+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T17:36:39.773+0000] {processor.py:161} INFO - Started process (PID=10312) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:39.774+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:36:39.774+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:39.774+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:39.787+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:36:39.811+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:39.811+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:36:39.820+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:36:39.820+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:36:39.833+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T17:37:10.055+0000] {processor.py:161} INFO - Started process (PID=10315) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:10.056+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:37:10.056+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:10.056+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:10.068+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:10.089+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:10.089+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:37:10.098+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:10.097+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:37:10.110+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:37:40.312+0000] {processor.py:161} INFO - Started process (PID=10318) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:40.314+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:37:40.315+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:40.314+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:40.329+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:37:40.353+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:40.353+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:37:40.363+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:37:40.363+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:37:40.378+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.072 seconds
[2025-09-09T17:38:10.626+0000] {processor.py:161} INFO - Started process (PID=10321) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:10.627+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:38:10.627+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:10.627+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:10.640+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:10.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:10.660+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:38:10.669+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:10.669+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:38:10.682+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:38:40.964+0000] {processor.py:161} INFO - Started process (PID=10324) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:40.965+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:38:40.966+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:40.966+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:40.978+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:38:41.002+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:41.002+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:38:41.011+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:38:41.011+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:38:41.023+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T17:39:11.258+0000] {processor.py:161} INFO - Started process (PID=10327) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:11.259+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:39:11.260+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:11.260+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:11.271+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:11.294+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:11.294+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:39:11.303+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:11.303+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:39:11.317+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T17:39:41.558+0000] {processor.py:161} INFO - Started process (PID=10330) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:41.559+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:39:41.559+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:41.559+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:41.571+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:39:41.592+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:41.592+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:39:41.601+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:39:41.601+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:39:41.615+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:40:11.839+0000] {processor.py:161} INFO - Started process (PID=10333) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:11.840+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:40:11.841+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:11.841+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:11.855+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:11.878+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:11.877+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:40:11.895+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:11.895+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:40:11.912+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T17:40:42.108+0000] {processor.py:161} INFO - Started process (PID=10336) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:42.108+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:40:42.109+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:42.109+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:42.121+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:40:42.142+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:42.142+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:40:42.151+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:40:42.151+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:40:42.165+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T17:41:12.417+0000] {processor.py:161} INFO - Started process (PID=10339) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:12.418+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:41:12.418+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:12.418+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:12.430+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:12.451+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:12.451+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:41:12.461+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:12.460+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:41:12.474+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T17:41:42.719+0000] {processor.py:161} INFO - Started process (PID=10342) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:42.720+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:41:42.721+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:42.720+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:42.733+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:41:42.754+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:42.754+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:41:42.763+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:41:42.763+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:41:42.776+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T17:42:12.972+0000] {processor.py:161} INFO - Started process (PID=10345) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:12.973+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:42:12.974+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:12.974+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:12.987+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:13.008+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:13.007+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:42:13.016+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:13.015+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:42:13.028+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T17:42:43.255+0000] {processor.py:161} INFO - Started process (PID=10348) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:43.256+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:42:43.257+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:43.256+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:43.272+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:42:43.298+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:43.297+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:42:43.306+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:42:43.306+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:42:43.319+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T17:43:46.779+0000] {processor.py:161} INFO - Started process (PID=10349) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:43:46.783+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:43:46.784+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:43:46.784+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:43:46.804+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:43:46.866+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:43:46.866+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:43:46.877+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:43:46.877+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:43:47.001+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.228 seconds
[2025-09-09T17:44:17.487+0000] {processor.py:161} INFO - Started process (PID=10352) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:17.491+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:44:17.500+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:17.495+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:17.673+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:17.885+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:17.885+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:44:17.944+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:17.943+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:44:17.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.591 seconds
[2025-09-09T17:44:38.612+0000] {processor.py:161} INFO - Started process (PID=10355) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:38.613+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:44:38.614+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:38.613+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:38.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:38.623+0000] {dagbag.py:355} ERROR - Failed to import: /opt/airflow/dags/etl_jobs_dag.py
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/dagbag.py", line 351, in parse
    loader.exec_module(new_module)
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1074, in get_code
  File "<frozen importlib._bootstrap_external>", line 1004, in source_to_code
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/opt/airflow/dags/etl_jobs_dag.py", line 73
    )+
      ^
SyntaxError: invalid syntax
[2025-09-09T17:44:38.630+0000] {processor.py:842} WARNING - No viable dags retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:38.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.047 seconds
[2025-09-09T17:44:43.653+0000] {processor.py:161} INFO - Started process (PID=10356) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:43.654+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:44:43.655+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:43.655+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:43.681+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:44:43.705+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:43.705+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:44:43.714+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:44:43.714+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:44:43.734+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T17:45:13.868+0000] {processor.py:161} INFO - Started process (PID=10359) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:13.869+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:45:13.869+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:13.869+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:13.883+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:13.909+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:13.909+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:45:13.920+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:13.919+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:45:13.933+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:45:44.127+0000] {processor.py:161} INFO - Started process (PID=10362) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:44.128+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:45:44.130+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:44.129+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:44.146+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:45:44.173+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:44.173+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:45:44.183+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:45:44.182+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:45:44.196+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T17:46:14.437+0000] {processor.py:161} INFO - Started process (PID=10365) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:14.438+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:46:14.439+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:14.439+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:14.456+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:14.486+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:14.485+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:46:14.494+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:14.494+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:46:14.507+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T17:46:33.658+0000] {processor.py:161} INFO - Started process (PID=10368) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:33.659+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:46:33.660+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:33.659+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:33.681+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:46:33.706+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:33.706+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:46:33.717+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:46:33.717+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:46:33.738+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.085 seconds
[2025-09-09T17:47:03.903+0000] {processor.py:161} INFO - Started process (PID=10371) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:03.904+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:47:03.905+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:03.905+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:03.919+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:03.946+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:03.945+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:47:03.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:03.954+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:47:03.969+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T17:47:34.133+0000] {processor.py:161} INFO - Started process (PID=10374) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:34.134+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:47:34.135+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:34.135+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:34.165+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:47:34.203+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:34.203+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:47:34.213+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:47:34.213+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:47:34.227+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.103 seconds
[2025-09-09T17:48:04.429+0000] {processor.py:161} INFO - Started process (PID=10377) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:04.431+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:48:04.432+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:04.431+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:04.451+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:04.483+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:04.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:48:04.495+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:04.494+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:48:04.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T17:48:34.669+0000] {processor.py:161} INFO - Started process (PID=10380) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:34.670+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:48:34.670+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:34.670+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:34.682+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:48:34.703+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:34.703+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:48:34.712+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:48:34.712+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:48:34.725+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:49:04.884+0000] {processor.py:161} INFO - Started process (PID=10383) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:04.884+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:49:04.885+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:04.885+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:04.898+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:04.922+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:04.922+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:49:04.932+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:04.932+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:49:04.948+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:49:35.073+0000] {processor.py:161} INFO - Started process (PID=10384) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:35.074+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:49:35.075+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:35.074+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:35.091+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:49:35.124+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:35.124+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:49:35.136+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:49:35.135+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:49:35.157+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T17:50:05.339+0000] {processor.py:161} INFO - Started process (PID=10387) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:05.340+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:50:05.340+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:05.340+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:05.354+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:05.378+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:05.378+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:50:05.389+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:05.388+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:50:05.407+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T17:50:35.588+0000] {processor.py:161} INFO - Started process (PID=10390) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:35.589+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:50:35.590+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:35.589+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:35.604+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:50:35.629+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:35.629+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:50:35.638+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:50:35.638+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:50:35.652+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:51:05.875+0000] {processor.py:161} INFO - Started process (PID=10393) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:05.876+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:51:05.876+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:05.876+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:05.894+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:05.924+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:05.924+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:51:05.934+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:05.933+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:51:05.950+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.082 seconds
[2025-09-09T17:51:36.151+0000] {processor.py:161} INFO - Started process (PID=10396) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:36.152+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:51:36.152+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:36.152+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:36.166+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:51:36.191+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:36.190+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:51:36.199+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:51:36.199+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:51:36.212+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T17:52:06.411+0000] {processor.py:161} INFO - Started process (PID=10399) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:06.411+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:52:06.412+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:06.412+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:06.426+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:06.449+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:06.449+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:52:06.458+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:06.458+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:52:06.475+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T17:52:36.664+0000] {processor.py:161} INFO - Started process (PID=10402) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:36.665+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:52:36.666+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:36.666+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:36.682+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:52:36.704+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:36.704+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:52:36.714+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:52:36.714+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:52:36.729+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:53:06.937+0000] {processor.py:161} INFO - Started process (PID=10405) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:06.938+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:53:06.939+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:06.938+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:06.955+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:06.991+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:06.991+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:53:07.002+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:07.002+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:53:07.020+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.090 seconds
[2025-09-09T17:53:37.208+0000] {processor.py:161} INFO - Started process (PID=10408) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:37.209+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:53:37.210+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:37.209+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:37.224+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:53:37.253+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:37.253+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:53:37.264+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:53:37.264+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:53:37.278+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.077 seconds
[2025-09-09T17:54:07.449+0000] {processor.py:161} INFO - Started process (PID=10411) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:07.450+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:54:07.452+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:07.451+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:07.466+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:07.496+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:07.495+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:54:07.509+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:07.509+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:54:07.525+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.083 seconds
[2025-09-09T17:54:37.694+0000] {processor.py:161} INFO - Started process (PID=10414) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:37.695+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:54:37.696+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:37.696+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:37.709+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:54:37.735+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:37.735+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:54:37.744+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:54:37.744+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:54:37.758+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T17:55:07.929+0000] {processor.py:161} INFO - Started process (PID=10417) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:07.930+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:55:07.931+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:07.930+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:07.945+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:07.970+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:07.970+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:55:07.980+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:07.980+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:55:07.993+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T17:55:38.202+0000] {processor.py:161} INFO - Started process (PID=10420) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:38.204+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:55:38.205+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:38.204+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:38.218+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:55:38.245+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:38.245+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:55:38.254+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:55:38.254+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:55:38.267+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:56:08.442+0000] {processor.py:161} INFO - Started process (PID=10423) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:08.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:56:08.444+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:08.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:08.457+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:08.483+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:08.483+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:56:08.492+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:08.492+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:56:08.506+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T17:56:38.705+0000] {processor.py:161} INFO - Started process (PID=10426) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:38.706+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:56:38.707+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:38.706+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:38.722+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:56:38.747+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:38.746+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:56:38.755+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:56:38.755+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:56:38.769+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T17:57:08.942+0000] {processor.py:161} INFO - Started process (PID=10429) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:08.943+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:57:08.944+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:08.943+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:08.955+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:08.983+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:08.983+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:57:08.994+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:08.994+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:57:09.007+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.070 seconds
[2025-09-09T17:57:39.235+0000] {processor.py:161} INFO - Started process (PID=10432) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:39.236+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:57:39.236+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:39.236+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:39.249+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:57:39.270+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:39.270+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:57:39.278+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:57:39.278+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:57:39.290+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:58:09.505+0000] {processor.py:161} INFO - Started process (PID=10435) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:09.505+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:58:09.506+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:09.506+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:09.519+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:09.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:09.539+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:58:09.548+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:09.548+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:58:09.560+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T17:58:39.777+0000] {processor.py:161} INFO - Started process (PID=10438) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:39.778+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:58:39.779+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:39.778+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:39.790+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:58:39.815+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:39.814+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:58:39.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:58:39.824+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:58:39.839+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T17:59:10.050+0000] {processor.py:161} INFO - Started process (PID=10441) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:10.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:59:10.051+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:10.051+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:10.066+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:10.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:10.092+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:59:10.102+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:10.101+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:59:10.116+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T17:59:40.327+0000] {processor.py:161} INFO - Started process (PID=10444) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:40.328+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T17:59:40.328+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:40.328+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:40.340+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T17:59:40.361+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:40.360+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T17:59:40.369+0000] {logging_mixin.py:188} INFO - [2025-09-09T17:59:40.369+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T17:59:40.383+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T18:00:10.632+0000] {processor.py:161} INFO - Started process (PID=10447) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:10.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:00:10.634+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:10.634+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:10.653+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:10.680+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:10.679+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:00:10.692+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:10.691+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:00:10.709+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.084 seconds
[2025-09-09T18:00:40.918+0000] {processor.py:161} INFO - Started process (PID=10450) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:40.919+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:00:40.919+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:40.919+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:40.931+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:00:40.954+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:40.954+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:00:40.963+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:00:40.963+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:00:40.980+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T18:01:11.184+0000] {processor.py:161} INFO - Started process (PID=10453) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:11.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:01:11.185+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:11.185+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:11.201+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:11.230+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:11.229+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:01:11.240+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:11.240+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:01:11.257+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T18:01:41.489+0000] {processor.py:161} INFO - Started process (PID=10456) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:41.490+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:01:41.491+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:41.491+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:41.507+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:01:41.532+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:41.532+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:01:41.543+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:01:41.543+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:01:41.557+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.074 seconds
[2025-09-09T18:02:11.840+0000] {processor.py:161} INFO - Started process (PID=10459) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:11.841+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:02:11.842+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:11.842+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:11.857+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:11.879+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:11.879+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:02:11.887+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:11.887+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:02:11.900+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T18:02:42.136+0000] {processor.py:161} INFO - Started process (PID=10462) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:42.137+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:02:42.138+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:42.137+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:42.151+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:02:42.178+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:42.178+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:02:42.189+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:02:42.188+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:02:42.206+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.075 seconds
[2025-09-09T18:03:12.491+0000] {processor.py:161} INFO - Started process (PID=10465) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:12.492+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:03:12.492+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:12.492+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:12.505+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:12.527+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:12.527+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:03:12.539+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:12.539+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:03:12.559+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T18:03:42.804+0000] {processor.py:161} INFO - Started process (PID=10468) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:42.806+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:03:42.806+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:42.806+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:42.820+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:03:42.846+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:42.846+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:03:42.857+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:03:42.856+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:03:42.870+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.071 seconds
[2025-09-09T18:04:13.048+0000] {processor.py:161} INFO - Started process (PID=10471) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:13.049+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:04:13.049+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:13.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:13.062+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:13.082+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:13.082+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:04:13.090+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:13.090+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:04:13.103+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.060 seconds
[2025-09-09T18:04:43.263+0000] {processor.py:161} INFO - Started process (PID=10474) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:43.264+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:04:43.265+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:43.265+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:43.278+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:04:43.300+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:43.300+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:04:43.311+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:04:43.311+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:04:43.358+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.100 seconds
[2025-09-09T18:05:13.464+0000] {processor.py:161} INFO - Started process (PID=10475) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:13.464+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:05:13.465+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:13.465+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:13.481+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:13.505+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:13.505+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:05:13.515+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:13.515+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:05:13.530+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.073 seconds
[2025-09-09T18:05:43.801+0000] {processor.py:161} INFO - Started process (PID=10478) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:43.802+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:05:43.802+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:43.802+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:43.815+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:05:43.836+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:43.836+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:05:43.845+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:05:43.845+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:05:43.859+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.064 seconds
[2025-09-09T18:06:14.131+0000] {processor.py:161} INFO - Started process (PID=10481) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:14.132+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:06:14.133+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:14.132+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:14.144+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:14.168+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:14.168+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:06:14.176+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:14.176+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:06:14.188+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.062 seconds
[2025-09-09T18:06:44.421+0000] {processor.py:161} INFO - Started process (PID=10484) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:44.422+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:06:44.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:44.422+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:44.435+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:06:44.456+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:44.456+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:06:44.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:06:44.466+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:06:44.481+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T18:07:14.737+0000] {processor.py:161} INFO - Started process (PID=10487) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:14.738+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:07:14.739+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:14.739+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:14.751+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:14.773+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:14.773+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:07:14.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:14.782+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:07:14.795+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T18:07:45.116+0000] {processor.py:161} INFO - Started process (PID=10490) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:45.117+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:07:45.118+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:45.118+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:45.134+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:07:45.163+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:45.162+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:07:45.172+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:07:45.172+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:07:45.189+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.080 seconds
[2025-09-09T18:08:15.442+0000] {processor.py:161} INFO - Started process (PID=10493) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:15.443+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:08:15.443+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:15.443+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:15.456+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:15.478+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:15.478+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:08:15.487+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:15.487+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:08:15.499+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.063 seconds
[2025-09-09T18:08:45.735+0000] {processor.py:161} INFO - Started process (PID=10496) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:45.736+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:08:45.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:45.736+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:45.749+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:08:45.772+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:45.772+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:08:45.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:08:45.782+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:08:45.796+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.066 seconds
[2025-09-09T18:09:15.986+0000] {processor.py:161} INFO - Started process (PID=10499) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:15.987+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:09:15.988+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:15.987+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:16.002+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:16.025+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:16.025+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:09:16.034+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:16.034+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:09:16.047+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T18:09:46.221+0000] {processor.py:161} INFO - Started process (PID=10502) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:46.222+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:09:46.223+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:46.222+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:46.235+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:09:46.260+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:46.260+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:09:46.269+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:09:46.269+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:09:46.282+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.067 seconds
[2025-09-09T18:10:16.501+0000] {processor.py:161} INFO - Started process (PID=10505) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:16.502+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:10:16.503+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:16.502+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:16.516+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:16.540+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:16.540+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:10:16.550+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:16.550+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:10:16.565+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
[2025-09-09T18:10:46.780+0000] {processor.py:161} INFO - Started process (PID=10508) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:46.781+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:10:46.782+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:46.782+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:46.794+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:10:46.815+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:46.815+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:10:46.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:10:46.823+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:10:46.836+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.061 seconds
[2025-09-09T18:11:17.028+0000] {processor.py:161} INFO - Started process (PID=10511) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:11:17.029+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:11:17.029+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:11:17.029+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:11:17.049+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:11:17.079+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:11:17.079+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:11:17.092+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:11:17.091+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:11:17.115+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.094 seconds
[2025-09-09T18:13:48.940+0000] {processor.py:161} INFO - Started process (PID=10514) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:13:48.977+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:13:48.987+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:13:48.986+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:13:49.680+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:13:49.837+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:13:49.836+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:13:49.897+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:13:49.896+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:13:49.988+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.200 seconds
[2025-09-09T18:14:20.222+0000] {processor.py:161} INFO - Started process (PID=10517) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:20.224+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:14:20.225+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:20.224+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:20.242+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:20.274+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:20.274+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:14:20.286+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:20.286+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:14:20.302+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T18:14:50.451+0000] {processor.py:161} INFO - Started process (PID=10520) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:50.452+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:14:50.453+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:50.452+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:50.467+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:14:50.489+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:50.489+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:14:50.497+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:14:50.497+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:14:50.511+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.065 seconds
[2025-09-09T18:15:20.673+0000] {processor.py:161} INFO - Started process (PID=10523) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:20.674+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:15:20.675+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:20.674+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:20.693+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:20.725+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:20.725+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:15:20.737+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:20.736+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:15:20.753+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T18:15:50.955+0000] {processor.py:161} INFO - Started process (PID=10526) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:50.956+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:15:50.957+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:50.956+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:50.973+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:15:51.002+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:51.002+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:15:51.013+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:15:51.013+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:15:51.027+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.078 seconds
[2025-09-09T18:16:21.194+0000] {processor.py:161} INFO - Started process (PID=10529) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:21.194+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:16:21.195+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:21.195+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:21.211+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:21.243+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:21.243+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:16:21.255+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:21.255+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:16:21.275+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.087 seconds
[2025-09-09T18:16:51.444+0000] {processor.py:161} INFO - Started process (PID=10532) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:51.446+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:16:51.447+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:51.446+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:51.464+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:16:51.491+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:51.491+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:16:51.502+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:16:51.502+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:16:51.516+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.079 seconds
[2025-09-09T18:17:21.677+0000] {processor.py:161} INFO - Started process (PID=10535) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:21.678+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:17:21.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:21.679+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:21.699+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:21.731+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:21.731+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:17:21.743+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:21.743+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:17:21.763+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.097 seconds
[2025-09-09T18:17:51.983+0000] {processor.py:161} INFO - Started process (PID=10538) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:51.984+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:17:51.985+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:51.984+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:52.000+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:17:52.023+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:52.023+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:17:52.032+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:17:52.032+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:17:52.045+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.068 seconds
[2025-09-09T18:18:22.881+0000] {processor.py:161} INFO - Started process (PID=10541) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:23.842+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:18:23.904+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:23.865+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:24.200+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:24.573+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:24.572+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:18:24.692+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:24.692+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:18:25.352+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 2.586 seconds
[2025-09-09T18:18:56.193+0000] {processor.py:161} INFO - Started process (PID=10544) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:56.201+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:18:56.209+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:56.205+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:56.559+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:18:56.889+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:56.889+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:18:56.917+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:18:56.916+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:18:57.111+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.048 seconds
[2025-09-09T18:19:28.070+0000] {processor.py:161} INFO - Started process (PID=10547) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:19:28.077+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:19:28.082+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:19:28.079+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:19:28.288+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:19:28.758+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:19:28.758+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:19:28.863+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:19:28.862+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:19:29.092+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.178 seconds
[2025-09-09T18:20:01.410+0000] {processor.py:161} INFO - Started process (PID=10548) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:01.423+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:20:01.429+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:01.425+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:01.507+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:02.373+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:02.373+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:20:02.416+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:02.416+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:20:02.510+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 1.279 seconds
[2025-09-09T18:20:36.298+0000] {processor.py:161} INFO - Started process (PID=10551) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:36.330+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:20:36.479+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:36.381+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:38.738+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:20:40.701+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:40.684+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:20:40.979+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:20:40.979+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:20:41.343+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 6.308 seconds
[2025-09-09T18:21:11.826+0000] {processor.py:161} INFO - Started process (PID=10554) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:11.828+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:21:11.831+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:11.831+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:11.995+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:12.623+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:12.623+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:21:12.679+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:12.679+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:21:12.719+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.902 seconds
[2025-09-09T18:21:43.044+0000] {processor.py:161} INFO - Started process (PID=10555) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:43.050+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:21:43.053+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:43.052+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:43.102+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:21:43.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:43.185+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:21:43.215+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:21:43.214+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:21:43.243+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.220 seconds
[2025-09-09T18:22:13.465+0000] {processor.py:161} INFO - Started process (PID=10558) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:13.466+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:22:13.466+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:13.466+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:13.482+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:13.516+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:13.515+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:22:13.530+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:13.529+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:22:13.545+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.086 seconds
[2025-09-09T18:22:43.632+0000] {processor.py:161} INFO - Started process (PID=10561) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:43.633+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:22:43.633+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:43.633+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:43.657+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:22:43.683+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:43.683+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:22:43.693+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:22:43.693+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:22:43.707+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.081 seconds
[2025-09-09T18:23:13.880+0000] {processor.py:161} INFO - Started process (PID=10564) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:13.883+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:23:13.884+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:13.884+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:13.906+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:13.938+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:13.938+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:23:13.950+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:13.950+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:23:13.969+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.105 seconds
[2025-09-09T18:23:44.184+0000] {processor.py:161} INFO - Started process (PID=10567) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:44.185+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:23:44.186+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:44.185+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:44.215+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:23:44.244+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:44.243+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:23:44.255+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:23:44.255+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:23:44.271+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.095 seconds
[2025-09-09T18:24:14.485+0000] {processor.py:161} INFO - Started process (PID=10570) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:14.486+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:24:14.487+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:14.487+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:14.540+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:14.577+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:14.577+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:24:14.587+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:14.587+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:24:14.602+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.123 seconds
[2025-09-09T18:24:44.774+0000] {processor.py:161} INFO - Started process (PID=10573) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:44.775+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:24:44.776+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:44.776+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:44.793+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:24:44.824+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:44.824+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:24:44.838+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:24:44.838+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:24:44.855+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.089 seconds
[2025-09-09T18:25:15.047+0000] {processor.py:161} INFO - Started process (PID=10576) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:15.048+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:25:15.049+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:15.049+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:15.070+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:15.103+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:15.102+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:25:15.120+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:15.119+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:25:15.147+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.114 seconds
[2025-09-09T18:25:45.383+0000] {processor.py:161} INFO - Started process (PID=10579) to work on /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:45.384+0000] {processor.py:830} INFO - Processing file /opt/airflow/dags/etl_jobs_dag.py for tasks to queue
[2025-09-09T18:25:45.385+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:45.385+0000] {dagbag.py:545} INFO - Filling up the DagBag from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:45.398+0000] {processor.py:840} INFO - DAG(s) 'spark_transform_pipeline' retrieved from /opt/airflow/dags/etl_jobs_dag.py
[2025-09-09T18:25:45.422+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:45.422+0000] {dag.py:3096} INFO - Sync 1 DAGs
[2025-09-09T18:25:45.433+0000] {logging_mixin.py:188} INFO - [2025-09-09T18:25:45.432+0000] {dag.py:3954} INFO - Setting next_dagrun for spark_transform_pipeline to None, run_after=None
[2025-09-09T18:25:45.446+0000] {processor.py:183} INFO - Processing /opt/airflow/dags/etl_jobs_dag.py took 0.069 seconds
