[2025-09-09T14:56:40.147+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-09-09T14:56:40.185+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_transform_pipeline.silver_group.silver_topcv_transform manual__2025-09-09T14:56:33.449192+00:00 [queued]>
[2025-09-09T14:56:40.191+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_transform_pipeline.silver_group.silver_topcv_transform manual__2025-09-09T14:56:33.449192+00:00 [queued]>
[2025-09-09T14:56:40.191+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-09-09T14:56:40.202+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): silver_group.silver_topcv_transform> on 2025-09-09 14:56:33.449192+00:00
[2025-09-09T14:56:40.206+0000] {standard_task_runner.py:63} INFO - Started process 1875 to run task
[2025-09-09T14:56:40.210+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'spark_transform_pipeline', 'silver_group.silver_topcv_transform', 'manual__2025-09-09T14:56:33.449192+00:00', '--job-id', '119', '--raw', '--subdir', 'DAGS_FOLDER/etl_jobs_dag.py', '--cfg-path', '/tmp/tmpf1gydz54']
[2025-09-09T14:56:40.212+0000] {standard_task_runner.py:91} INFO - Job 119: Subtask silver_group.silver_topcv_transform
[2025-09-09T14:56:40.255+0000] {task_command.py:426} INFO - Running <TaskInstance: spark_transform_pipeline.silver_group.silver_topcv_transform manual__2025-09-09T14:56:33.449192+00:00 [running]> on host 118acb478e13
[2025-09-09T14:56:40.351+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_transform_pipeline' AIRFLOW_CTX_TASK_ID='silver_group.silver_topcv_transform' AIRFLOW_CTX_EXECUTION_DATE='2025-09-09T14:56:33.449192+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-09-09T14:56:33.449192+00:00'
[2025-09-09T14:56:40.352+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-09-09T14:56:40.382+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2025-09-09T14:56:40.383+0000] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minio --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --jars /opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,/opt/custom-jars/hadoop-aws-3.3.4.jar --name arrow-spark --verbose /opt/spark-jobs/elt/transform/silver_topcv_transform.py
[2025-09-09T14:56:41.329+0000] {spark_submit.py:521} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-09-09T14:56:42.617+0000] {spark_submit.py:521} INFO - Parsed arguments:
[2025-09-09T14:56:42.618+0000] {spark_submit.py:521} INFO - master                  spark://spark-master:7077
[2025-09-09T14:56:42.619+0000] {spark_submit.py:521} INFO - remote                  null
[2025-09-09T14:56:42.619+0000] {spark_submit.py:521} INFO - deployMode              null
[2025-09-09T14:56:42.620+0000] {spark_submit.py:521} INFO - executorMemory          null
[2025-09-09T14:56:42.620+0000] {spark_submit.py:521} INFO - executorCores           null
[2025-09-09T14:56:42.620+0000] {spark_submit.py:521} INFO - totalExecutorCores      null
[2025-09-09T14:56:42.620+0000] {spark_submit.py:521} INFO - propertiesFile          null
[2025-09-09T14:56:42.621+0000] {spark_submit.py:521} INFO - driverMemory            null
[2025-09-09T14:56:42.621+0000] {spark_submit.py:521} INFO - driverCores             null
[2025-09-09T14:56:42.621+0000] {spark_submit.py:521} INFO - driverExtraClassPath    null
[2025-09-09T14:56:42.621+0000] {spark_submit.py:521} INFO - driverExtraLibraryPath  null
[2025-09-09T14:56:42.622+0000] {spark_submit.py:521} INFO - driverExtraJavaOptions  null
[2025-09-09T14:56:42.622+0000] {spark_submit.py:521} INFO - supervise               false
[2025-09-09T14:56:42.623+0000] {spark_submit.py:521} INFO - queue                   null
[2025-09-09T14:56:42.623+0000] {spark_submit.py:521} INFO - numExecutors            null
[2025-09-09T14:56:42.623+0000] {spark_submit.py:521} INFO - files                   null
[2025-09-09T14:56:42.624+0000] {spark_submit.py:521} INFO - pyFiles                 null
[2025-09-09T14:56:42.624+0000] {spark_submit.py:521} INFO - archives                null
[2025-09-09T14:56:42.625+0000] {spark_submit.py:521} INFO - mainClass               null
[2025-09-09T14:56:42.625+0000] {spark_submit.py:521} INFO - primaryResource         file:/opt/spark-jobs/elt/transform/silver_topcv_transform.py
[2025-09-09T14:56:42.625+0000] {spark_submit.py:521} INFO - name                    arrow-spark
[2025-09-09T14:56:42.625+0000] {spark_submit.py:521} INFO - childArgs               []
[2025-09-09T14:56:42.626+0000] {spark_submit.py:521} INFO - jars                    file:/opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,file:/opt/custom-jars/hadoop-aws-3.3.4.jar
[2025-09-09T14:56:42.626+0000] {spark_submit.py:521} INFO - packages                org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367
[2025-09-09T14:56:42.627+0000] {spark_submit.py:521} INFO - packagesExclusions      null
[2025-09-09T14:56:42.627+0000] {spark_submit.py:521} INFO - repositories            null
[2025-09-09T14:56:42.628+0000] {spark_submit.py:521} INFO - verbose                 true
[2025-09-09T14:56:42.628+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:56:42.629+0000] {spark_submit.py:521} INFO - Spark properties used, including those specified through
[2025-09-09T14:56:42.629+0000] {spark_submit.py:521} INFO - --conf and those from the properties file null:
[2025-09-09T14:56:42.630+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-09-09T14:56:42.630+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-09-09T14:56:42.630+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-09-09T14:56:42.630+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-09-09T14:56:42.631+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-09-09T14:56:42.631+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-09-09T14:56:42.631+0000] {spark_submit.py:521} INFO - (spark.jars.packages,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367)
[2025-09-09T14:56:42.631+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-09-09T14:56:42.632+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:56:42.632+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:56:42.734+0000] {spark_submit.py:521} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-09-09T14:56:42.808+0000] {spark_submit.py:521} INFO - Ivy Default Cache set to: /home/***/.ivy2.5.2/cache
[2025-09-09T14:56:42.809+0000] {spark_submit.py:521} INFO - The jars for the packages stored in: /home/***/.ivy2.5.2/jars
[2025-09-09T14:56:42.813+0000] {spark_submit.py:521} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-09-09T14:56:42.813+0000] {spark_submit.py:521} INFO - com.amazonaws#aws-java-sdk-bundle added as a dependency
[2025-09-09T14:56:42.814+0000] {spark_submit.py:521} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-55dbbcae-d21b-43ef-880b-4eaa8a0bdf24;1.0
[2025-09-09T14:56:42.814+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-09-09T14:56:42.991+0000] {spark_submit.py:521} INFO - found org.apache.hadoop#hadoop-aws;3.3.4 in central
[2025-09-09T14:56:43.015+0000] {spark_submit.py:521} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-09-09T14:56:45.747+0000] {spark_submit.py:521} INFO - found com.amazonaws#aws-java-sdk-bundle;1.12.367 in central
[2025-09-09T14:56:45.929+0000] {spark_submit.py:521} INFO - downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.367/aws-java-sdk-bundle-1.12.367.jar ...
[2025-09-09T14:56:55.153+0000] {spark_submit.py:521} INFO - [SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.12.367!aws-java-sdk-bundle.jar (9389ms)
[2025-09-09T14:56:55.157+0000] {spark_submit.py:521} INFO - :: resolution report :: resolve 2946ms :: artifacts dl 9398ms
[2025-09-09T14:56:55.158+0000] {spark_submit.py:521} INFO - :: modules in use:
[2025-09-09T14:56:55.158+0000] {spark_submit.py:521} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.367 from central in [default]
[2025-09-09T14:56:55.159+0000] {spark_submit.py:521} INFO - org.apache.hadoop#hadoop-aws;3.3.4 from central in [default]
[2025-09-09T14:56:55.159+0000] {spark_submit.py:521} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-09-09T14:56:55.159+0000] {spark_submit.py:521} INFO - :: evicted modules:
[2025-09-09T14:56:55.160+0000] {spark_submit.py:521} INFO - com.amazonaws#aws-java-sdk-bundle;1.12.262 by [com.amazonaws#aws-java-sdk-bundle;1.12.367] in [default]
[2025-09-09T14:56:55.160+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T14:56:55.160+0000] {spark_submit.py:521} INFO - |                  |            modules            ||   artifacts   |
[2025-09-09T14:56:55.160+0000] {spark_submit.py:521} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-09-09T14:56:55.161+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T14:56:55.161+0000] {spark_submit.py:521} INFO - |      default     |   4   |   1   |   1   |   1   ||   3   |   1   |
[2025-09-09T14:56:55.162+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T14:56:55.163+0000] {spark_submit.py:521} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-55dbbcae-d21b-43ef-880b-4eaa8a0bdf24
[2025-09-09T14:56:55.163+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-09-09T14:56:56.873+0000] {spark_submit.py:521} INFO - 1 artifacts copied, 2 already retrieved (303302kB/1707ms)
[2025-09-09T14:56:58.143+0000] {spark_submit.py:521} INFO - 25/09/09 14:56:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-09-09T14:56:58.744+0000] {spark_submit.py:521} INFO - Main class:
[2025-09-09T14:56:58.746+0000] {spark_submit.py:521} INFO - org.apache.spark.deploy.PythonRunner
[2025-09-09T14:56:58.748+0000] {spark_submit.py:521} INFO - Arguments:
[2025-09-09T14:56:58.749+0000] {spark_submit.py:521} INFO - file:/opt/spark-jobs/elt/transform/silver_topcv_transform.py
[2025-09-09T14:56:58.749+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-09-09T14:56:58.754+0000] {spark_submit.py:521} INFO - Spark config:
[2025-09-09T14:56:58.755+0000] {spark_submit.py:521} INFO - (spark.app.name,arrow-spark)
[2025-09-09T14:56:58.755+0000] {spark_submit.py:521} INFO - (spark.app.submitTime,1757429818695)
[2025-09-09T14:56:58.755+0000] {spark_submit.py:521} INFO - (spark.files,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar)
[2025-09-09T14:56:58.756+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-09-09T14:56:58.756+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-09-09T14:56:58.757+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-09-09T14:56:58.757+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-09-09T14:56:58.757+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-09-09T14:56:58.758+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-09-09T14:56:58.758+0000] {spark_submit.py:521} INFO - (spark.jars,file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,file:///opt/custom-jars/hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar)
[2025-09-09T14:56:58.758+0000] {spark_submit.py:521} INFO - (spark.jars.packages,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367)
[2025-09-09T14:56:58.759+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-09-09T14:56:58.759+0000] {spark_submit.py:521} INFO - (spark.repl.local.jars,file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,file:///opt/custom-jars/hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar)
[2025-09-09T14:56:58.760+0000] {spark_submit.py:521} INFO - (spark.submit.deployMode,client)
[2025-09-09T14:56:58.760+0000] {spark_submit.py:521} INFO - (spark.submit.pyFiles,/home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,/home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,/home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar)
[2025-09-09T14:56:58.760+0000] {spark_submit.py:521} INFO - Classpath elements:
[2025-09-09T14:56:58.760+0000] {spark_submit.py:521} INFO - file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar
[2025-09-09T14:56:58.761+0000] {spark_submit.py:521} INFO - file:///opt/custom-jars/hadoop-aws-3.3.4.jar
[2025-09-09T14:56:58.761+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-09-09T14:56:58.761+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2025-09-09T14:56:58.762+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-09-09T14:56:58.762+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:56:58.762+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:57:01.979+0000] {spark_submit.py:521} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-09-09T14:57:01.989+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:01 INFO SparkContext: Running Spark version 4.0.1
[2025-09-09T14:57:01.992+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:01 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-09-09T14:57:01.993+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:01 INFO SparkContext: Java version 17.0.16
[2025-09-09T14:57:02.040+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceUtils: ==============================================================
[2025-09-09T14:57:02.041+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-09-09T14:57:02.042+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceUtils: ==============================================================
[2025-09-09T14:57:02.043+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SparkContext: Submitted application: SilverTopCVTransformTest
[2025-09-09T14:57:02.113+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-09-09T14:57:02.124+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceProfile: Limiting resource is cpu
[2025-09-09T14:57:02.128+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-09-09T14:57:02.266+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SecurityManager: Changing view acls to: ***
[2025-09-09T14:57:02.270+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SecurityManager: Changing modify acls to: ***
[2025-09-09T14:57:02.272+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SecurityManager: Changing view acls groups to: ***
[2025-09-09T14:57:02.273+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SecurityManager: Changing modify acls groups to: ***
[2025-09-09T14:57:02.279+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-09-09T14:57:02.751+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO Utils: Successfully started service 'sparkDriver' on port 40217.
[2025-09-09T14:57:02.825+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SparkEnv: Registering MapOutputTracker
[2025-09-09T14:57:02.849+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SparkEnv: Registering BlockManagerMaster
[2025-09-09T14:57:02.877+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-09-09T14:57:02.877+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-09-09T14:57:02.883+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-09-09T14:57:02.949+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e6cf9718-efa4-4708-a500-7cb1de95a103
[2025-09-09T14:57:02.988+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:02 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-09-09T14:57:03.181+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-09-09T14:57:03.295+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-09-09T14:57:03.344+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added JAR file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar at spark://118acb478e13:40217/jars/aws-java-sdk-bundle-1.12.262.jar with timestamp 1757429821967
[2025-09-09T14:57:03.345+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added JAR file:///opt/custom-jars/hadoop-aws-3.3.4.jar at spark://118acb478e13:40217/jars/hadoop-aws-3.3.4.jar with timestamp 1757429821967
[2025-09-09T14:57:03.345+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://118acb478e13:40217/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1757429821967
[2025-09-09T14:57:03.346+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at spark://118acb478e13:40217/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1757429821967
[2025-09-09T14:57:03.347+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://118acb478e13:40217/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1757429821967
[2025-09-09T14:57:03.350+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar at spark://118acb478e13:40217/files/org.apache.hadoop_hadoop-aws-3.3.4.jar with timestamp 1757429821967
[2025-09-09T14:57:03.352+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO Utils: Copying /home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar to /tmp/spark-3d9b9ed3-df41-4379-a5a2-3e287b67d9d6/userFiles-8345c177-6cd5-43c0-98ee-e7522a98a11b/org.apache.hadoop_hadoop-aws-3.3.4.jar
[2025-09-09T14:57:03.399+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar at spark://118acb478e13:40217/files/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar with timestamp 1757429821967
[2025-09-09T14:57:03.399+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:03 INFO Utils: Copying /home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar to /tmp/spark-3d9b9ed3-df41-4379-a5a2-3e287b67d9d6/userFiles-8345c177-6cd5-43c0-98ee-e7522a98a11b/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar
[2025-09-09T14:57:04.034+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:04 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://118acb478e13:40217/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1757429821967
[2025-09-09T14:57:04.034+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:04 INFO Utils: Copying /home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-3d9b9ed3-df41-4379-a5a2-3e287b67d9d6/userFiles-8345c177-6cd5-43c0-98ee-e7522a98a11b/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-09-09T14:57:06.674+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:06 INFO SecurityManager: Changing view acls to: ***
[2025-09-09T14:57:06.675+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:06 INFO SecurityManager: Changing modify acls to: ***
[2025-09-09T14:57:06.676+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:06 INFO SecurityManager: Changing view acls groups to: ***
[2025-09-09T14:57:06.676+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:06 INFO SecurityManager: Changing modify acls groups to: ***
[2025-09-09T14:57:06.678+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-09-09T14:57:07.156+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-09-09T14:57:07.260+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:07 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 56 ms (0 ms spent in bootstraps)
[2025-09-09T14:57:08.179+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250909145708-0008
[2025-09-09T14:57:08.191+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44251.
[2025-09-09T14:57:08.192+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO NettyBlockTransferService: Server created on 118acb478e13:44251
[2025-09-09T14:57:08.194+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-09-09T14:57:08.210+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 118acb478e13, 44251, None)
[2025-09-09T14:57:08.219+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO BlockManagerMasterEndpoint: Registering block manager 118acb478e13:44251 with 434.4 MiB RAM, BlockManagerId(driver, 118acb478e13, 44251, None)
[2025-09-09T14:57:08.223+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 118acb478e13, 44251, None)
[2025-09-09T14:57:08.225+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 118acb478e13, 44251, None)
[2025-09-09T14:57:08.254+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250909145708-0008/0 on worker-20250909135521-172.18.0.7-44119 (172.18.0.7:44119) with 2 core(s)
[2025-09-09T14:57:08.257+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20250909145708-0008/0 on hostPort 172.18.0.7:44119 with 2 core(s), 1024.0 MiB RAM
[2025-09-09T14:57:08.442+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:08 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-09-09T14:57:09.350+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:09 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250909145708-0008/0 is now RUNNING
[2025-09-09T14:57:11.143+0000] {spark_submit.py:521} INFO - 👉 Đang thử đọc từ MinIO...
[2025-09-09T14:57:11.161+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:11 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-09-09T14:57:11.163+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:11 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-09-09T14:57:13.213+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-09-09T14:57:13.248+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-09-09T14:57:13.258+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-09-09T14:57:13.283+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-09-09T14:57:13.284+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-09-09T14:57:13.285+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-09-09T14:57:13.286+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 WARN FileSystem: Failed to initialize filesystem s3a://job/bronze/topcv/topcv_jobs.parquet: java.lang.NumberFormatException: For input string: "60s"
[2025-09-09T14:57:13.294+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://job/bronze/topcv/topcv_jobs.parquet.
[2025-09-09T14:57:13.296+0000] {spark_submit.py:521} INFO - java.lang.NumberFormatException: For input string: "60s"
[2025-09-09T14:57:13.296+0000] {spark_submit.py:521} INFO - at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)
[2025-09-09T14:57:13.297+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Long.parseLong(Long.java:711)
[2025-09-09T14:57:13.297+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Long.parseLong(Long.java:836)
[2025-09-09T14:57:13.297+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1607)
[2025-09-09T14:57:13.298+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.s3a.S3AUtils.longOption(S3AUtils.java:1024)
[2025-09-09T14:57:13.298+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.s3a.S3AFileSystem.initThreadPools(S3AFileSystem.java:719)
[2025-09-09T14:57:13.298+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:498)
[2025-09-09T14:57:13.298+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)
[2025-09-09T14:57:13.299+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)
[2025-09-09T14:57:13.299+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)
[2025-09-09T14:57:13.299+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)
[2025-09-09T14:57:13.299+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)
[2025-09-09T14:57:13.300+0000] {spark_submit.py:521} INFO - at org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)
[2025-09-09T14:57:13.300+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)
[2025-09-09T14:57:13.307+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)
[2025-09-09T14:57:13.317+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)
[2025-09-09T14:57:13.321+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)
[2025-09-09T14:57:13.323+0000] {spark_submit.py:521} INFO - at scala.Option.getOrElse(Option.scala:201)
[2025-09-09T14:57:13.324+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)
[2025-09-09T14:57:13.324+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)
[2025-09-09T14:57:13.325+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)
[2025-09-09T14:57:13.325+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)
[2025-09-09T14:57:13.326+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)
[2025-09-09T14:57:13.328+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)
[2025-09-09T14:57:13.329+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)
[2025-09-09T14:57:13.330+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)
[2025-09-09T14:57:13.330+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)
[2025-09-09T14:57:13.331+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)
[2025-09-09T14:57:13.331+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)
[2025-09-09T14:57:13.332+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)
[2025-09-09T14:57:13.332+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)
[2025-09-09T14:57:13.332+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)
[2025-09-09T14:57:13.333+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)
[2025-09-09T14:57:13.333+0000] {spark_submit.py:521} INFO - at scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)
[2025-09-09T14:57:13.334+0000] {spark_submit.py:521} INFO - at scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)
[2025-09-09T14:57:13.338+0000] {spark_submit.py:521} INFO - at scala.collection.immutable.List.foldLeft(List.scala:79)
[2025-09-09T14:57:13.338+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)
[2025-09-09T14:57:13.339+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)
[2025-09-09T14:57:13.339+0000] {spark_submit.py:521} INFO - at scala.collection.immutable.List.foreach(List.scala:334)
[2025-09-09T14:57:13.340+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)
[2025-09-09T14:57:13.340+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)
[2025-09-09T14:57:13.341+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)
[2025-09-09T14:57:13.341+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)
[2025-09-09T14:57:13.341+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)
[2025-09-09T14:57:13.342+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)
[2025-09-09T14:57:13.343+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)
[2025-09-09T14:57:13.343+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)
[2025-09-09T14:57:13.344+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)
[2025-09-09T14:57:13.344+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)
[2025-09-09T14:57:13.345+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)
[2025-09-09T14:57:13.345+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)
[2025-09-09T14:57:13.345+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)
[2025-09-09T14:57:13.346+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)
[2025-09-09T14:57:13.346+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)
[2025-09-09T14:57:13.346+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)
[2025-09-09T14:57:13.347+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)
[2025-09-09T14:57:13.347+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)
[2025-09-09T14:57:13.347+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)
[2025-09-09T14:57:13.348+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-09-09T14:57:13.348+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)
[2025-09-09T14:57:13.348+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-09-09T14:57:13.349+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)
[2025-09-09T14:57:13.349+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)
[2025-09-09T14:57:13.349+0000] {spark_submit.py:521} INFO - at scala.util.Try$.apply(Try.scala:217)
[2025-09-09T14:57:13.350+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)
[2025-09-09T14:57:13.350+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)
[2025-09-09T14:57:13.350+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)
[2025-09-09T14:57:13.351+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.LazyTry.get(LazyTry.scala:58)
[2025-09-09T14:57:13.351+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)
[2025-09-09T14:57:13.351+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)
[2025-09-09T14:57:13.352+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)
[2025-09-09T14:57:13.352+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-09-09T14:57:13.352+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)
[2025-09-09T14:57:13.353+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)
[2025-09-09T14:57:13.353+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)
[2025-09-09T14:57:13.354+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:457)
[2025-09-09T14:57:13.354+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:306)
[2025-09-09T14:57:13.354+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.DataFrameReader.parquet(DataFrameReader.scala:58)
[2025-09-09T14:57:13.355+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-09T14:57:13.355+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-09-09T14:57:13.356+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-09-09T14:57:13.356+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-09-09T14:57:13.356+0000] {spark_submit.py:521} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-09-09T14:57:13.357+0000] {spark_submit.py:521} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-09-09T14:57:13.357+0000] {spark_submit.py:521} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-09-09T14:57:13.358+0000] {spark_submit.py:521} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-09-09T14:57:13.358+0000] {spark_submit.py:521} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-09-09T14:57:13.359+0000] {spark_submit.py:521} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-09-09T14:57:13.359+0000] {spark_submit.py:521} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-09-09T14:57:13.359+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-09-09T14:57:13.361+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-09-09T14:57:13.362+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-09-09T14:57:13.364+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-09-09T14:57:13.364+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-09-09T14:57:13.365+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-09-09T14:57:13.365+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-09-09T14:57:13.365+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:13 WARN FileSystem: Failed to initialize filesystem s3a://job/bronze/topcv/topcv_jobs.parquet: java.lang.NumberFormatException: For input string: "60s"
[2025-09-09T14:57:13.426+0000] {spark_submit.py:521} INFO - ⚠️ Không đọc được từ MinIO, fallback sang DataFrame giả lập
[2025-09-09T14:57:13.440+0000] {spark_submit.py:521} INFO - Lỗi: For input string: "60s"
[2025-09-09T14:57:14.176+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:14 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:54686) with ID 0, ResourceProfileId 0
[2025-09-09T14:57:15.534+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:15 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.7:33261 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.7, 33261, None)
[2025-09-09T14:57:23.939+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:23 INFO CodeGenerator: Code generated in 692.903213 ms
[2025-09-09T14:57:24.254+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-09-09T14:57:24.504+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-09-09T14:57:24.515+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2025-09-09T14:57:24.518+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T14:57:24.535+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO DAGScheduler: Missing parents: List()
[2025-09-09T14:57:24.568+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:24 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-09-09T14:57:25.869+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:25 INFO AsyncEventQueue: Process of event SparkListenerJobStart(0,1757429844536,ArraySeq(org.apache.spark.scheduler.StageInfo@4a51e1c1),{spark.submit.pyFiles=/home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,/home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,/home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark.app.startTime=1757429821967, spark.rdd.compress=True, spark.hadoop.fs.s3a.vectored.read.min.seek.size=128K, spark.sql.warehouse.dir=file:/opt/***/spark-warehouse, spark.master=spark://spark-master:7077, spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367, spark.job.interruptOnCancel=true, spark.app.name=SilverTopCVTransformTest, spark.hadoop.fs.s3a.access.key=minio, spark.hadoop.fs.s3a.path.style.access=true, spark.driver.host=118acb478e13, spark.app.id=app-20250909145708-0008, spark.job.tags=spark-session-3c5692e9-5ccc-4688-b1e4-f983be165977-execution-root-id-0,spark-session-3c5692e9-5ccc-4688-b1e4-f983be165977, spark.files=file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark.sql.execution.root.id=0, spark.jars=file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,file:///opt/custom-jars/hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark.app.initial.file.urls=spark://118acb478e13:40217/files/org.apache.hadoop_hadoop-aws-3.3.4.jar,spark://118acb478e13:40217/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,spark://118acb478e13:40217/files/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar, spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true, spark.driver.port=40217, spark.hadoop.fs.s3a.endpoint=http://minio:9000, spark.hadoop.fs.s3a.vectored.read.max.merged.size=2M, spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem, spark.app.submitTime=1757429818695, spark.app.initial.jar.urls=spark://118acb478e13:40217/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,spark://118acb478e13:40217/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,spark://118acb478e13:40217/jars/hadoop-aws-3.3.4.jar,spark://118acb478e13:40217/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar,spark://118acb478e13:40217/jars/aws-java-sdk-bundle-1.12.262.jar, spark.sql.execution.id=0, spark.repl.local.jars=file:///opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,file:///opt/custom-jars/hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.3.4.jar,file:///home/***/.ivy2.5.2/jars/com.amazonaws_aws-java-sdk-bundle-1.12.367.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar, spark.hadoop.fs.s3a.secret.key=minio123, spark.executor.id=driver, spark.submit.deployMode=client, spark.sql.artifact.isolation.enabled=false, spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-modules=jdk.incubator.vector --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dio.netty.tryReflectionSetAccessible=true, spark.hadoop.fs.s3a.connection.ssl.enabled=false, spark.serializer.objectStreamReset=100}) bylistener AppStatusListener took 1349.235119ms.
[2025-09-09T14:57:25.968+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:25 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-09-09T14:57:26.188+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.4 KiB, free 434.4 MiB)
[2025-09-09T14:57:26.430+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:26 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)
[2025-09-09T14:57:26.463+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:26 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1676
[2025-09-09T14:57:26.519+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-09-09T14:57:26.547+0000] {spark_submit.py:521} INFO - 25/09/09 14:57:26 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-09-09T14:58:12.171+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:12 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.7,executor 0, partition 0, PROCESS_LOCAL, 11171 bytes)
[2025-09-09T14:58:14.154+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.7 executor 0): java.io.InvalidClassException: org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830
[2025-09-09T14:58:14.162+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(Unknown Source)
[2025-09-09T14:58:14.163+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
[2025-09-09T14:58:14.181+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
[2025-09-09T14:58:14.181+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.181+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.181+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:14.183+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.185+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.192+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.193+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.199+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.205+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:14.205+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.207+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.208+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.208+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.208+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.209+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:14.210+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.210+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.210+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.213+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.213+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.213+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.216+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.216+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.217+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.217+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.217+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.217+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.228+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.240+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.246+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.272+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:14.277+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-09T14:58:14.290+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:14.291+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:14.307+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:14.308+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:14.309+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.311+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.311+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.319+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.324+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.326+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.327+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.330+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.331+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.331+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.332+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.333+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.334+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.334+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:14.335+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-09T14:58:14.336+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:14.337+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:14.337+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:14.338+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:14.340+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.340+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.341+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.342+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.348+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.349+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.349+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.350+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:14.350+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:14.350+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:14.374+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:14.375+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.375+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:14.375+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)
[2025-09-09T14:58:14.379+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:136)
[2025-09-09T14:58:14.382+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)
[2025-09-09T14:58:14.385+0000] {spark_submit.py:521} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
[2025-09-09T14:58:14.410+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:147)
[2025-09-09T14:58:14.413+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
[2025-09-09T14:58:14.416+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
[2025-09-09T14:58:14.417+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
[2025-09-09T14:58:14.419+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
[2025-09-09T14:58:14.419+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
[2025-09-09T14:58:14.419+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-09-09T14:58:14.419+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-09-09T14:58:14.420+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Unknown Source)
[2025-09-09T14:58:14.420+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:58:14.420+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (172.18.0.7,executor 0, partition 0, PROCESS_LOCAL, 11171 bytes)
[2025-09-09T14:58:14.441+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on 172.18.0.7, executor 0: java.io.InvalidClassException (org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830) [duplicate 1]
[2025-09-09T14:58:14.450+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2) (172.18.0.7,executor 0, partition 0, PROCESS_LOCAL, 11171 bytes)
[2025-09-09T14:58:14.574+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on 172.18.0.7, executor 0: java.io.InvalidClassException (org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830) [duplicate 2]
[2025-09-09T14:58:14.605+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3) (172.18.0.7,executor 0, partition 0, PROCESS_LOCAL, 11171 bytes)
[2025-09-09T14:58:14.722+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on 172.18.0.7, executor 0: java.io.InvalidClassException (org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830) [duplicate 3]
[2025-09-09T14:58:14.956+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:14 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job
[2025-09-09T14:58:15.169+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool
[2025-09-09T14:58:15.408+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:15 INFO TaskSchedulerImpl: Canceling stage 0
[2025-09-09T14:58:15.445+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.7 executor 0): java.io.InvalidClassException: org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830
[2025-09-09T14:58:15.460+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(Unknown Source)
[2025-09-09T14:58:15.464+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
[2025-09-09T14:58:15.469+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
[2025-09-09T14:58:15.471+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.475+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.476+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.480+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.483+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.487+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.489+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.491+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.494+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.494+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.495+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.495+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.495+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.495+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.496+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.496+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.500+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.511+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.519+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.522+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.524+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.526+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.526+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.527+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.528+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.528+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.528+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.531+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.532+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.535+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.536+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:15.536+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:15.537+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:15.537+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:15.537+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:15.539+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.539+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.540+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.540+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.542+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.543+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.544+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.545+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.545+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.545+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.546+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.546+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.548+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.549+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:15.550+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:15.551+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:15.551+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:15.553+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:15.553+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.555+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.556+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.556+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.558+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.559+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.560+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.560+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.561+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.562+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.563+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.563+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.563+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.564+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)
[2025-09-09T14:58:15.564+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:136)
[2025-09-09T14:58:15.565+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)
[2025-09-09T14:58:15.566+0000] {spark_submit.py:521} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
[2025-09-09T14:58:15.570+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:147)
[2025-09-09T14:58:15.571+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
[2025-09-09T14:58:15.572+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
[2025-09-09T14:58:15.572+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
[2025-09-09T14:58:15.576+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
[2025-09-09T14:58:15.576+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
[2025-09-09T14:58:15.576+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-09-09T14:58:15.577+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-09-09T14:58:15.577+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Unknown Source)
[2025-09-09T14:58:15.577+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:58:15.578+0000] {spark_submit.py:521} INFO - Driver stacktrace:
[2025-09-09T14:58:15.583+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:15 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) failed in 50367 ms due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.7 executor 0): java.io.InvalidClassException: org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830
[2025-09-09T14:58:15.601+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(Unknown Source)
[2025-09-09T14:58:15.601+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
[2025-09-09T14:58:15.601+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.602+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.603+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.604+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.605+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.605+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.606+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.608+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.608+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.609+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.609+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.609+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.609+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:15.611+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:15.611+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:15.613+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:15.613+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:15.613+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.613+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.613+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.614+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:15.615+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.616+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.618+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.619+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.620+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:15.620+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:15.620+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:15.620+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:136)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)
[2025-09-09T14:58:15.621+0000] {spark_submit.py:521} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
[2025-09-09T14:58:15.623+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:147)
[2025-09-09T14:58:15.623+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
[2025-09-09T14:58:15.623+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
[2025-09-09T14:58:15.625+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
[2025-09-09T14:58:15.633+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
[2025-09-09T14:58:15.635+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
[2025-09-09T14:58:15.635+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-09-09T14:58:15.635+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-09-09T14:58:15.635+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Unknown Source)
[2025-09-09T14:58:15.636+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:58:15.637+0000] {spark_submit.py:521} INFO - Driver stacktrace:
[2025-09-09T14:58:15.670+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:15 INFO DAGScheduler: Job 0 failed: showString at NativeMethodAccessorImpl.java:0, took 51460.284029 ms
[2025-09-09T14:58:16.996+0000] {spark_submit.py:521} INFO - Traceback (most recent call last):
[2025-09-09T14:58:16.997+0000] {spark_submit.py:521} INFO - File "/opt/spark-jobs/elt/transform/silver_topcv_transform.py", line 41, in <module>
[2025-09-09T14:58:17.049+0000] {spark_submit.py:521} INFO - df.show()
[2025-09-09T14:58:17.069+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/classic/dataframe.py", line 285, in show
[2025-09-09T14:58:17.111+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/classic/dataframe.py", line 303, in _show_string
[2025-09-09T14:58:17.115+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2025-09-09T14:58:17.132+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 282, in deco
[2025-09-09T14:58:17.132+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py", line 327, in get_return_value
[2025-09-09T14:58:17.179+0000] {spark_submit.py:521} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o55.showString.
[2025-09-09T14:58:17.182+0000] {spark_submit.py:521} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.7 executor 0): java.io.InvalidClassException: org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830
[2025-09-09T14:58:17.183+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(Unknown Source)
[2025-09-09T14:58:17.183+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
[2025-09-09T14:58:17.184+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
[2025-09-09T14:58:17.185+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.186+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.186+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.187+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.187+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.190+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.191+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.192+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.194+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.194+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.194+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.195+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.195+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.195+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.196+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.196+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.197+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.199+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.199+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.200+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.200+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.201+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.202+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.202+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.203+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.204+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.206+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.207+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.208+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.208+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.210+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:17.210+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:17.211+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:17.211+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:17.211+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:17.212+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.213+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.214+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.215+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.216+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.218+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.219+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.219+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.222+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.223+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.224+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.224+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.224+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.224+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:17.225+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:17.225+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:17.225+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:17.226+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:17.226+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.226+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.227+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.227+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.227+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.227+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.227+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.228+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.228+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.228+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.229+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.229+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.229+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.229+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)
[2025-09-09T14:58:17.230+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:136)
[2025-09-09T14:58:17.230+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)
[2025-09-09T14:58:17.230+0000] {spark_submit.py:521} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
[2025-09-09T14:58:17.230+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:147)
[2025-09-09T14:58:17.231+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
[2025-09-09T14:58:17.231+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
[2025-09-09T14:58:17.231+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
[2025-09-09T14:58:17.231+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
[2025-09-09T14:58:17.232+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
[2025-09-09T14:58:17.232+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-09-09T14:58:17.232+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-09-09T14:58:17.233+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Unknown Source)
[2025-09-09T14:58:17.233+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:58:17.233+0000] {spark_submit.py:521} INFO - Driver stacktrace:
[2025-09-09T14:58:17.233+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)
[2025-09-09T14:58:17.233+0000] {spark_submit.py:521} INFO - at scala.Option.getOrElse(Option.scala:201)
[2025-09-09T14:58:17.234+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)
[2025-09-09T14:58:17.234+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)
[2025-09-09T14:58:17.234+0000] {spark_submit.py:521} INFO - at scala.collection.immutable.List.foreach(List.scala:334)
[2025-09-09T14:58:17.235+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)
[2025-09-09T14:58:17.235+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)
[2025-09-09T14:58:17.235+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)
[2025-09-09T14:58:17.235+0000] {spark_submit.py:521} INFO - at scala.Option.foreach(Option.scala:437)
[2025-09-09T14:58:17.235+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)
[2025-09-09T14:58:17.236+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)
[2025-09-09T14:58:17.236+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)
[2025-09-09T14:58:17.236+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)
[2025-09-09T14:58:17.236+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)
[2025-09-09T14:58:17.237+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)
[2025-09-09T14:58:17.238+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)
[2025-09-09T14:58:17.238+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)
[2025-09-09T14:58:17.238+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)
[2025-09-09T14:58:17.238+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)
[2025-09-09T14:58:17.238+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)
[2025-09-09T14:58:17.239+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)
[2025-09-09T14:58:17.239+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)
[2025-09-09T14:58:17.239+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)
[2025-09-09T14:58:17.239+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)
[2025-09-09T14:58:17.240+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)
[2025-09-09T14:58:17.241+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:2810)
[2025-09-09T14:58:17.242+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)
[2025-09-09T14:58:17.242+0000] {spark_submit.py:521} INFO - at org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)
[2025-09-09T14:58:17.242+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-09-09T14:58:17.242+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-09-09T14:58:17.243+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-09-09T14:58:17.243+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-09-09T14:58:17.243+0000] {spark_submit.py:521} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-09-09T14:58:17.243+0000] {spark_submit.py:521} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-09-09T14:58:17.244+0000] {spark_submit.py:521} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-09-09T14:58:17.245+0000] {spark_submit.py:521} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-09-09T14:58:17.246+0000] {spark_submit.py:521} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-09-09T14:58:17.246+0000] {spark_submit.py:521} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)
[2025-09-09T14:58:17.246+0000] {spark_submit.py:521} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:108)
[2025-09-09T14:58:17.249+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-09-09T14:58:17.250+0000] {spark_submit.py:521} INFO - Caused by: java.io.InvalidClassException: org.apache.spark.sql.types.StructField; local class incompatible: stream classdesc serialVersionUID = 211707512700890436, local class serialVersionUID = 8518787353547913830
[2025-09-09T14:58:17.251+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.initNonProxy(Unknown Source)
[2025-09-09T14:58:17.253+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readNonProxyDesc(Unknown Source)
[2025-09-09T14:58:17.253+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readClassDesc(Unknown Source)
[2025-09-09T14:58:17.253+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.253+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.253+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.254+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.254+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.254+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.254+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.255+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.255+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.255+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.255+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.255+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.256+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.256+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.256+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readArray(Unknown Source)
[2025-09-09T14:58:17.256+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.256+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.257+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.257+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.257+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.257+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.257+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.258+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.258+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.258+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.258+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.259+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.259+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.259+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.259+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.260+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:17.260+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:17.260+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:17.260+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:17.260+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:17.261+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.261+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.261+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.261+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.262+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.262+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.262+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.262+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.262+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.263+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.263+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.263+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.263+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.263+0000] {spark_submit.py:521} INFO - at scala.collection.generic.DefaultSerializationProxy.readObject(DefaultSerializationProxy.scala:58)
[2025-09-09T14:58:17.264+0000] {spark_submit.py:521} INFO - at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
[2025-09-09T14:58:17.264+0000] {spark_submit.py:521} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-09-09T14:58:17.264+0000] {spark_submit.py:521} INFO - at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-09-09T14:58:17.265+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectStreamClass.invokeReadObject(Unknown Source)
[2025-09-09T14:58:17.265+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.265+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.265+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.265+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream$FieldValues.<init>(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readSerialData(Unknown Source)
[2025-09-09T14:58:17.266+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readOrdinaryObject(Unknown Source)
[2025-09-09T14:58:17.267+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject0(Unknown Source)
[2025-09-09T14:58:17.267+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.267+0000] {spark_submit.py:521} INFO - at java.base/java.io.ObjectInputStream.readObject(Unknown Source)
[2025-09-09T14:58:17.267+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:88)
[2025-09-09T14:58:17.267+0000] {spark_submit.py:521} INFO - at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:136)
[2025-09-09T14:58:17.268+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:86)
[2025-09-09T14:58:17.268+0000] {spark_submit.py:521} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)
[2025-09-09T14:58:17.268+0000] {spark_submit.py:521} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:147)
[2025-09-09T14:58:17.268+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-09-09T14:58:17.269+0000] {spark_submit.py:521} INFO - at java.base/java.lang.Thread.run(Unknown Source)
[2025-09-09T14:58:17.283+0000] {spark_submit.py:521} INFO - 
[2025-09-09T14:58:23.941+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:23 INFO SparkContext: Invoking stop() from shutdown hook
[2025-09-09T14:58:23.973+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:23 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.
[2025-09-09T14:58:24.305+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:24 INFO SparkUI: Stopped Spark web UI at http://118acb478e13:4040
[2025-09-09T14:58:24.357+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:24 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-09-09T14:58:24.379+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:24 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-09-09T14:58:24.964+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-09-09T14:58:25.650+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:25 INFO MemoryStore: MemoryStore cleared
[2025-09-09T14:58:25.660+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:25 INFO BlockManager: BlockManager stopped
[2025-09-09T14:58:25.742+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:25 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-09-09T14:58:25.827+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:25 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-09-09T14:58:26.705+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO SparkContext: Successfully stopped SparkContext
[2025-09-09T14:58:26.707+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO ShutdownHookManager: Shutdown hook called
[2025-09-09T14:58:26.715+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-9263326e-8cee-450b-a987-52042f147cad
[2025-09-09T14:58:26.735+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d9b9ed3-df41-4379-a5a2-3e287b67d9d6/pyspark-ec1f9bb9-c215-46d5-8f2d-6c8dd3a47210
[2025-09-09T14:58:26.790+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-3d9b9ed3-df41-4379-a5a2-3e287b67d9d6
[2025-09-09T14:58:26.826+0000] {spark_submit.py:521} INFO - 25/09/09 14:58:26 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-43fba40f-374c-46d7-ae29-5e364e3fd674
[2025-09-09T14:58:27.812+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-09-09T14:58:28.804+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 452, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minio --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --jars /opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,/opt/custom-jars/hadoop-aws-3.3.4.jar --name arrow-spark --verbose /opt/spark-jobs/elt/transform/silver_topcv_transform.py. Error code is: 1.
[2025-09-09T14:58:29.136+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=spark_transform_pipeline, task_id=silver_group.silver_topcv_transform, run_id=manual__2025-09-09T14:56:33.449192+00:00, execution_date=20250909T145633, start_date=20250909T145640, end_date=20250909T145829
[2025-09-09T14:58:29.989+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 119 for task silver_group.silver_topcv_transform (Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minio --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --jars /opt/custom-jars/aws-java-sdk-bundle-1.12.262.jar,/opt/custom-jars/hadoop-aws-3.3.4.jar --name arrow-spark --verbose /opt/spark-jobs/elt/transform/silver_topcv_transform.py. Error code is: 1.; 1875)
[2025-09-09T14:58:30.426+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-09-09T14:58:31.239+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-09-09T14:58:31.253+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
