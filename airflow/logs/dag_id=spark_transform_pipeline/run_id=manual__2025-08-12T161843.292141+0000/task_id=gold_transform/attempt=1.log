[2025-08-12T16:20:09.885+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-08-12T16:20:09.906+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_transform_pipeline.gold_transform manual__2025-08-12T16:18:43.292141+00:00 [queued]>
[2025-08-12T16:20:09.911+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_transform_pipeline.gold_transform manual__2025-08-12T16:18:43.292141+00:00 [queued]>
[2025-08-12T16:20:09.911+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-08-12T16:20:09.921+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): gold_transform> on 2025-08-12 16:18:43.292141+00:00
[2025-08-12T16:20:09.927+0000] {standard_task_runner.py:63} INFO - Started process 1369 to run task
[2025-08-12T16:20:09.929+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'spark_transform_pipeline', 'gold_transform', 'manual__2025-08-12T16:18:43.292141+00:00', '--job-id', '76', '--raw', '--subdir', 'DAGS_FOLDER/etl_jobs_dag.py', '--cfg-path', '/tmp/tmpdbin_e4k']
[2025-08-12T16:20:09.933+0000] {standard_task_runner.py:91} INFO - Job 76: Subtask gold_transform
[2025-08-12T16:20:09.983+0000] {task_command.py:426} INFO - Running <TaskInstance: spark_transform_pipeline.gold_transform manual__2025-08-12T16:18:43.292141+00:00 [running]> on host 689c6879ed29
[2025-08-12T16:20:10.060+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_transform_pipeline' AIRFLOW_CTX_TASK_ID='gold_transform' AIRFLOW_CTX_EXECUTION_DATE='2025-08-12T16:18:43.292141+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-12T16:18:43.292141+00:00'
[2025-08-12T16:20:10.061+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-08-12T16:20:10.079+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2025-08-12T16:20:10.080+0000] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.driver.extraClassPath=/opt/custom-jars/* --conf spark.executor.extraClassPath=/opt/custom-jars/* --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/opt/***/gcp-key.json --jars /opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar --name arrow-spark --verbose /opt/spark-jobs/gold.transfrom.py
[2025-08-12T16:20:10.615+0000] {spark_submit.py:521} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-12T16:20:13.066+0000] {spark_submit.py:521} INFO - Parsed arguments:
[2025-08-12T16:20:13.068+0000] {spark_submit.py:521} INFO - master                  spark://spark-master:7077
[2025-08-12T16:20:13.069+0000] {spark_submit.py:521} INFO - remote                  null
[2025-08-12T16:20:13.070+0000] {spark_submit.py:521} INFO - deployMode              null
[2025-08-12T16:20:13.070+0000] {spark_submit.py:521} INFO - executorMemory          null
[2025-08-12T16:20:13.071+0000] {spark_submit.py:521} INFO - executorCores           null
[2025-08-12T16:20:13.072+0000] {spark_submit.py:521} INFO - totalExecutorCores      null
[2025-08-12T16:20:13.072+0000] {spark_submit.py:521} INFO - propertiesFile          null
[2025-08-12T16:20:13.072+0000] {spark_submit.py:521} INFO - driverMemory            null
[2025-08-12T16:20:13.073+0000] {spark_submit.py:521} INFO - driverCores             null
[2025-08-12T16:20:13.073+0000] {spark_submit.py:521} INFO - driverExtraClassPath    /opt/custom-jars/*
[2025-08-12T16:20:13.073+0000] {spark_submit.py:521} INFO - driverExtraLibraryPath  null
[2025-08-12T16:20:13.074+0000] {spark_submit.py:521} INFO - driverExtraJavaOptions  null
[2025-08-12T16:20:13.074+0000] {spark_submit.py:521} INFO - supervise               false
[2025-08-12T16:20:13.075+0000] {spark_submit.py:521} INFO - queue                   null
[2025-08-12T16:20:13.075+0000] {spark_submit.py:521} INFO - numExecutors            null
[2025-08-12T16:20:13.075+0000] {spark_submit.py:521} INFO - files                   null
[2025-08-12T16:20:13.075+0000] {spark_submit.py:521} INFO - pyFiles                 null
[2025-08-12T16:20:13.076+0000] {spark_submit.py:521} INFO - archives                null
[2025-08-12T16:20:13.076+0000] {spark_submit.py:521} INFO - mainClass               null
[2025-08-12T16:20:13.077+0000] {spark_submit.py:521} INFO - primaryResource         file:/opt/spark-jobs/gold.transfrom.py
[2025-08-12T16:20:13.077+0000] {spark_submit.py:521} INFO - name                    arrow-spark
[2025-08-12T16:20:13.077+0000] {spark_submit.py:521} INFO - childArgs               []
[2025-08-12T16:20:13.077+0000] {spark_submit.py:521} INFO - jars                    file:/opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar
[2025-08-12T16:20:13.078+0000] {spark_submit.py:521} INFO - packages                null
[2025-08-12T16:20:13.078+0000] {spark_submit.py:521} INFO - packagesExclusions      null
[2025-08-12T16:20:13.079+0000] {spark_submit.py:521} INFO - repositories            null
[2025-08-12T16:20:13.079+0000] {spark_submit.py:521} INFO - verbose                 true
[2025-08-12T16:20:13.079+0000] {spark_submit.py:521} INFO - 
[2025-08-12T16:20:13.079+0000] {spark_submit.py:521} INFO - Spark properties used, including those specified through
[2025-08-12T16:20:13.080+0000] {spark_submit.py:521} INFO - --conf and those from the properties file null:
[2025-08-12T16:20:13.080+0000] {spark_submit.py:521} INFO - (spark.driver.extraClassPath,/opt/custom-jars/*)
[2025-08-12T16:20:13.080+0000] {spark_submit.py:521} INFO - (spark.executor.extraClassPath,/opt/custom-jars/*)
[2025-08-12T16:20:13.081+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.AbstractFileSystem.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS)
[2025-08-12T16:20:13.081+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-08-12T16:20:13.081+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.enable,true)
[2025-08-12T16:20:13.081+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.json.keyfile,/opt/***/gcp-key.json)
[2025-08-12T16:20:13.082+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-08-12T16:20:13.082+0000] {spark_submit.py:521} INFO - 
[2025-08-12T16:20:13.082+0000] {spark_submit.py:521} INFO - 
[2025-08-12T16:20:13.490+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-12T16:20:14.000+0000] {spark_submit.py:521} INFO - Main class:
[2025-08-12T16:20:14.000+0000] {spark_submit.py:521} INFO - org.apache.spark.deploy.PythonRunner
[2025-08-12T16:20:14.001+0000] {spark_submit.py:521} INFO - Arguments:
[2025-08-12T16:20:14.001+0000] {spark_submit.py:521} INFO - file:/opt/spark-jobs/gold.transfrom.py
[2025-08-12T16:20:14.001+0000] {spark_submit.py:521} INFO - null
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - Spark config:
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - (spark.app.name,arrow-spark)
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - (spark.app.submitTime,1755015613982)
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - (spark.driver.extraClassPath,/opt/custom-jars/*)
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - (spark.executor.extraClassPath,/opt/custom-jars/*)
[2025-08-12T16:20:14.002+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.AbstractFileSystem.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS)
[2025-08-12T16:20:14.003+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-08-12T16:20:14.003+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.enable,true)
[2025-08-12T16:20:14.003+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.json.keyfile,/opt/***/gcp-key.json)
[2025-08-12T16:20:14.003+0000] {spark_submit.py:521} INFO - (spark.jars,file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar)
[2025-08-12T16:20:14.004+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-08-12T16:20:14.004+0000] {spark_submit.py:521} INFO - (spark.repl.local.jars,file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar)
[2025-08-12T16:20:14.004+0000] {spark_submit.py:521} INFO - (spark.submit.deployMode,client)
[2025-08-12T16:20:14.004+0000] {spark_submit.py:521} INFO - (spark.submit.pyFiles,)
[2025-08-12T16:20:14.004+0000] {spark_submit.py:521} INFO - Classpath elements:
[2025-08-12T16:20:14.005+0000] {spark_submit.py:521} INFO - file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar
[2025-08-12T16:20:14.005+0000] {spark_submit.py:521} INFO - 
[2025-08-12T16:20:14.005+0000] {spark_submit.py:521} INFO - 
[2025-08-12T16:20:15.184+0000] {spark_submit.py:521} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-12T16:20:15.188+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkContext: Running Spark version 4.0.0
[2025-08-12T16:20:15.190+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-08-12T16:20:15.191+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkContext: Java version 17.0.15
[2025-08-12T16:20:15.212+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceUtils: ==============================================================
[2025-08-12T16:20:15.212+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-12T16:20:15.213+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceUtils: ==============================================================
[2025-08-12T16:20:15.213+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkContext: Submitted application: gold_transform
[2025-08-12T16:20:15.238+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-12T16:20:15.241+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceProfile: Limiting resource is cpu
[2025-08-12T16:20:15.242+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-12T16:20:15.297+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SecurityManager: Changing view acls to: ***
[2025-08-12T16:20:15.298+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SecurityManager: Changing modify acls to: ***
[2025-08-12T16:20:15.298+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-12T16:20:15.299+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-12T16:20:15.300+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-12T16:20:15.610+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO Utils: Successfully started service 'sparkDriver' on port 33763.
[2025-08-12T16:20:15.641+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkEnv: Registering MapOutputTracker
[2025-08-12T16:20:15.654+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-12T16:20:15.670+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-12T16:20:15.671+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-12T16:20:15.675+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-12T16:20:15.694+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9fdfd41d-75e2-43e9-80e8-9a86b04fa18d
[2025-08-12T16:20:15.712+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-12T16:20:15.837+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-12T16:20:15.949+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-08-12T16:20:16.002+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SparkContext: Added JAR file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar at spark://689c6879ed29:33763/jars/gcs-connector-hadoop3-2.2.14-shaded.jar with timestamp 1755015615176
[2025-08-12T16:20:16.023+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SecurityManager: Changing view acls to: ***
[2025-08-12T16:20:16.024+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SecurityManager: Changing modify acls to: ***
[2025-08-12T16:20:16.024+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-12T16:20:16.024+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-12T16:20:16.025+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-12T16:20:16.166+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-12T16:20:16.205+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.2:7077 after 19 ms (0 ms spent in bootstraps)
[2025-08-12T16:20:16.275+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250812162016-0005
[2025-08-12T16:20:16.287+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38495.
[2025-08-12T16:20:16.287+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO NettyBlockTransferService: Server created on 689c6879ed29:38495
[2025-08-12T16:20:16.287+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250812162016-0005/0 on worker-20250812161319-172.18.0.6-46147 (172.18.0.6:46147) with 2 core(s)
[2025-08-12T16:20:16.289+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-12T16:20:16.290+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneSchedulerBackend: Granted executor ID app-20250812162016-0005/0 on hostPort 172.18.0.6:46147 with 2 core(s), 1024.0 MiB RAM
[2025-08-12T16:20:16.302+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 689c6879ed29, 38495, None)
[2025-08-12T16:20:16.306+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO BlockManagerMasterEndpoint: Registering block manager 689c6879ed29:38495 with 434.4 MiB RAM, BlockManagerId(driver, 689c6879ed29, 38495, None)
[2025-08-12T16:20:16.309+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 689c6879ed29, 38495, None)
[2025-08-12T16:20:16.311+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 689c6879ed29, 38495, None)
[2025-08-12T16:20:16.423+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250812162016-0005/0 is now RUNNING
[2025-08-12T16:20:16.475+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-12T16:20:19.753+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-12T16:20:19.757+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:19 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-08-12T16:20:20.069+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:20 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:33264) with ID 0, ResourceProfileId 0
[2025-08-12T16:20:20.269+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:20 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.6:41125 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.6, 41125, None)
[2025-08-12T16:20:23.425+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO InMemoryFileIndex: It took 127 ms to list leaf files for 1 paths.
[2025-08-12T16:20:23.659+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-12T16:20:23.674+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-12T16:20:23.677+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-12T16:20:23.679+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Parents of final stage: List()
[2025-08-12T16:20:23.683+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Missing parents: List()
[2025-08-12T16:20:23.687+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-12T16:20:23.728+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-12T16:20:23.742+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 116.0 KiB, free 434.3 MiB)
[2025-08-12T16:20:23.785+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.2 MiB)
[2025-08-12T16:20:23.795+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1676
[2025-08-12T16:20:23.821+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-12T16:20:23.837+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-08-12T16:20:23.870+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.6,executor 0, partition 0, PROCESS_LOCAL, 10067 bytes)
[2025-08-12T16:20:26.908+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3047 ms on 172.18.0.6 (executor 0) (1/1)
[2025-08-12T16:20:26.910+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool
[2025-08-12T16:20:26.916+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 3212 ms
[2025-08-12T16:20:26.918+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-12T16:20:26.919+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO TaskSchedulerImpl: Canceling stage 0
[2025-08-12T16:20:26.920+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-08-12T16:20:26.923+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:26 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 3263.232565 ms
[2025-08-12T16:20:27.765+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO InMemoryFileIndex: It took 240 ms to list leaf files for 1 paths.
[2025-08-12T16:20:27.804+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-12T16:20:27.805+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-12T16:20:27.806+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-12T16:20:27.806+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Parents of final stage: List()
[2025-08-12T16:20:27.806+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Missing parents: List()
[2025-08-12T16:20:27.807+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-12T16:20:27.816+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 116.0 KiB, free 434.1 MiB)
[2025-08-12T16:20:27.821+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.1 MiB)
[2025-08-12T16:20:27.822+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676
[2025-08-12T16:20:27.823+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-12T16:20:27.824+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-08-12T16:20:27.825+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.6,executor 0, partition 0, PROCESS_LOCAL, 10083 bytes)
[2025-08-12T16:20:28.353+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 528 ms on 172.18.0.6 (executor 0) (1/1)
[2025-08-12T16:20:28.355+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool
[2025-08-12T16:20:28.358+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 547 ms
[2025-08-12T16:20:28.363+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-12T16:20:28.366+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO TaskSchedulerImpl: Canceling stage 1
[2025-08-12T16:20:28.369+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-08-12T16:20:28.370+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:28 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 550.882058 ms
[2025-08-12T16:20:29.045+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceStrategy: Pushed Filters:
[2025-08-12T16:20:29.049+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-12T16:20:29.063+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceStrategy: Pushed Filters:
[2025-08-12T16:20:29.064+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-12T16:20:29.719+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO CodeGenerator: Code generated in 349.929834 ms
[2025-08-12T16:20:29.759+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 219.9 KiB, free 434.2 MiB)
[2025-08-12T16:20:29.778+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.0 KiB, free 434.1 MiB)
[2025-08-12T16:20:29.781+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0
[2025-08-12T16:20:29.810+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-12T16:20:29.919+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO CodeGenerator: Code generated in 37.498088 ms
[2025-08-12T16:20:29.925+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 219.9 KiB, free 433.9 MiB)
[2025-08-12T16:20:29.935+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 40.1 KiB, free 433.9 MiB)
[2025-08-12T16:20:29.936+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO SparkContext: Created broadcast 3 from showString at NativeMethodAccessorImpl.java:0
[2025-08-12T16:20:29.938+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-12T16:20:29.957+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-08-12T16:20:29.960+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-12T16:20:29.961+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)
[2025-08-12T16:20:29.962+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Parents of final stage: List()
[2025-08-12T16:20:29.963+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Missing parents: List()
[2025-08-12T16:20:29.963+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-12T16:20:29.987+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 50.4 KiB, free 433.8 MiB)
[2025-08-12T16:20:29.989+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 433.8 MiB)
[2025-08-12T16:20:29.991+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1676
[2025-08-12T16:20:29.992+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-12T16:20:29.993+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-08-12T16:20:29.999+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.6,executor 0, partition 0, PROCESS_LOCAL, 10617 bytes)
[2025-08-12T16:20:31.885+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1891 ms on 172.18.0.6 (executor 0) (1/1)
[2025-08-12T16:20:31.886+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool
[2025-08-12T16:20:31.887+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 1922 ms
[2025-08-12T16:20:31.887+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-12T16:20:31.887+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO TaskSchedulerImpl: Canceling stage 2
[2025-08-12T16:20:31.888+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-08-12T16:20:31.888+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 1929.604808 ms
[2025-08-12T16:20:31.964+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:31 INFO CodeGenerator: Code generated in 35.166951 ms
[2025-08-12T16:20:32.033+0000] {spark_submit.py:521} INFO - +-----------------------------------------------------+--------------------------------------------------------+---+----------------+----------+----------+----------------+----------------+-------------------------------------+-----------+--------------+--------------+--------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------+
[2025-08-12T16:20:32.034+0000] {spark_submit.py:521} INFO - |job_position                                         |company_name                                            |yoe|job_posting_date|min_salary|max_salary|company_size_min|company_size_max|major_field                          |level      |num_of_recruit|work_form     |relation_fields                                               |skills_extracted                                                                                                                          |city       |district    |source|
[2025-08-12T16:20:32.034+0000] {spark_submit.py:521} INFO - +-----------------------------------------------------+--------------------------------------------------------+---+----------------+----------+----------+----------------+----------------+-------------------------------------+-----------+--------------+--------------+--------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------+
[2025-08-12T16:20:32.035+0000] {spark_submit.py:521} INFO - |cdp data engineering                                 |CÔNG TY CỔ PHẦN DƯỢC PHẨM FPT LONG CHÂU                 |3.0|2025-08-31      |NULL      |NULL      |10000           |10000           |Dược phẩm / Y tế / Công nghệ sinh học|Trưởng nhóm|1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[]                                                                                                                                        |Hồ Chí Minh|Quận 7      |topcv |
[2025-08-12T16:20:32.035+0000] {spark_submit.py:521} INFO - |data engineer                                        |Alphaway Technology                                     |3.0|2025-08-14      |39000000  |NULL      |100             |499             |IT - Phần mềm                        |Trưởng nhóm|5.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, sql, javascript, aws, spark, redshift]                                                                                     |Hà Nội     |Cầu Giấy    |topcv |
[2025-08-12T16:20:32.035+0000] {spark_submit.py:521} INFO - |data engineer                                        |CÔNG TY CỔ PHẦN CÔNG NGHỆ ALPHAWAY                      |2.0|2025-09-04      |NULL      |NULL      |500             |1000            |IT - Phần mềm                        |Trưởng nhóm|2.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, sql, javascript, aws, spark, kafka, data warehouse, data lake, redshift]                                                   |Hà Nội     |Cầu Giấy    |topcv |
[2025-08-12T16:20:32.035+0000] {spark_submit.py:521} INFO - |data engineer                                        |CÔNG TY CỔ PHẦN IKAME GLOBAL                            |1.0|2025-09-30      |1000000   |NULL      |100             |499             |IT - Phần mềm                        |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, sql, aws, azure, gcp, nosql]                                                                                               |Hà Nội     |Đống Đa     |topcv |
[2025-08-12T16:20:32.036+0000] {spark_submit.py:521} INFO - |data engineer                                        |CÔNG TY TNHH GIẢI PHÁP BRAIN TECH                       |2.0|2025-08-13      |NULL      |NULL      |25              |99              |IT - Phần mềm                        |Nhân viên  |3.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, scala, sql, docker, kubernetes, spark, kafka, flink, etl]                                                                        |Hồ Chí Minh|Phú Nhuận   |topcv |
[2025-08-12T16:20:32.036+0000] {spark_submit.py:521} INFO - |data engineer                                        |Công Ty Tài Chính TNHH MTV Mirae Asset                  |3.0|2025-08-21      |NULL      |NULL      |5000            |5000            |Tài chính                            |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, scala, sql, ***, etl, data warehouse, data lake]                                                                       |Hồ Chí Minh|Quận 1      |topcv |
[2025-08-12T16:20:32.036+0000] {spark_submit.py:521} INFO - |data engineer                                        |Công ty Cổ phần Hệ thống Công nghệ ETC                  |2.0|2025-08-31      |NULL      |NULL      |500             |1000            |IT - Phần mềm                        |Nhân viên  |5.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[aws, azure, etl, data warehouse, data lake]                                                                                              |Hà Nội     |Cầu Giấy    |topcv |
[2025-08-12T16:20:32.037+0000] {spark_submit.py:521} INFO - |data engineer                                        |FPT Software                                            |4.0|2025-09-30      |NULL      |NULL      |10000           |10000           |Tài chính                            |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, sql, azure, gcp, git, rest, spark, nosql, tensorflow, pytorch, etl, data pipeline, data lake, bigquery, redshift, snowflake, api]|Đà Nẵng    |Ngũ Hành Sơn|topcv |
[2025-08-12T16:20:32.037+0000] {spark_submit.py:521} INFO - |data engineer                                        |TMA Solutions                                           |1.0|2025-10-17      |NULL      |NULL      |3000            |3000            |IT - Phần mềm                        |Nhân viên  |3.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, scala, sql, aws, azure, spark, etl, redshift]                                                                                    |Bình Định  |TP Quy Nhơn |topcv |
[2025-08-12T16:20:32.038+0000] {spark_submit.py:521} INFO - |data engineer (banking)                              |Công ty Cổ phần Công nghệ Alphaway                      |2.0|2025-08-24      |25000000  |35000000  |100             |499             |IT - Phần mềm                        |Trưởng nhóm|2.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, aws, spark, etl]                                                                                                                 |Hà Nội     |Hoàn Kiếm   |topcv |
[2025-08-12T16:20:32.038+0000] {spark_submit.py:521} INFO - |data engineer (big data)                             |CÔNG TY CỔ PHẦN PARALINE SOFTWARE                       |2.0|2025-10-31      |18000000  |30000000  |100             |499             |IT - Phần mềm                        |Nhân viên  |6.0           |Toàn thời gian|[Công nghệ Thông tin, Software Engineering, Software Engineer]|[sql, etl]                                                                                                                                |Hà Nội     |Thanh Xuân  |topcv |
[2025-08-12T16:20:32.038+0000] {spark_submit.py:521} INFO - |data engineer (biết làm báo cáo)                     |Công ty TNHH Giải pháp Phân tích dữ liệu Insight Data   |2.0|2025-08-20      |25000000  |30000000  |25              |99              |IT - Phần mềm                        |Trưởng nhóm|3.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[sql, ***, data warehouse]                                                                                                            |Hà Nội     |Thanh Xuân  |topcv |
[2025-08-12T16:20:32.038+0000] {spark_submit.py:521} INFO - |data engineer (cloud)                                |Công ty TNHH Horus Productions                          |1.0|2025-08-23      |15000000  |25000000  |100             |499             |IT - Phần mềm                        |Trưởng nhóm|1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, scala, sql, aws, azure, gcp, bigquery]                                                                                     |Hà Nội     |Nam Từ Liêm |topcv |
[2025-08-12T16:20:32.039+0000] {spark_submit.py:521} INFO - |data engineer (có xe đưa đón)                        |CÔNG TY CỔ PHẦN CÔNG NGHỆ AN NINH MẠNG QUỐC GIA VIỆT NAM|1.0|2025-08-09      |9000000   |15000000  |100             |499             |IT - Phần mềm                        |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, scala, sql, spark, nosql, zeppelin, etl, data warehouse]                                                                         |Hưng Yên   |Yên Mỹ      |topcv |
[2025-08-12T16:20:32.039+0000] {spark_submit.py:521} INFO - |data engineer (da)                                   |Công Ty TNHH LG CNS VIỆT NAM                            |3.0|2025-08-31      |NULL      |NULL      |100             |499             |IT - Phần mềm                        |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[scala, sql, aws, git, data warehouse, data lake, redshift]                                                                               |Hà Nội     |Nam Từ Liêm |topcv |
[2025-08-12T16:20:32.040+0000] {spark_submit.py:521} INFO - |data engineer (fluent english)                       |TECHVIFY SOFTWARE., JSC                                 |3.0|2025-08-12      |NULL      |NULL      |100             |499             |IT - Phần mềm                        |Trưởng nhóm|2.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, sql, azure, rest, graphql, spark, etl, data pipeline, api]                                                                       |Hà Nội     |Nam Từ Liêm |topcv |
[2025-08-12T16:20:32.040+0000] {spark_submit.py:521} INFO - |data engineer (onsite)                               |Công ty cổ phần Ecoit                                   |3.0|2025-08-16      |20000000  |28000000  |25              |99              |IT - Phần mềm                        |Trưởng nhóm|5.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[spark, hadoop, etl, data warehouse, data lake]                                                                                           |Hà Nội     |Cầu Giấy    |topcv |
[2025-08-12T16:20:32.040+0000] {spark_submit.py:521} INFO - |data engineer (python/java, data lake)               |Công ty CP Kinh doanh F88                               |3.0|2025-08-31      |20000000  |55000000  |5000            |5000            |Tài chính                            |Trưởng nhóm|2.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, spark, kafka, ***, etl, data warehouse, data lake]                                                                           |Hà Nội     |Thanh Xuân  |topcv |
[2025-08-12T16:20:32.041+0000] {spark_submit.py:521} INFO - |data engineer - từ 3 năm kinh nghiệm - onsite project|CÔNG TY CỔ PHẦN AHT TECH                                |3.0|2025-08-09      |NULL      |NULL      |100             |499             |IT - Phần mềm                        |Nhân viên  |1.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[sql, git, ***, etl, data warehouse, snowflake]                                                                                       |Hà Nội     |Hoàn Kiếm   |topcv |
[2025-08-12T16:20:32.041+0000] {spark_submit.py:521} INFO - |data engineer các level từ fresher,junior,middle     |công ty cổ phần Data Impact                             |1.0|2025-08-16      |NULL      |NULL      |25              |99              |IT - Phần mềm                        |Nhân viên  |3.0           |Toàn thời gian|[Công nghệ Thông tin, Data Science, Data Engineer]            |[python, java, scala, sql, aws, azure, gcp, spark, hadoop, kafka, ***, nosql, mongodb, etl, data pipeline]                            |Hà Nội     |Hà Đông     |topcv |
[2025-08-12T16:20:32.041+0000] {spark_submit.py:521} INFO - +-----------------------------------------------------+--------------------------------------------------------+---+----------------+----------+----------+----------------+----------------+-------------------------------------+-----------+--------------+--------------+--------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------+-----------+------------+------+
[2025-08-12T16:20:32.042+0000] {spark_submit.py:521} INFO - only showing top 20 rows
[2025-08-12T16:20:32.425+0000] {spark_submit.py:521} INFO - Traceback (most recent call last):
[2025-08-12T16:20:32.426+0000] {spark_submit.py:521} INFO - File "/opt/spark-jobs/gold.transfrom.py", line 67, in <module>
[2025-08-12T16:20:32.427+0000] {spark_submit.py:521} INFO - .join(dim_workform, df.work_form == dim_workform.WorkForm, "left") \
[2025-08-12T16:20:32.428+0000] {spark_submit.py:521} INFO - ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-08-12T16:20:32.428+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/classic/dataframe.py", line 710, in join
[2025-08-12T16:20:32.428+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py", line 1362, in __call__
[2025-08-12T16:20:32.429+0000] {spark_submit.py:521} INFO - File "/home/***/.local/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 288, in deco
[2025-08-12T16:20:32.433+0000] {spark_submit.py:521} INFO - pyspark.errors.exceptions.captured.AnalysisException: Column work_form#11 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as("a").join(df.as("b"), $"a.id" > $"b.id")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.
[2025-08-12T16:20:32.586+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO SparkContext: Invoking stop() from shutdown hook
[2025-08-12T16:20:32.588+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO SparkContext: SparkContext is stopping with exitCode 0 from run at Executors.java:539.
[2025-08-12T16:20:32.599+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO SparkUI: Stopped Spark web UI at http://689c6879ed29:4040
[2025-08-12T16:20:32.602+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-12T16:20:32.602+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-12T16:20:32.622+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-12T16:20:32.644+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO MemoryStore: MemoryStore cleared
[2025-08-12T16:20:32.645+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO BlockManager: BlockManager stopped
[2025-08-12T16:20:32.648+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-12T16:20:32.650+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-12T16:20:32.661+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO SparkContext: Successfully stopped SparkContext
[2025-08-12T16:20:32.662+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO ShutdownHookManager: Shutdown hook called
[2025-08-12T16:20:32.662+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-cc798bf6-9b18-430b-b84e-e564d3b23d3b
[2025-08-12T16:20:32.667+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a84ffff-ce4b-4920-ad81-673e394cb689
[2025-08-12T16:20:32.672+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO ShutdownHookManager: Deleting directory /opt/***/artifacts/spark-843ffb7b-21cf-48bb-8060-d07cbeb308e1
[2025-08-12T16:20:32.676+0000] {spark_submit.py:521} INFO - 25/08/12 16:20:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a84ffff-ce4b-4920-ad81-673e394cb689/pyspark-e7617285-0947-44c0-8aac-0ebd87042c98
[2025-08-12T16:20:32.723+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-08-12T16:20:32.736+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/models/baseoperator.py", line 400, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 156, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.11/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 452, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.driver.extraClassPath=/opt/custom-jars/* --conf spark.executor.extraClassPath=/opt/custom-jars/* --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/opt/***/gcp-key.json --jars /opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar --name arrow-spark --verbose /opt/spark-jobs/gold.transfrom.py. Error code is: 1.
[2025-08-12T16:20:32.747+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=spark_transform_pipeline, task_id=gold_transform, run_id=manual__2025-08-12T16:18:43.292141+00:00, execution_date=20250812T161843, start_date=20250812T162009, end_date=20250812T162032
[2025-08-12T16:20:32.765+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 76 for task gold_transform (Cannot execute: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.driver.extraClassPath=/opt/custom-jars/* --conf spark.executor.extraClassPath=/opt/custom-jars/* --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/opt/***/gcp-key.json --jars /opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar --name arrow-spark --verbose /opt/spark-jobs/gold.transfrom.py. Error code is: 1.; 1369)
[2025-08-12T16:20:32.788+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-08-12T16:20:32.807+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-12T16:20:32.810+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
