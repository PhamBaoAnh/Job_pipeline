[2025-08-13T16:25:34.937+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-08-13T16:25:34.982+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_transform_pipeline.silver_topcv_transform manual__2025-08-13T16:25:29.932262+00:00 [queued]>
[2025-08-13T16:25:34.990+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_transform_pipeline.silver_topcv_transform manual__2025-08-13T16:25:29.932262+00:00 [queued]>
[2025-08-13T16:25:34.990+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-08-13T16:25:35.003+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): silver_topcv_transform> on 2025-08-13 16:25:29.932262+00:00
[2025-08-13T16:25:35.014+0000] {standard_task_runner.py:63} INFO - Started process 1355 to run task
[2025-08-13T16:25:35.018+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'spark_transform_pipeline', 'silver_topcv_transform', 'manual__2025-08-13T16:25:29.932262+00:00', '--job-id', '145', '--raw', '--subdir', 'DAGS_FOLDER/etl_jobs_dag.py', '--cfg-path', '/tmp/tmp8m79q7fg']
[2025-08-13T16:25:35.020+0000] {standard_task_runner.py:91} INFO - Job 145: Subtask silver_topcv_transform
[2025-08-13T16:25:35.078+0000] {task_command.py:426} INFO - Running <TaskInstance: spark_transform_pipeline.silver_topcv_transform manual__2025-08-13T16:25:29.932262+00:00 [running]> on host 45bb482e5411
[2025-08-13T16:25:35.217+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_transform_pipeline' AIRFLOW_CTX_TASK_ID='silver_topcv_transform' AIRFLOW_CTX_EXECUTION_DATE='2025-08-13T16:25:29.932262+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-08-13T16:25:29.932262+00:00'
[2025-08-13T16:25:35.218+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-08-13T16:25:35.257+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2025-08-13T16:25:35.259+0000] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.driver.extraClassPath=/opt/custom-jars/* --conf spark.executor.extraClassPath=/opt/custom-jars/* --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS --conf spark.hadoop.google.cloud.auth.service.account.enable=true --conf spark.hadoop.google.cloud.auth.service.account.json.keyfile=/opt/***/gcp-key.json --jars /opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar --name arrow-spark --verbose /opt/spark-jobs/load/upload_jobs_to_dwh.py
[2025-08-13T16:25:37.094+0000] {spark_submit.py:521} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-08-13T16:25:38.682+0000] {spark_submit.py:521} INFO - Parsed arguments:
[2025-08-13T16:25:38.684+0000] {spark_submit.py:521} INFO - master                  spark://spark-master:7077
[2025-08-13T16:25:38.684+0000] {spark_submit.py:521} INFO - remote                  null
[2025-08-13T16:25:38.684+0000] {spark_submit.py:521} INFO - deployMode              null
[2025-08-13T16:25:38.685+0000] {spark_submit.py:521} INFO - executorMemory          null
[2025-08-13T16:25:38.685+0000] {spark_submit.py:521} INFO - executorCores           null
[2025-08-13T16:25:38.685+0000] {spark_submit.py:521} INFO - totalExecutorCores      null
[2025-08-13T16:25:38.686+0000] {spark_submit.py:521} INFO - propertiesFile          null
[2025-08-13T16:25:38.686+0000] {spark_submit.py:521} INFO - driverMemory            null
[2025-08-13T16:25:38.687+0000] {spark_submit.py:521} INFO - driverCores             null
[2025-08-13T16:25:38.687+0000] {spark_submit.py:521} INFO - driverExtraClassPath    /opt/custom-jars/*
[2025-08-13T16:25:38.687+0000] {spark_submit.py:521} INFO - driverExtraLibraryPath  null
[2025-08-13T16:25:38.687+0000] {spark_submit.py:521} INFO - driverExtraJavaOptions  null
[2025-08-13T16:25:38.688+0000] {spark_submit.py:521} INFO - supervise               false
[2025-08-13T16:25:38.688+0000] {spark_submit.py:521} INFO - queue                   null
[2025-08-13T16:25:38.689+0000] {spark_submit.py:521} INFO - numExecutors            null
[2025-08-13T16:25:38.689+0000] {spark_submit.py:521} INFO - files                   null
[2025-08-13T16:25:38.689+0000] {spark_submit.py:521} INFO - pyFiles                 null
[2025-08-13T16:25:38.689+0000] {spark_submit.py:521} INFO - archives                null
[2025-08-13T16:25:38.690+0000] {spark_submit.py:521} INFO - mainClass               null
[2025-08-13T16:25:38.690+0000] {spark_submit.py:521} INFO - primaryResource         file:/opt/spark-jobs/load/upload_jobs_to_dwh.py
[2025-08-13T16:25:38.691+0000] {spark_submit.py:521} INFO - name                    arrow-spark
[2025-08-13T16:25:38.691+0000] {spark_submit.py:521} INFO - childArgs               []
[2025-08-13T16:25:38.692+0000] {spark_submit.py:521} INFO - jars                    file:/opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar
[2025-08-13T16:25:38.692+0000] {spark_submit.py:521} INFO - packages                null
[2025-08-13T16:25:38.692+0000] {spark_submit.py:521} INFO - packagesExclusions      null
[2025-08-13T16:25:38.692+0000] {spark_submit.py:521} INFO - repositories            null
[2025-08-13T16:25:38.693+0000] {spark_submit.py:521} INFO - verbose                 true
[2025-08-13T16:25:38.693+0000] {spark_submit.py:521} INFO - 
[2025-08-13T16:25:38.693+0000] {spark_submit.py:521} INFO - Spark properties used, including those specified through
[2025-08-13T16:25:38.693+0000] {spark_submit.py:521} INFO - --conf and those from the properties file null:
[2025-08-13T16:25:38.694+0000] {spark_submit.py:521} INFO - (spark.driver.extraClassPath,/opt/custom-jars/*)
[2025-08-13T16:25:38.694+0000] {spark_submit.py:521} INFO - (spark.executor.extraClassPath,/opt/custom-jars/*)
[2025-08-13T16:25:38.694+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.AbstractFileSystem.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS)
[2025-08-13T16:25:38.694+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-08-13T16:25:38.695+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.enable,true)
[2025-08-13T16:25:38.695+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.json.keyfile,/opt/***/gcp-key.json)
[2025-08-13T16:25:38.695+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-08-13T16:25:38.696+0000] {spark_submit.py:521} INFO - 
[2025-08-13T16:25:38.696+0000] {spark_submit.py:521} INFO - 
[2025-08-13T16:25:39.441+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-08-13T16:25:41.082+0000] {spark_submit.py:521} INFO - Main class:
[2025-08-13T16:25:41.083+0000] {spark_submit.py:521} INFO - org.apache.spark.deploy.PythonRunner
[2025-08-13T16:25:41.083+0000] {spark_submit.py:521} INFO - Arguments:
[2025-08-13T16:25:41.084+0000] {spark_submit.py:521} INFO - file:/opt/spark-jobs/load/upload_jobs_to_dwh.py
[2025-08-13T16:25:41.084+0000] {spark_submit.py:521} INFO - null
[2025-08-13T16:25:41.084+0000] {spark_submit.py:521} INFO - Spark config:
[2025-08-13T16:25:41.084+0000] {spark_submit.py:521} INFO - (spark.app.name,arrow-spark)
[2025-08-13T16:25:41.085+0000] {spark_submit.py:521} INFO - (spark.app.submitTime,1755102341058)
[2025-08-13T16:25:41.085+0000] {spark_submit.py:521} INFO - (spark.driver.extraClassPath,/opt/custom-jars/*)
[2025-08-13T16:25:41.085+0000] {spark_submit.py:521} INFO - (spark.executor.extraClassPath,/opt/custom-jars/*)
[2025-08-13T16:25:41.085+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.AbstractFileSystem.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS)
[2025-08-13T16:25:41.086+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.gs.impl,com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem)
[2025-08-13T16:25:41.086+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.enable,true)
[2025-08-13T16:25:41.086+0000] {spark_submit.py:521} INFO - (spark.hadoop.google.cloud.auth.service.account.json.keyfile,/opt/***/gcp-key.json)
[2025-08-13T16:25:41.086+0000] {spark_submit.py:521} INFO - (spark.jars,file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar)
[2025-08-13T16:25:41.087+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-08-13T16:25:41.087+0000] {spark_submit.py:521} INFO - (spark.repl.local.jars,file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar)
[2025-08-13T16:25:41.087+0000] {spark_submit.py:521} INFO - (spark.submit.deployMode,client)
[2025-08-13T16:25:41.087+0000] {spark_submit.py:521} INFO - (spark.submit.pyFiles,)
[2025-08-13T16:25:41.088+0000] {spark_submit.py:521} INFO - Classpath elements:
[2025-08-13T16:25:41.088+0000] {spark_submit.py:521} INFO - file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar
[2025-08-13T16:25:41.089+0000] {spark_submit.py:521} INFO - 
[2025-08-13T16:25:41.089+0000] {spark_submit.py:521} INFO - 
[2025-08-13T16:25:43.128+0000] {spark_submit.py:521} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-08-13T16:25:43.139+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkContext: Running Spark version 4.0.0
[2025-08-13T16:25:43.143+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-08-13T16:25:43.144+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkContext: Java version 17.0.16
[2025-08-13T16:25:43.177+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceUtils: ==============================================================
[2025-08-13T16:25:43.178+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-08-13T16:25:43.179+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceUtils: ==============================================================
[2025-08-13T16:25:43.179+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkContext: Submitted application: load_to_bq_direct_parquet
[2025-08-13T16:25:43.215+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-08-13T16:25:43.219+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceProfile: Limiting resource is cpu
[2025-08-13T16:25:43.220+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-08-13T16:25:43.290+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SecurityManager: Changing view acls to: ***
[2025-08-13T16:25:43.291+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SecurityManager: Changing modify acls to: ***
[2025-08-13T16:25:43.291+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-13T16:25:43.291+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-13T16:25:43.294+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-13T16:25:43.645+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO Utils: Successfully started service 'sparkDriver' on port 34291.
[2025-08-13T16:25:43.734+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkEnv: Registering MapOutputTracker
[2025-08-13T16:25:43.768+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkEnv: Registering BlockManagerMaster
[2025-08-13T16:25:43.797+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-08-13T16:25:43.798+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-08-13T16:25:43.807+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-08-13T16:25:43.864+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8b7b8717-dbf5-4a12-94ac-820c0c8fe617
[2025-08-13T16:25:43.898+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-08-13T16:25:44.078+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-08-13T16:25:44.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-08-13T16:25:44.214+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SparkContext: Added JAR file:///opt/custom-jars/gcs-connector-hadoop3-2.2.14-shaded.jar at spark://45bb482e5411:34291/jars/gcs-connector-hadoop3-2.2.14-shaded.jar with timestamp 1755102343117
[2025-08-13T16:25:44.246+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SecurityManager: Changing view acls to: ***
[2025-08-13T16:25:44.247+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SecurityManager: Changing modify acls to: ***
[2025-08-13T16:25:44.248+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SecurityManager: Changing view acls groups to: ***
[2025-08-13T16:25:44.248+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SecurityManager: Changing modify acls groups to: ***
[2025-08-13T16:25:44.248+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-08-13T16:25:44.433+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-08-13T16:25:44.520+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:44 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.3:7077 after 59 ms (0 ms spent in bootstraps)
[2025-08-13T16:25:45.363+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250813162545-0003
[2025-08-13T16:25:45.382+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34849.
[2025-08-13T16:25:45.383+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO NettyBlockTransferService: Server created on 45bb482e5411:34849
[2025-08-13T16:25:45.385+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-08-13T16:25:45.421+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 45bb482e5411, 34849, None)
[2025-08-13T16:25:45.430+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO BlockManagerMasterEndpoint: Registering block manager 45bb482e5411:34849 with 434.4 MiB RAM, BlockManagerId(driver, 45bb482e5411, 34849, None)
[2025-08-13T16:25:45.433+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 45bb482e5411, 34849, None)
[2025-08-13T16:25:45.437+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 45bb482e5411, 34849, None)
[2025-08-13T16:25:45.501+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250813162545-0003/0 on worker-20250813160956-172.18.0.4-33339 (172.18.0.4:33339) with 2 core(s)
[2025-08-13T16:25:45.503+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20250813162545-0003/0 on hostPort 172.18.0.4:33339 with 2 core(s), 1024.0 MiB RAM
[2025-08-13T16:25:45.617+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-08-13T16:25:47.929+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:47 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250813162545-0003/0 is now RUNNING
[2025-08-13T16:25:48.600+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:48 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-08-13T16:25:48.602+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:48 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-08-13T16:25:53.939+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:53 INFO InMemoryFileIndex: It took 202 ms to list leaf files for 1 paths.
[2025-08-13T16:25:54.498+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:25:54.550+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:25:54.552+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:25:54.555+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:25:54.567+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:25:54.577+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:25:54.717+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-08-13T16:25:54.763+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 116.0 KiB, free 434.3 MiB)
[2025-08-13T16:25:54.878+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.2 MiB)
[2025-08-13T16:25:54.894+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:25:54.932+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:25:54.969+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:54 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-08-13T16:25:58.808+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:58 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:39762) with ID 0, ResourceProfileId 0
[2025-08-13T16:25:59.329+0000] {spark_submit.py:521} INFO - 25/08/13 16:25:59 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.4:44995 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.4, 44995, None)
[2025-08-13T16:26:03.778+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10077 bytes)
[2025-08-13T16:26:13.604+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 9875 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:13.610+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool
[2025-08-13T16:26:13.615+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 19003 ms
[2025-08-13T16:26:13.617+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:13.618+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO TaskSchedulerImpl: Canceling stage 0
[2025-08-13T16:26:13.619+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-08-13T16:26:13.622+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:13 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 19122.942604 ms
[2025-08-13T16:26:14.584+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:14 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:26:14.589+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:14 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:26:15.138+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:15.168+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:15.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:15.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:15.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:15.170+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:15.170+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:16.305+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO CodeGenerator: Code generated in 189.835856 ms
[2025-08-13T16:26:16.325+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 217.8 KiB, free 434.2 MiB)
[2025-08-13T16:26:16.342+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 434.1 MiB)
[2025-08-13T16:26:16.345+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO SparkContext: Created broadcast 1 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:16.375+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:26:16.432+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:16.436+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:16.436+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:16.436+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:16.437+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:16.437+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:16.467+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 243.1 KiB, free 433.9 MiB)
[2025-08-13T16:26:16.470+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 87.6 KiB, free 433.8 MiB)
[2025-08-13T16:26:16.472+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:16.473+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:16.473+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-08-13T16:26:16.477+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:16 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10848 bytes)
[2025-08-13T16:26:22.901+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 6426 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:22.902+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool
[2025-08-13T16:26:22.903+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 6465 ms
[2025-08-13T16:26:22.903+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:22.903+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO TaskSchedulerImpl: Canceling stage 1
[2025-08-13T16:26:22.904+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-08-13T16:26:22.904+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 6471.129557 ms
[2025-08-13T16:26:22.906+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:22 INFO FileFormatWriter: Start to commit write Job 09bc2c92-9713-4aeb-8b1c-a11af3e96e4d.
[2025-08-13T16:26:24.991+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:24 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_company/_temporary/0/task_20250813162616453461460685210441_0001_m_000000/' directory.
[2025-08-13T16:26:25.803+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:25 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_company/' directory.
[2025-08-13T16:26:28.603+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:28 INFO FileFormatWriter: Write Job 09bc2c92-9713-4aeb-8b1c-a11af3e96e4d committed. Elapsed time: 5696 ms.
[2025-08-13T16:26:28.613+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:28 INFO FileFormatWriter: Finished processing stats for write job 09bc2c92-9713-4aeb-8b1c-a11af3e96e4d.
[2025-08-13T16:26:28.634+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'dim_company': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:26:29.312+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO InMemoryFileIndex: It took 119 ms to list leaf files for 1 paths.
[2025-08-13T16:26:29.418+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:29.419+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Got job 2 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:29.420+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Final stage: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:29.420+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:29.421+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:29.421+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:29.429+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 116.0 KiB, free 433.7 MiB)
[2025-08-13T16:26:29.434+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 433.7 MiB)
[2025-08-13T16:26:29.435+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:29.436+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:29.436+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-08-13T16:26:29.437+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10085 bytes)
[2025-08-13T16:26:30.144+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 707 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:30.145+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool
[2025-08-13T16:26:30.145+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO DAGScheduler: ResultStage 2 (parquet at NativeMethodAccessorImpl.java:0) finished in 724 ms
[2025-08-13T16:26:30.146+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:30.146+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO TaskSchedulerImpl: Canceling stage 2
[2025-08-13T16:26:30.146+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-08-13T16:26:30.146+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO DAGScheduler: Job 2 finished: parquet at NativeMethodAccessorImpl.java:0, took 727.713412 ms
[2025-08-13T16:26:30.168+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:26:30.168+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:26:30.280+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:30.282+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:30.282+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:30.284+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:30.284+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:30.285+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:30.285+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:30 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:31.118+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO CodeGenerator: Code generated in 17.63333 ms
[2025-08-13T16:26:31.122+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 218.0 KiB, free 433.5 MiB)
[2025-08-13T16:26:31.131+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 39.8 KiB, free 433.4 MiB)
[2025-08-13T16:26:31.137+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO SparkContext: Created broadcast 4 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:31.138+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:26:31.143+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:31.144+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Got job 3 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:31.144+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Final stage: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:31.144+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:31.145+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:31.145+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:31.155+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 245.3 KiB, free 433.2 MiB)
[2025-08-13T16:26:31.166+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 88.0 KiB, free 433.1 MiB)
[2025-08-13T16:26:31.168+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:31.171+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:31.172+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-08-13T16:26:31.177+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10856 bytes)
[2025-08-13T16:26:35.695+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 4517 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:35.696+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO TaskSchedulerImpl: Removed TaskSet 3.0 whose tasks have all completed, from pool
[2025-08-13T16:26:35.697+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO DAGScheduler: ResultStage 3 (parquet at NativeMethodAccessorImpl.java:0) finished in 4550 ms
[2025-08-13T16:26:35.697+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:35.698+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO TaskSchedulerImpl: Canceling stage 3
[2025-08-13T16:26:35.698+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-08-13T16:26:35.699+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO DAGScheduler: Job 3 finished: parquet at NativeMethodAccessorImpl.java:0, took 4553.434842 ms
[2025-08-13T16:26:35.700+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:35 INFO FileFormatWriter: Start to commit write Job 9ce6d837-079e-49cc-afd6-a776e3128dbc.
[2025-08-13T16:26:37.767+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:37 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_jobposition/_temporary/0/task_202508131626314885583952672997056_0003_m_000000/' directory.
[2025-08-13T16:26:38.579+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:38 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_jobposition/' directory.
[2025-08-13T16:26:39.399+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:39 INFO FileFormatWriter: Write Job 9ce6d837-079e-49cc-afd6-a776e3128dbc committed. Elapsed time: 3699 ms.
[2025-08-13T16:26:39.401+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:39 INFO FileFormatWriter: Finished processing stats for write job 9ce6d837-079e-49cc-afd6-a776e3128dbc.
[2025-08-13T16:26:39.406+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'dim_jobposition': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:26:40.116+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO InMemoryFileIndex: It took 118 ms to list leaf files for 1 paths.
[2025-08-13T16:26:40.166+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:40.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Got job 4 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:40.171+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Final stage: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:40.172+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:40.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:40.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:40.191+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 116.0 KiB, free 434.0 MiB)
[2025-08-13T16:26:40.214+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.0 MiB)
[2025-08-13T16:26:40.224+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:40.231+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:40.237+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-08-13T16:26:40.239+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10071 bytes)
[2025-08-13T16:26:40.887+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 654 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:40.889+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool
[2025-08-13T16:26:40.890+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: ResultStage 4 (parquet at NativeMethodAccessorImpl.java:0) finished in 714 ms
[2025-08-13T16:26:40.890+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:40.890+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSchedulerImpl: Canceling stage 4
[2025-08-13T16:26:40.891+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-08-13T16:26:40.891+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO DAGScheduler: Job 4 finished: parquet at NativeMethodAccessorImpl.java:0, took 723.729857 ms
[2025-08-13T16:26:40.933+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:26:40.934+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:40 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:26:41.037+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:41.039+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:41.039+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:41.040+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:41.040+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:41.040+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:41.040+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:41.409+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO CodeGenerator: Code generated in 14.159381 ms
[2025-08-13T16:26:41.416+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 218.0 KiB, free 433.8 MiB)
[2025-08-13T16:26:41.434+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 433.9 MiB)
[2025-08-13T16:26:41.442+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO SparkContext: Created broadcast 7 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:41.447+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:26:41.470+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:41.472+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Got job 5 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:41.473+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Final stage: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:41.474+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:41.475+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:41.475+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:41.506+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 244.3 KiB, free 433.9 MiB)
[2025-08-13T16:26:41.523+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 87.5 KiB, free 433.8 MiB)
[2025-08-13T16:26:41.527+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:41.528+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:41.529+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-08-13T16:26:41.532+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:41 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10842 bytes)
[2025-08-13T16:26:46.747+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 5215 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:46.748+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool
[2025-08-13T16:26:46.748+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO DAGScheduler: ResultStage 5 (parquet at NativeMethodAccessorImpl.java:0) finished in 5272 ms
[2025-08-13T16:26:46.749+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:46.749+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO TaskSchedulerImpl: Canceling stage 5
[2025-08-13T16:26:46.749+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-08-13T16:26:46.750+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO DAGScheduler: Job 5 finished: parquet at NativeMethodAccessorImpl.java:0, took 5278.356981 ms
[2025-08-13T16:26:46.750+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:46 INFO FileFormatWriter: Start to commit write Job 5baf8bc0-f47a-429b-acfc-7e9aa8b2718e.
[2025-08-13T16:26:48.773+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:48 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_date/_temporary/0/task_202508131626417240583204319650476_0005_m_000000/' directory.
[2025-08-13T16:26:49.590+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:49 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_date/' directory.
[2025-08-13T16:26:50.432+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:50 INFO FileFormatWriter: Write Job 5baf8bc0-f47a-429b-acfc-7e9aa8b2718e committed. Elapsed time: 3682 ms.
[2025-08-13T16:26:50.433+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:50 INFO FileFormatWriter: Finished processing stats for write job 5baf8bc0-f47a-429b-acfc-7e9aa8b2718e.
[2025-08-13T16:26:50.436+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'dim_date': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:26:51.116+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO InMemoryFileIndex: It took 117 ms to list leaf files for 1 paths.
[2025-08-13T16:26:51.146+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:51.147+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Got job 6 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:51.148+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Final stage: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:51.148+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:51.149+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:51.149+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:51.160+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 116.0 KiB, free 434.0 MiB)
[2025-08-13T16:26:51.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.2 MiB)
[2025-08-13T16:26:51.172+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:51.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:51.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-08-13T16:26:51.175+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10079 bytes)
[2025-08-13T16:26:51.595+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 421 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:51.597+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSchedulerImpl: Removed TaskSet 6.0 whose tasks have all completed, from pool
[2025-08-13T16:26:51.597+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: ResultStage 6 (parquet at NativeMethodAccessorImpl.java:0) finished in 447 ms
[2025-08-13T16:26:51.598+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:51.598+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSchedulerImpl: Canceling stage 6
[2025-08-13T16:26:51.599+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-08-13T16:26:51.599+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO DAGScheduler: Job 6 finished: parquet at NativeMethodAccessorImpl.java:0, took 450.511279 ms
[2025-08-13T16:26:51.627+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:26:51.628+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:26:51.730+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:51.731+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:51.732+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:51.733+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:51.733+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:26:51.734+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:26:51.734+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:51 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:26:52.105+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO CodeGenerator: Code generated in 11.015378 ms
[2025-08-13T16:26:52.109+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 217.7 KiB, free 434.0 MiB)
[2025-08-13T16:26:52.123+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 39.7 KiB, free 434.1 MiB)
[2025-08-13T16:26:52.128+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO SparkContext: Created broadcast 10 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:52.130+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:26:52.138+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:26:52.139+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Got job 7 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:26:52.140+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Final stage: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:26:52.140+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:26:52.141+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:26:52.141+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:26:52.159+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 242.4 KiB, free 433.9 MiB)
[2025-08-13T16:26:52.169+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 87.3 KiB, free 433.8 MiB)
[2025-08-13T16:26:52.172+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:26:52.173+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:26:52.174+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-08-13T16:26:52.176+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:52 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10850 bytes)
[2025-08-13T16:26:56.068+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 3893 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:26:56.071+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool
[2025-08-13T16:26:56.072+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO DAGScheduler: ResultStage 7 (parquet at NativeMethodAccessorImpl.java:0) finished in 3928 ms
[2025-08-13T16:26:56.072+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:26:56.073+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO TaskSchedulerImpl: Canceling stage 7
[2025-08-13T16:26:56.073+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-08-13T16:26:56.073+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO DAGScheduler: Job 7 finished: parquet at NativeMethodAccessorImpl.java:0, took 3931.591143 ms
[2025-08-13T16:26:56.073+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:56 INFO FileFormatWriter: Start to commit write Job 2f37cac6-1526-47cb-bc54-38c647c42873.
[2025-08-13T16:26:57.904+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:57 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_workform/_temporary/0/task_202508131626525935233133943763876_0007_m_000000/' directory.
[2025-08-13T16:26:58.705+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:58 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_workform/' directory.
[2025-08-13T16:26:59.716+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:59 INFO FileFormatWriter: Write Job 2f37cac6-1526-47cb-bc54-38c647c42873 committed. Elapsed time: 3647 ms.
[2025-08-13T16:26:59.717+0000] {spark_submit.py:521} INFO - 25/08/13 16:26:59 INFO FileFormatWriter: Finished processing stats for write job 2f37cac6-1526-47cb-bc54-38c647c42873.
[2025-08-13T16:26:59.722+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'dim_workform': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:27:00.404+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO InMemoryFileIndex: It took 114 ms to list leaf files for 1 paths.
[2025-08-13T16:27:00.429+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:00.429+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Got job 8 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:00.430+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Final stage: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:00.430+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:00.431+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:00.431+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[29] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:00.439+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 116.0 KiB, free 434.0 MiB)
[2025-08-13T16:27:00.447+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.0 MiB)
[2025-08-13T16:27:00.451+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:00.452+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[29] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:00.453+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-08-13T16:27:00.454+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10075 bytes)
[2025-08-13T16:27:00.846+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 392 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:00.847+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool
[2025-08-13T16:27:00.847+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: ResultStage 8 (parquet at NativeMethodAccessorImpl.java:0) finished in 415 ms
[2025-08-13T16:27:00.847+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:00.848+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSchedulerImpl: Canceling stage 8
[2025-08-13T16:27:00.848+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-08-13T16:27:00.848+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO DAGScheduler: Job 8 finished: parquet at NativeMethodAccessorImpl.java:0, took 418.022604 ms
[2025-08-13T16:27:00.861+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:27:00.861+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:27:00.960+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:00.962+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:00.962+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:00.962+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:00.963+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:00.963+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:00.964+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:00 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:01.331+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 217.7 KiB, free 434.0 MiB)
[2025-08-13T16:27:01.345+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 434.1 MiB)
[2025-08-13T16:27:01.349+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO SparkContext: Created broadcast 13 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:01.350+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:27:01.355+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:01.356+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Got job 9 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:01.356+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Final stage: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:01.357+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:01.357+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:01.357+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:01.375+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 242.3 KiB, free 433.9 MiB)
[2025-08-13T16:27:01.382+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 87.4 KiB, free 433.8 MiB)
[2025-08-13T16:27:01.383+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:01.384+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[34] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:01.384+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-08-13T16:27:01.385+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:01 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10846 bytes)
[2025-08-13T16:27:05.140+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 3754 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:05.141+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO TaskSchedulerImpl: Removed TaskSet 9.0 whose tasks have all completed, from pool
[2025-08-13T16:27:05.142+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO DAGScheduler: ResultStage 9 (parquet at NativeMethodAccessorImpl.java:0) finished in 3783 ms
[2025-08-13T16:27:05.142+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:05.142+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO TaskSchedulerImpl: Canceling stage 9
[2025-08-13T16:27:05.142+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-08-13T16:27:05.143+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO DAGScheduler: Job 9 finished: parquet at NativeMethodAccessorImpl.java:0, took 3786.191599 ms
[2025-08-13T16:27:05.143+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:05 INFO FileFormatWriter: Start to commit write Job dd010ba5-59c3-4393-9d03-c8b05561c3a6.
[2025-08-13T16:27:06.722+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:06 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_skills/_temporary/0/task_202508131627013292354646910054473_0009_m_000000/' directory.
[2025-08-13T16:27:07.411+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:07 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/dim_skills/' directory.
[2025-08-13T16:27:08.228+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO FileFormatWriter: Write Job dd010ba5-59c3-4393-9d03-c8b05561c3a6 committed. Elapsed time: 3085 ms.
[2025-08-13T16:27:08.229+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO FileFormatWriter: Finished processing stats for write job dd010ba5-59c3-4393-9d03-c8b05561c3a6.
[2025-08-13T16:27:08.239+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'dim_skills': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:27:08.932+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO InMemoryFileIndex: It took 116 ms to list leaf files for 1 paths.
[2025-08-13T16:27:08.967+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:08.969+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Got job 10 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:08.969+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Final stage: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:08.969+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:08.970+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:08.970+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:08.981+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 116.0 KiB, free 434.0 MiB)
[2025-08-13T16:27:08.990+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.0 MiB)
[2025-08-13T16:27:08.991+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:08.992+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[36] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:08.993+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-08-13T16:27:08.999+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:08 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10071 bytes)
[2025-08-13T16:27:09.404+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 406 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:09.406+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool
[2025-08-13T16:27:09.407+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO DAGScheduler: ResultStage 10 (parquet at NativeMethodAccessorImpl.java:0) finished in 436 ms
[2025-08-13T16:27:09.408+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:09.408+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO TaskSchedulerImpl: Canceling stage 10
[2025-08-13T16:27:09.409+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-08-13T16:27:09.409+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO DAGScheduler: Job 10 finished: parquet at NativeMethodAccessorImpl.java:0, took 438.817986 ms
[2025-08-13T16:27:09.463+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:27:09.464+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:27:09.586+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:09.587+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:09.588+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:09.588+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:09.589+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:09.589+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:09.589+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:09.967+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO CodeGenerator: Code generated in 34.255211 ms
[2025-08-13T16:27:09.974+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 218.6 KiB, free 434.0 MiB)
[2025-08-13T16:27:09.999+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:09 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 39.8 KiB, free 434.1 MiB)
[2025-08-13T16:27:10.005+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO SparkContext: Created broadcast 16 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:10.008+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4203556 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:27:10.025+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:10.026+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Got job 11 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:10.027+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Final stage: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:10.028+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:10.030+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:10.031+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[41] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:10.051+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 247.3 KiB, free 433.9 MiB)
[2025-08-13T16:27:10.054+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 88.2 KiB, free 433.8 MiB)
[2025-08-13T16:27:10.055+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:10.056+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[41] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:10.056+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-08-13T16:27:10.059+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:10 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 11039 bytes)
[2025-08-13T16:27:15.392+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 5332 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:15.393+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool
[2025-08-13T16:27:15.393+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO DAGScheduler: ResultStage 11 (parquet at NativeMethodAccessorImpl.java:0) finished in 5364 ms
[2025-08-13T16:27:15.394+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:15.394+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO TaskSchedulerImpl: Canceling stage 11
[2025-08-13T16:27:15.395+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-08-13T16:27:15.395+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO DAGScheduler: Job 11 finished: parquet at NativeMethodAccessorImpl.java:0, took 5368.33709 ms
[2025-08-13T16:27:15.395+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:15 INFO FileFormatWriter: Start to commit write Job 795d0942-c5d3-44d3-aa11-de046d3bfd94.
[2025-08-13T16:27:17.442+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:17 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/fact_job/_temporary/0/task_202508131627107586198246611391275_0011_m_000000/' directory.
[2025-08-13T16:27:18.242+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:18 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/fact_job/' directory.
[2025-08-13T16:27:19.064+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO FileFormatWriter: Write Job 795d0942-c5d3-44d3-aa11-de046d3bfd94 committed. Elapsed time: 3669 ms.
[2025-08-13T16:27:19.065+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO FileFormatWriter: Finished processing stats for write job 795d0942-c5d3-44d3-aa11-de046d3bfd94.
[2025-08-13T16:27:19.099+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'fact_job': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:27:19.784+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO InMemoryFileIndex: It took 139 ms to list leaf files for 1 paths.
[2025-08-13T16:27:19.805+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:19.806+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Got job 12 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:19.806+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Final stage: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:19.806+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:19.807+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:19.807+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:19.812+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 116.0 KiB, free 434.0 MiB)
[2025-08-13T16:27:19.832+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 434.0 MiB)
[2025-08-13T16:27:19.833+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:19.834+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[43] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:19.835+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-08-13T16:27:19.837+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:19 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10081 bytes)
[2025-08-13T16:27:20.292+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 456 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:20.293+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSchedulerImpl: Removed TaskSet 12.0 whose tasks have all completed, from pool
[2025-08-13T16:27:20.294+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: ResultStage 12 (parquet at NativeMethodAccessorImpl.java:0) finished in 486 ms
[2025-08-13T16:27:20.294+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:20.295+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSchedulerImpl: Canceling stage 12
[2025-08-13T16:27:20.295+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-08-13T16:27:20.295+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Job 12 finished: parquet at NativeMethodAccessorImpl.java:0, took 488.16175 ms
[2025-08-13T16:27:20.313+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileSourceStrategy: Pushed Filters:
[2025-08-13T16:27:20.314+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-08-13T16:27:20.413+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:20.414+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:20.414+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:20.415+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:20.415+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-08-13T16:27:20.415+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-08-13T16:27:20.415+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-08-13T16:27:20.792+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO CodeGenerator: Code generated in 9.939089 ms
[2025-08-13T16:27:20.796+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 217.7 KiB, free 434.0 MiB)
[2025-08-13T16:27:20.814+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 39.6 KiB, free 434.1 MiB)
[2025-08-13T16:27:20.826+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO SparkContext: Created broadcast 19 from parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:20.829+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-08-13T16:27:20.849+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-08-13T16:27:20.850+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Got job 13 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-08-13T16:27:20.851+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Final stage: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0)
[2025-08-13T16:27:20.852+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Parents of final stage: List()
[2025-08-13T16:27:20.852+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Missing parents: List()
[2025-08-13T16:27:20.852+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[48] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-08-13T16:27:20.864+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 242.2 KiB, free 433.9 MiB)
[2025-08-13T16:27:20.878+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 87.3 KiB, free 433.8 MiB)
[2025-08-13T16:27:20.881+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676
[2025-08-13T16:27:20.882+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[48] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-08-13T16:27:20.883+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-08-13T16:27:20.884+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:20 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (172.18.0.4,executor 0, partition 0, PROCESS_LOCAL, 10852 bytes)
[2025-08-13T16:27:24.492+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 3609 ms on 172.18.0.4 (executor 0) (1/1)
[2025-08-13T16:27:24.493+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO TaskSchedulerImpl: Removed TaskSet 13.0 whose tasks have all completed, from pool
[2025-08-13T16:27:24.494+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO DAGScheduler: ResultStage 13 (parquet at NativeMethodAccessorImpl.java:0) finished in 3641 ms
[2025-08-13T16:27:24.495+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-08-13T16:27:24.495+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO TaskSchedulerImpl: Canceling stage 13
[2025-08-13T16:27:24.495+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-08-13T16:27:24.495+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO DAGScheduler: Job 13 finished: parquet at NativeMethodAccessorImpl.java:0, took 3644.733267 ms
[2025-08-13T16:27:24.496+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:24 INFO FileFormatWriter: Start to commit write Job 86ef5450-0f77-4b98-adc2-5c8408ef2e01.
[2025-08-13T16:27:26.065+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/fact_jobskill/_temporary/0/task_20250813162720219643886297537045_0013_m_000000/' directory.
[2025-08-13T16:27:26.864+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:26 INFO GoogleCloudStorageFileSystem: Successfully repaired 'gs://job_gcp/tmp/fact_jobskill/' directory.
[2025-08-13T16:27:27.654+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:27 INFO FileFormatWriter: Write Job 86ef5450-0f77-4b98-adc2-5c8408ef2e01 committed. Elapsed time: 3170 ms.
[2025-08-13T16:27:27.655+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:27 INFO FileFormatWriter: Finished processing stats for write job 86ef5450-0f77-4b98-adc2-5c8408ef2e01.
[2025-08-13T16:27:27.740+0000] {spark_submit.py:521} INFO - ❌ Exception với bảng 'fact_jobskill': [Errno 13] Permission denied: 'gsutil'
[2025-08-13T16:27:27.784+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:27 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.
[2025-08-13T16:27:28.036+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO SparkUI: Stopped Spark web UI at http://45bb482e5411:4040
[2025-08-13T16:27:28.065+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-08-13T16:27:28.070+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-08-13T16:27:28.365+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-08-13T16:27:28.659+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO MemoryStore: MemoryStore cleared
[2025-08-13T16:27:28.660+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO BlockManager: BlockManager stopped
[2025-08-13T16:27:28.672+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-08-13T16:27:28.701+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-08-13T16:27:28.762+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:28 INFO SparkContext: Successfully stopped SparkContext
[2025-08-13T16:27:30.405+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:30 INFO ShutdownHookManager: Shutdown hook called
[2025-08-13T16:27:30.406+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc423385-96e5-48b8-a267-739c0d1fc4e9
[2025-08-13T16:27:30.439+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-951237b9-87fb-4d83-925c-5781a9965ca6
[2025-08-13T16:27:30.459+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:30 INFO ShutdownHookManager: Deleting directory /opt/***/artifacts/spark-69b59b09-a9c8-4b1a-8e8e-84ae47b9dd29
[2025-08-13T16:27:30.487+0000] {spark_submit.py:521} INFO - 25/08/13 16:27:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-dc423385-96e5-48b8-a267-739c0d1fc4e9/pyspark-347659f5-6c1b-43b6-9141-8fa71f1b3151
[2025-08-13T16:27:30.731+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-08-13T16:27:31.038+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=spark_transform_pipeline, task_id=silver_topcv_transform, run_id=manual__2025-08-13T16:25:29.932262+00:00, execution_date=20250813T162529, start_date=20250813T162534, end_date=20250813T162731
[2025-08-13T16:27:31.257+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-08-13T16:27:31.611+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-08-13T16:27:31.618+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
