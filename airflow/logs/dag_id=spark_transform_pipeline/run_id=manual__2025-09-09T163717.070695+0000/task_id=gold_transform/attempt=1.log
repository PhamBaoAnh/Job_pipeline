[2025-09-09T16:39:39.515+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-09-09T16:39:39.561+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: spark_transform_pipeline.gold_transform manual__2025-09-09T16:37:17.070695+00:00 [queued]>
[2025-09-09T16:39:39.570+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: spark_transform_pipeline.gold_transform manual__2025-09-09T16:37:17.070695+00:00 [queued]>
[2025-09-09T16:39:39.571+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2025-09-09T16:39:39.582+0000] {taskinstance.py:2330} INFO - Executing <Task(SparkSubmitOperator): gold_transform> on 2025-09-09 16:37:17.070695+00:00
[2025-09-09T16:39:39.594+0000] {standard_task_runner.py:63} INFO - Started process 6350 to run task
[2025-09-09T16:39:39.596+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'spark_transform_pipeline', 'gold_transform', 'manual__2025-09-09T16:37:17.070695+00:00', '--job-id', '139', '--raw', '--subdir', 'DAGS_FOLDER/etl_jobs_dag.py', '--cfg-path', '/tmp/tmp2un_wuw3']
[2025-09-09T16:39:39.598+0000] {standard_task_runner.py:91} INFO - Job 139: Subtask gold_transform
[2025-09-09T16:39:39.641+0000] {task_command.py:426} INFO - Running <TaskInstance: spark_transform_pipeline.gold_transform manual__2025-09-09T16:37:17.070695+00:00 [running]> on host 118acb478e13
[2025-09-09T16:39:39.749+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='spark_transform_pipeline' AIRFLOW_CTX_TASK_ID='gold_transform' AIRFLOW_CTX_EXECUTION_DATE='2025-09-09T16:37:17.070695+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-09-09T16:37:17.070695+00:00'
[2025-09-09T16:39:39.750+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-09-09T16:39:39.784+0000] {base.py:84} INFO - Using connection ID 'spark_default' for task execution.
[2025-09-09T16:39:39.785+0000] {spark_submit.py:351} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.master=spark://spark-master:7077 --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 --conf spark.hadoop.fs.s3a.access.key=minio --conf spark.hadoop.fs.s3a.secret.key=****** --conf spark.hadoop.fs.s3a.path.style.access=true --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false --packages org.apache.hadoop:hadoop-aws:3.4.1,software.amazon.awssdk:bundle:2.20.15 --name arrow-spark --verbose /opt/spark-jobs/elt/transform/gold_transform.py
[2025-09-09T16:39:41.058+0000] {spark_submit.py:521} INFO - WARNING: Using incubator modules: jdk.incubator.vector
[2025-09-09T16:39:42.429+0000] {spark_submit.py:521} INFO - Parsed arguments:
[2025-09-09T16:39:42.430+0000] {spark_submit.py:521} INFO - master                  spark://spark-master:7077
[2025-09-09T16:39:42.430+0000] {spark_submit.py:521} INFO - remote                  null
[2025-09-09T16:39:42.430+0000] {spark_submit.py:521} INFO - deployMode              null
[2025-09-09T16:39:42.431+0000] {spark_submit.py:521} INFO - executorMemory          null
[2025-09-09T16:39:42.431+0000] {spark_submit.py:521} INFO - executorCores           null
[2025-09-09T16:39:42.431+0000] {spark_submit.py:521} INFO - totalExecutorCores      null
[2025-09-09T16:39:42.431+0000] {spark_submit.py:521} INFO - propertiesFile          null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - driverMemory            null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - driverCores             null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - driverExtraClassPath    null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - driverExtraLibraryPath  null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - driverExtraJavaOptions  null
[2025-09-09T16:39:42.432+0000] {spark_submit.py:521} INFO - supervise               false
[2025-09-09T16:39:42.433+0000] {spark_submit.py:521} INFO - queue                   null
[2025-09-09T16:39:42.433+0000] {spark_submit.py:521} INFO - numExecutors            null
[2025-09-09T16:39:42.433+0000] {spark_submit.py:521} INFO - files                   null
[2025-09-09T16:39:42.433+0000] {spark_submit.py:521} INFO - pyFiles                 null
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - archives                null
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - mainClass               null
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - primaryResource         file:/opt/spark-jobs/elt/transform/gold_transform.py
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - name                    arrow-spark
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - childArgs               []
[2025-09-09T16:39:42.434+0000] {spark_submit.py:521} INFO - jars                    null
[2025-09-09T16:39:42.435+0000] {spark_submit.py:521} INFO - packages                org.apache.hadoop:hadoop-aws:3.4.1,software.amazon.awssdk:bundle:2.20.15
[2025-09-09T16:39:42.435+0000] {spark_submit.py:521} INFO - packagesExclusions      null
[2025-09-09T16:39:42.435+0000] {spark_submit.py:521} INFO - repositories            null
[2025-09-09T16:39:42.435+0000] {spark_submit.py:521} INFO - verbose                 true
[2025-09-09T16:39:42.436+0000] {spark_submit.py:521} INFO - 
[2025-09-09T16:39:42.436+0000] {spark_submit.py:521} INFO - Spark properties used, including those specified through
[2025-09-09T16:39:42.436+0000] {spark_submit.py:521} INFO - --conf and those from the properties file null:
[2025-09-09T16:39:42.437+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-09-09T16:39:42.437+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-09-09T16:39:42.437+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-09-09T16:39:42.438+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-09-09T16:39:42.438+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-09-09T16:39:42.438+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-09-09T16:39:42.438+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-09-09T16:39:42.439+0000] {spark_submit.py:521} INFO - 
[2025-09-09T16:39:42.439+0000] {spark_submit.py:521} INFO - 
[2025-09-09T16:39:42.538+0000] {spark_submit.py:521} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-09-09T16:39:42.622+0000] {spark_submit.py:521} INFO - Ivy Default Cache set to: /home/***/.ivy2.5.2/cache
[2025-09-09T16:39:42.623+0000] {spark_submit.py:521} INFO - The jars for the packages stored in: /home/***/.ivy2.5.2/jars
[2025-09-09T16:39:42.627+0000] {spark_submit.py:521} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-09-09T16:39:42.627+0000] {spark_submit.py:521} INFO - software.amazon.awssdk#bundle added as a dependency
[2025-09-09T16:39:42.628+0000] {spark_submit.py:521} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-3f94bfc6-f032-4116-8f6e-c495be103b0c;1.0
[2025-09-09T16:39:42.628+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-09-09T16:39:42.802+0000] {spark_submit.py:521} INFO - found org.apache.hadoop#hadoop-aws;3.4.1 in central
[2025-09-09T16:39:42.827+0000] {spark_submit.py:521} INFO - found software.amazon.awssdk#bundle;2.24.6 in central
[2025-09-09T16:39:42.843+0000] {spark_submit.py:521} INFO - found org.wildfly.openssl#wildfly-openssl;1.1.3.Final in central
[2025-09-09T16:39:42.862+0000] {spark_submit.py:521} INFO - :: resolution report :: resolve 228ms :: artifacts dl 6ms
[2025-09-09T16:39:42.863+0000] {spark_submit.py:521} INFO - :: modules in use:
[2025-09-09T16:39:42.864+0000] {spark_submit.py:521} INFO - org.apache.hadoop#hadoop-aws;3.4.1 from central in [default]
[2025-09-09T16:39:42.864+0000] {spark_submit.py:521} INFO - org.wildfly.openssl#wildfly-openssl;1.1.3.Final from central in [default]
[2025-09-09T16:39:42.865+0000] {spark_submit.py:521} INFO - software.amazon.awssdk#bundle;2.24.6 from central in [default]
[2025-09-09T16:39:42.865+0000] {spark_submit.py:521} INFO - :: evicted modules:
[2025-09-09T16:39:42.865+0000] {spark_submit.py:521} INFO - software.amazon.awssdk#bundle;2.20.15 by [software.amazon.awssdk#bundle;2.24.6] in [default]
[2025-09-09T16:39:42.865+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T16:39:42.866+0000] {spark_submit.py:521} INFO - |                  |            modules            ||   artifacts   |
[2025-09-09T16:39:42.866+0000] {spark_submit.py:521} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-09-09T16:39:42.866+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T16:39:42.866+0000] {spark_submit.py:521} INFO - |      default     |   4   |   0   |   0   |   1   ||   3   |   0   |
[2025-09-09T16:39:42.867+0000] {spark_submit.py:521} INFO - ---------------------------------------------------------------------
[2025-09-09T16:39:42.868+0000] {spark_submit.py:521} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-3f94bfc6-f032-4116-8f6e-c495be103b0c
[2025-09-09T16:39:42.869+0000] {spark_submit.py:521} INFO - confs: [default]
[2025-09-09T16:39:42.874+0000] {spark_submit.py:521} INFO - 0 artifacts copied, 3 already retrieved (0kB/6ms)
[2025-09-09T16:39:43.103+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-09-09T16:39:43.302+0000] {spark_submit.py:521} INFO - Main class:
[2025-09-09T16:39:43.303+0000] {spark_submit.py:521} INFO - org.apache.spark.deploy.PythonRunner
[2025-09-09T16:39:43.304+0000] {spark_submit.py:521} INFO - Arguments:
[2025-09-09T16:39:43.305+0000] {spark_submit.py:521} INFO - file:/opt/spark-jobs/elt/transform/gold_transform.py
[2025-09-09T16:39:43.305+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar,file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2025-09-09T16:39:43.307+0000] {spark_submit.py:521} INFO - Spark config:
[2025-09-09T16:39:43.308+0000] {spark_submit.py:521} INFO - (spark.app.name,arrow-spark)
[2025-09-09T16:39:43.309+0000] {spark_submit.py:521} INFO - (spark.app.submitTime,1757435983265)
[2025-09-09T16:39:43.309+0000] {spark_submit.py:521} INFO - (spark.files,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar,file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar)
[2025-09-09T16:39:43.309+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.access.key,*********(redacted))
[2025-09-09T16:39:43.310+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.connection.ssl.enabled,false)
[2025-09-09T16:39:43.310+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.endpoint,http://minio:9000)
[2025-09-09T16:39:43.310+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.impl,org.apache.hadoop.fs.s3a.S3AFileSystem)
[2025-09-09T16:39:43.310+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.path.style.access,true)
[2025-09-09T16:39:43.311+0000] {spark_submit.py:521} INFO - (spark.hadoop.fs.s3a.secret.key,*********(redacted))
[2025-09-09T16:39:43.311+0000] {spark_submit.py:521} INFO - (spark.jars,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar,file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar)
[2025-09-09T16:39:43.311+0000] {spark_submit.py:521} INFO - (spark.master,spark://spark-master:7077)
[2025-09-09T16:39:43.312+0000] {spark_submit.py:521} INFO - (spark.repl.local.jars,file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar,file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar,file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar)
[2025-09-09T16:39:43.312+0000] {spark_submit.py:521} INFO - (spark.submit.deployMode,client)
[2025-09-09T16:39:43.312+0000] {spark_submit.py:521} INFO - (spark.submit.pyFiles,/home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar,/home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar,/home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar)
[2025-09-09T16:39:43.312+0000] {spark_submit.py:521} INFO - Classpath elements:
[2025-09-09T16:39:43.314+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar
[2025-09-09T16:39:43.315+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar
[2025-09-09T16:39:43.315+0000] {spark_submit.py:521} INFO - file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2025-09-09T16:39:43.315+0000] {spark_submit.py:521} INFO - 
[2025-09-09T16:39:43.316+0000] {spark_submit.py:521} INFO - 
[2025-09-09T16:39:45.530+0000] {spark_submit.py:521} INFO - Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
[2025-09-09T16:39:45.533+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SparkContext: Running Spark version 4.0.1
[2025-09-09T16:39:45.535+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SparkContext: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-09-09T16:39:45.535+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SparkContext: Java version 17.0.16
[2025-09-09T16:39:45.558+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceUtils: ==============================================================
[2025-09-09T16:39:45.559+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-09-09T16:39:45.559+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceUtils: ==============================================================
[2025-09-09T16:39:45.559+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SparkContext: Submitted application: SilverTopCVTransformTest
[2025-09-09T16:39:45.583+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-09-09T16:39:45.585+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceProfile: Limiting resource is cpu
[2025-09-09T16:39:45.586+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-09-09T16:39:45.636+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SecurityManager: Changing view acls to: ***
[2025-09-09T16:39:45.637+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SecurityManager: Changing modify acls to: ***
[2025-09-09T16:39:45.638+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SecurityManager: Changing view acls groups to: ***
[2025-09-09T16:39:45.638+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SecurityManager: Changing modify acls groups to: ***
[2025-09-09T16:39:45.641+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-09-09T16:39:45.962+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:45 INFO Utils: Successfully started service 'sparkDriver' on port 35929.
[2025-09-09T16:39:46.008+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkEnv: Registering MapOutputTracker
[2025-09-09T16:39:46.020+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkEnv: Registering BlockManagerMaster
[2025-09-09T16:39:46.037+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-09-09T16:39:46.037+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-09-09T16:39:46.040+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-09-09T16:39:46.058+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e31cf37f-1c19-4b20-8e1f-41199cb80e7a
[2025-09-09T16:39:46.085+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-09-09T16:39:46.210+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-09-09T16:39:46.311+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-09-09T16:39:46.346+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar at spark://118acb478e13:35929/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar with timestamp 1757435985523
[2025-09-09T16:39:46.346+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar at spark://118acb478e13:35929/jars/software.amazon.awssdk_bundle-2.24.6.jar with timestamp 1757435985523
[2025-09-09T16:39:46.347+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkContext: Added JAR file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at spark://118acb478e13:35929/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1757435985523
[2025-09-09T16:39:46.349+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar at file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar with timestamp 1757435985523
[2025-09-09T16:39:46.352+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO Utils: Copying /home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.apache.hadoop_hadoop-aws-3.4.1.jar
[2025-09-09T16:39:46.369+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar at file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar with timestamp 1757435985523
[2025-09-09T16:39:46.370+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:46 INFO Utils: Copying /home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/software.amazon.awssdk_bundle-2.24.6.jar
[2025-09-09T16:39:50.462+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SparkContext: Added file file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar at file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1757435985523
[2025-09-09T16:39:50.463+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Utils: Copying /home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2025-09-09T16:39:50.504+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SecurityManager: Changing view acls to: ***
[2025-09-09T16:39:50.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SecurityManager: Changing modify acls to: ***
[2025-09-09T16:39:50.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SecurityManager: Changing view acls groups to: ***
[2025-09-09T16:39:50.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SecurityManager: Changing modify acls groups to: ***
[2025-09-09T16:39:50.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-09-09T16:39:50.661+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Starting executor ID driver on host 118acb478e13
[2025-09-09T16:39:50.662+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64
[2025-09-09T16:39:50.662+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Java version 17.0.16
[2025-09-09T16:39:50.674+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-09-09T16:39:50.676+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@168a3166 for default.
[2025-09-09T16:39:50.688+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Fetching file:///home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1757435985523
[2025-09-09T16:39:50.708+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Utils: /home/***/.ivy2.5.2/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2025-09-09T16:39:50.714+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Fetching file:///home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar with timestamp 1757435985523
[2025-09-09T16:39:50.716+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Utils: /home/***/.ivy2.5.2/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.apache.hadoop_hadoop-aws-3.4.1.jar
[2025-09-09T16:39:50.723+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Fetching file:///home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar with timestamp 1757435985523
[2025-09-09T16:39:50.981+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Utils: /home/***/.ivy2.5.2/jars/software.amazon.awssdk_bundle-2.24.6.jar has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/software.amazon.awssdk_bundle-2.24.6.jar
[2025-09-09T16:39:50.988+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:50 INFO Executor: Fetching spark://118acb478e13:35929/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar with timestamp 1757435985523
[2025-09-09T16:39:51.026+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO TransportClientFactory: Successfully created connection to 118acb478e13/172.18.0.9:35929 after 24 ms (0 ms spent in bootstraps)
[2025-09-09T16:39:51.032+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Utils: Fetching spark://118acb478e13:35929/jars/org.apache.hadoop_hadoop-aws-3.4.1.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp4477884070884642063.tmp
[2025-09-09T16:39:51.055+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Utils: /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp4477884070884642063.tmp has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.apache.hadoop_hadoop-aws-3.4.1.jar
[2025-09-09T16:39:51.062+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Executor: Adding file:/tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.apache.hadoop_hadoop-aws-3.4.1.jar to class loader default
[2025-09-09T16:39:51.063+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Executor: Fetching spark://118acb478e13:35929/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar with timestamp 1757435985523
[2025-09-09T16:39:51.064+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Utils: Fetching spark://118acb478e13:35929/jars/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp809779018536780100.tmp
[2025-09-09T16:39:51.071+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Utils: /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp809779018536780100.tmp has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar
[2025-09-09T16:39:51.077+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Executor: Adding file:/tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/org.wildfly.openssl_wildfly-openssl-1.1.3.Final.jar to class loader default
[2025-09-09T16:39:51.077+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Executor: Fetching spark://118acb478e13:35929/jars/software.amazon.awssdk_bundle-2.24.6.jar with timestamp 1757435985523
[2025-09-09T16:39:51.078+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:51 INFO Utils: Fetching spark://118acb478e13:35929/jars/software.amazon.awssdk_bundle-2.24.6.jar to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp10646119392673791094.tmp
[2025-09-09T16:39:57.880+0000] {spark_submit.py:521} INFO - 25/09/09 16:39:57 INFO Utils: /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/fetchFileTemp10646119392673791094.tmp has been previously copied to /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/software.amazon.awssdk_bundle-2.24.6.jar
[2025-09-09T16:40:07.014+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO Executor: Adding file:/tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/userFiles-e12f91f1-14d5-4010-9ca0-d43beb474f57/software.amazon.awssdk_bundle-2.24.6.jar to class loader default
[2025-09-09T16:40:07.102+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40003.
[2025-09-09T16:40:07.103+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO NettyBlockTransferService: Server created on 118acb478e13:40003
[2025-09-09T16:40:07.108+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-09-09T16:40:07.178+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 118acb478e13, 40003, None)
[2025-09-09T16:40:07.262+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO BlockManagerMasterEndpoint: Registering block manager 118acb478e13:40003 with 434.4 MiB RAM, BlockManagerId(driver, 118acb478e13, 40003, None)
[2025-09-09T16:40:07.300+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 118acb478e13, 40003, None)
[2025-09-09T16:40:07.306+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 118acb478e13, 40003, None)
[2025-09-09T16:40:12.897+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-09-09T16:40:12.901+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:12 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-09-09T16:40:14.935+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:14 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-09-09T16:40:14.956+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:14 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-09-09T16:40:14.957+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:14 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-09-09T16:40:15.093+0000] {spark_submit.py:521} INFO - SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
[2025-09-09T16:40:15.094+0000] {spark_submit.py:521} INFO - SLF4J: Defaulting to no-operation (NOP) logger implementation
[2025-09-09T16:40:15.095+0000] {spark_submit.py:521} INFO - SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[2025-09-09T16:40:17.617+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:17 INFO HadoopFSUtils: Listing s3a://job/silver/topcv/topcv_jobs.parquet with listFiles API
[2025-09-09T16:40:17.761+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:17 INFO InMemoryFileIndex: It took 161 ms to list leaf files for 1 paths.
[2025-09-09T16:40:18.241+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-09-09T16:40:18.309+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-09-09T16:40:18.315+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)
[2025-09-09T16:40:18.317+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:18.324+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:18.333+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-09-09T16:40:18.454+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-09-09T16:40:18.478+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 117.1 KiB, free 434.3 MiB)
[2025-09-09T16:40:18.612+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.1 KiB, free 434.2 MiB)
[2025-09-09T16:40:18.636+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:18.668+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:18.679+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-09-09T16:40:18.742+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 10811 bytes)
[2025-09-09T16:40:18.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-09-09T16:40:19.918+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3166 bytes result sent to driver
[2025-09-09T16:40:19.939+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1222 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:19.952+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0 whose tasks have all completed, from pool
[2025-09-09T16:40:19.964+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO DAGScheduler: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1582 ms
[2025-09-09T16:40:19.970+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:19.971+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO TaskSchedulerImpl: Canceling stage 0
[2025-09-09T16:40:19.974+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-09-09T16:40:19.982+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:19 INFO DAGScheduler: Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1742.038235 ms
[2025-09-09T16:40:21.047+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO HadoopFSUtils: Listing s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet with listFiles API
[2025-09-09T16:40:21.097+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO InMemoryFileIndex: It took 51 ms to list leaf files for 1 paths.
[2025-09-09T16:40:21.129+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0
[2025-09-09T16:40:21.131+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-09-09T16:40:21.131+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)
[2025-09-09T16:40:21.132+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:21.133+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:21.134+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-09-09T16:40:21.144+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 117.1 KiB, free 434.1 MiB)
[2025-09-09T16:40:21.158+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 42.1 KiB, free 434.1 MiB)
[2025-09-09T16:40:21.159+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:21.167+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:21.168+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-09-09T16:40:21.174+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 10827 bytes)
[2025-09-09T16:40:21.177+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-09-09T16:40:21.353+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3177 bytes result sent to driver
[2025-09-09T16:40:21.363+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 192 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:21.364+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0 whose tasks have all completed, from pool
[2025-09-09T16:40:21.365+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) finished in 230 ms
[2025-09-09T16:40:21.365+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:21.366+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSchedulerImpl: Canceling stage 1
[2025-09-09T16:40:21.366+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-09-09T16:40:21.366+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:21 INFO DAGScheduler: Job 1 finished: parquet at NativeMethodAccessorImpl.java:0, took 235.523253 ms
[2025-09-09T16:40:23.315+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:23 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:23.322+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:23 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:23.340+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:23 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:23.342+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:23 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:24.085+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 326.487706 ms
[2025-09-09T16:40:24.123+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 15.507784 ms
[2025-09-09T16:40:24.142+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 219.4 KiB, free 434.2 MiB)
[2025-09-09T16:40:24.157+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 434.1 MiB)
[2025-09-09T16:40:24.163+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:24.183+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:24.223+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 7.481244 ms
[2025-09-09T16:40:24.226+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 219.4 KiB, free 433.9 MiB)
[2025-09-09T16:40:24.239+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 433.9 MiB)
[2025-09-09T16:40:24.240+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SparkContext: Created broadcast 3 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:24.241+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:24.350+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Registering RDD 12 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 0
[2025-09-09T16:40:24.355+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Got map stage job 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:24.356+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Final stage: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:24.357+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:24.358+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:24.359+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[12] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:24.374+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 44.0 KiB, free 433.9 MiB)
[2025-09-09T16:40:24.379+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 433.8 MiB)
[2025-09-09T16:40:24.384+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:24.385+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[12] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:24.386+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-09-09T16:40:24.390+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:24.390+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:24.391+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-09-09T16:40:24.392+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO Executor: Running task 1.0 in stage 2.0 (TID 3)
[2025-09-09T16:40:24.552+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 114.18883 ms
[2025-09-09T16:40:24.569+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 132.043419 ms
[2025-09-09T16:40:24.586+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 23.70935 ms
[2025-09-09T16:40:24.630+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 24.073259 ms
[2025-09-09T16:40:24.660+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SecurityManager: Changing view acls to: ***
[2025-09-09T16:40:24.660+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SecurityManager: Changing modify acls to: ***
[2025-09-09T16:40:24.664+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SecurityManager: Changing view acls groups to: ***
[2025-09-09T16:40:24.665+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SecurityManager: Changing modify acls groups to: ***
[2025-09-09T16:40:24.667+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: *** groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY; RPC SSL disabled
[2025-09-09T16:40:24.702+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 14.554862 ms
[2025-09-09T16:40:24.715+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO CodeGenerator: Code generated in 4.936434 ms
[2025-09-09T16:40:24.734+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:24.735+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:24 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:25.307+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:25 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-09-09T16:40:25.308+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:25 INFO CodecPool: Got brand-new decompressor [.snappy]
[2025-09-09T16:40:25.850+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 3439 bytes result sent to driver
[2025-09-09T16:40:25.885+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1496 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:26.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO Executor: Finished task 1.0 in stage 2.0 (TID 3). 3396 bytes result sent to driver
[2025-09-09T16:40:26.081+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 1690 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:26.083+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO TaskSchedulerImpl: Removed TaskSet 2.0 whose tasks have all completed, from pool
[2025-09-09T16:40:26.084+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: ShuffleMapStage 2 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1720 ms
[2025-09-09T16:40:26.085+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:26.089+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:26.090+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:26.091+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:26.113+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO ShufflePartitionsUtil: For shuffle(0, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:26.228+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:26.262+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:26.263+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:26.263+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:26.263+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:26.263+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:26.264+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:26.380+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:26.366564574, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:26.473+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO CodeGenerator: Code generated in 36.35899 ms
[2025-09-09T16:40:26.500+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:26.502+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:26.502+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:26.503+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-09-09T16:40:26.503+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:26.503+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:26.537+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 272.4 KiB, free 433.6 MiB)
[2025-09-09T16:40:26.540+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 99.4 KiB, free 433.5 MiB)
[2025-09-09T16:40:26.541+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:26.542+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:26.543+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-09-09T16:40:26.549+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:26.550+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
[2025-09-09T16:40:26.655+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO ShuffleBlockFetcherIterator: Getting 2 (44.3 KiB) non-empty blocks including 2 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:26.657+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 32 ms
[2025-09-09T16:40:26.763+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO CodeGenerator: Code generated in 54.526358 ms
[2025-09-09T16:40:26.768+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:26.769+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:26.769+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:26.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:26.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:26.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:26.775+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:26.778+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:26.812+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:26.896+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:26 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-09-09T16:40:27.105+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to gold/dim_company/dim_company_jobs.parquet/_temporary/0/_temporary/attempt_202509091640261632615753297003459_0004_m_000000_4/part-00000-9b5023b7-e45f-4ef8-bde1-44e4ba466c6e-c000.snappy.parquet. This is Unsupported
[2025-09-09T16:40:27.128+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-1950620236602226291.tmp, offset=0} BaseContentProvider{size=12330, initiated at 2025-09-09T16:40:27.106400356, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:27.585+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:27.583750551, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:27.621+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640261632615753297003459_0004_m_000000_4' to s3a://job/gold/dim_company/dim_company_jobs.parquet/_temporary/0/task_202509091640261632615753297003459_0004_m_000000
[2025-09-09T16:40:27.623+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO SparkHadoopMapRedUtil: attempt_202509091640261632615753297003459_0004_m_000000_4: Committed. Elapsed time: 381 ms.
[2025-09-09T16:40:27.629+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 6729 bytes result sent to driver
[2025-09-09T16:40:27.644+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 1098 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:27.645+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO TaskSchedulerImpl: Removed TaskSet 4.0 whose tasks have all completed, from pool
[2025-09-09T16:40:27.648+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 1133 ms
[2025-09-09T16:40:27.649+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:27.649+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO TaskSchedulerImpl: Canceling stage 4
[2025-09-09T16:40:27.650+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-09-09T16:40:27.650+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 1146.172169 ms
[2025-09-09T16:40:27.650+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO FileFormatWriter: Start to commit write Job a0eaa46c-7d9e-4b58-92f9-bd571bf6ceba.
[2025-09-09T16:40:27.913+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:27 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:27.910976218, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:28.221+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-5972077087142212096.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:28.219273753, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:28.269+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileFormatWriter: Write Job a0eaa46c-7d9e-4b58-92f9-bd571bf6ceba committed. Elapsed time: 618 ms.
[2025-09-09T16:40:28.271+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileFormatWriter: Finished processing stats for write job a0eaa46c-7d9e-4b58-92f9-bd571bf6ceba.
[2025-09-09T16:40:28.286+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/dim_company/dim_company_jobs.parquet
[2025-09-09T16:40:28.320+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:28.321+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:28.321+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:28.322+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:28.349+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-09-09T16:40:28.396+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 37.351152 ms
[2025-09-09T16:40:28.429+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 27.189774 ms
[2025-09-09T16:40:28.435+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 219.6 KiB, free 433.3 MiB)
[2025-09-09T16:40:28.475+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 433.2 MiB)
[2025-09-09T16:40:28.477+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:28.478+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:28.525+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 27.27745 ms
[2025-09-09T16:40:28.528+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 219.6 KiB, free 433.0 MiB)
[2025-09-09T16:40:28.571+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 433.0 MiB)
[2025-09-09T16:40:28.572+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO SparkContext: Created broadcast 7 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:28.573+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:28.584+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Registering RDD 24 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 1
[2025-09-09T16:40:28.585+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Got map stage job 4 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:28.585+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Final stage: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:28.586+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:28.586+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:28.586+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:28.595+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 46.1 KiB, free 432.9 MiB)
[2025-09-09T16:40:28.628+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 432.9 MiB)
[2025-09-09T16:40:28.628+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:28.629+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:28.629+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0
[2025-09-09T16:40:28.630+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:28.631+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:28.631+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
[2025-09-09T16:40:28.640+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO Executor: Running task 1.0 in stage 5.0 (TID 6)
[2025-09-09T16:40:28.661+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 24.354597 ms
[2025-09-09T16:40:28.703+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 41.691639 ms
[2025-09-09T16:40:28.718+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 53.19705 ms
[2025-09-09T16:40:28.736+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 11.121086 ms
[2025-09-09T16:40:28.746+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO CodeGenerator: Code generated in 4.407941 ms
[2025-09-09T16:40:28.748+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:28.750+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:28.896+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 3439 bytes result sent to driver
[2025-09-09T16:40:28.903+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 272 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:28.971+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO Executor: Finished task 1.0 in stage 5.0 (TID 6). 3396 bytes result sent to driver
[2025-09-09T16:40:28.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 347 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:28.979+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO TaskSchedulerImpl: Removed TaskSet 5.0 whose tasks have all completed, from pool
[2025-09-09T16:40:28.981+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: ShuffleMapStage 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 393 ms
[2025-09-09T16:40:28.983+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:28.986+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:28.987+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:28.987+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:28 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:29.006+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO ShufflePartitionsUtil: For shuffle(1, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:29.172+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:29.173+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:29.175+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:29.187+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:29.189+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:29.191+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:29.202+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:29.322+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:29.319847394, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:29.359+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-09-09T16:40:29.400+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO CodeGenerator: Code generated in 33.88608 ms
[2025-09-09T16:40:29.417+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:29.425+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Got job 5 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:29.426+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Final stage: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:29.427+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-09-09T16:40:29.428+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:29.428+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:29.484+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 271.4 KiB, free 433.1 MiB)
[2025-09-09T16:40:29.489+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 98.9 KiB, free 433.0 MiB)
[2025-09-09T16:40:29.492+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:29.496+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:29.498+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-09-09T16:40:29.499+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:29.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
[2025-09-09T16:40:29.547+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO ShuffleBlockFetcherIterator: Getting 2 (68.0 KiB) non-empty blocks including 2 (68.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:29.549+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2025-09-09T16:40:29.584+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO CodeGenerator: Code generated in 34.342724 ms
[2025-09-09T16:40:29.587+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:29.588+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:29.589+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:29.590+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:29.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:29.592+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:29.592+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:29.593+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:29.595+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:29.704+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:29 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-17194256104503466770.tmp, offset=0} BaseContentProvider{size=20198, initiated at 2025-09-09T16:40:29.702215653, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:30.215+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:30.212138368, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:30.294+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640295957004898713498385_0007_m_000000_7' to s3a://job/gold/dim_jobposition/dim_jobposition_jobs.parquet/_temporary/0/task_202509091640295957004898713498385_0007_m_000000
[2025-09-09T16:40:30.297+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO SparkHadoopMapRedUtil: attempt_202509091640295957004898713498385_0007_m_000000_7: Committed. Elapsed time: 410 ms.
[2025-09-09T16:40:30.305+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 6729 bytes result sent to driver
[2025-09-09T16:40:30.306+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 809 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:30.306+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO TaskSchedulerImpl: Removed TaskSet 7.0 whose tasks have all completed, from pool
[2025-09-09T16:40:30.308+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO DAGScheduler: ResultStage 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 873 ms
[2025-09-09T16:40:30.313+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:30.314+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO TaskSchedulerImpl: Canceling stage 7
[2025-09-09T16:40:30.315+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-09-09T16:40:30.315+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO DAGScheduler: Job 5 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 891.505974 ms
[2025-09-09T16:40:30.317+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO FileFormatWriter: Start to commit write Job dca346a5-5103-4f5b-b6db-cdb49f7b8d16.
[2025-09-09T16:40:30.728+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:30 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:30.724298540, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:31.055+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-15485693504832188003.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:31.053139291, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:31.117+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileFormatWriter: Write Job dca346a5-5103-4f5b-b6db-cdb49f7b8d16 committed. Elapsed time: 805 ms.
[2025-09-09T16:40:31.125+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileFormatWriter: Finished processing stats for write job dca346a5-5103-4f5b-b6db-cdb49f7b8d16.
[2025-09-09T16:40:31.128+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/dim_jobposition/dim_jobposition_jobs.parquet
[2025-09-09T16:40:31.238+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:31.239+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:31.240+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:31.241+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:31.308+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 19.223875 ms
[2025-09-09T16:40:31.349+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 35.13206 ms
[2025-09-09T16:40:31.355+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 219.2 KiB, free 432.7 MiB)
[2025-09-09T16:40:31.403+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.7 MiB)
[2025-09-09T16:40:31.412+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO SparkContext: Created broadcast 10 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:31.419+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:31.453+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 19.401225 ms
[2025-09-09T16:40:31.468+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 219.2 KiB, free 432.5 MiB)
[2025-09-09T16:40:31.478+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.5 MiB)
[2025-09-09T16:40:31.486+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO SparkContext: Created broadcast 11 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:31.486+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:31.503+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Registering RDD 36 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 2
[2025-09-09T16:40:31.509+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Got map stage job 6 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:31.510+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Final stage: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:31.511+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:31.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:31.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:31.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 40.3 KiB, free 432.4 MiB)
[2025-09-09T16:40:31.513+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 15.6 KiB, free 432.4 MiB)
[2025-09-09T16:40:31.513+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:31.513+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[36] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:31.514+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0
[2025-09-09T16:40:31.516+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:31.517+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 9) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:31.518+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
[2025-09-09T16:40:31.518+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO Executor: Running task 1.0 in stage 8.0 (TID 9)
[2025-09-09T16:40:31.542+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 14.256549 ms
[2025-09-09T16:40:31.550+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 14.235889 ms
[2025-09-09T16:40:31.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 43.53436 ms
[2025-09-09T16:40:31.608+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 11.170183 ms
[2025-09-09T16:40:31.637+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO CodeGenerator: Code generated in 9.764865 ms
[2025-09-09T16:40:31.641+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:31.644+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:31.852+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 3353 bytes result sent to driver
[2025-09-09T16:40:31.854+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 338 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:31.860+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO Executor: Finished task 1.0 in stage 8.0 (TID 9). 3396 bytes result sent to driver
[2025-09-09T16:40:31.862+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 9) in 346 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:31.863+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO TaskSchedulerImpl: Removed TaskSet 8.0 whose tasks have all completed, from pool
[2025-09-09T16:40:31.864+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: ShuffleMapStage 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 357 ms
[2025-09-09T16:40:31.865+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:31.865+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:31.866+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:31.866+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:31.874+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO ShufflePartitionsUtil: For shuffle(2, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:31.916+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:31.919+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:31.920+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:31.921+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:31.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:31.923+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:31.924+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:31 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:32.040+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:32.037656051, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:32.192+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO CodeGenerator: Code generated in 80.766385 ms
[2025-09-09T16:40:32.243+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:32.257+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Got job 7 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:32.259+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:32.264+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
[2025-09-09T16:40:32.281+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:32.283+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:32.304+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 272.1 KiB, free 432.1 MiB)
[2025-09-09T16:40:32.368+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 99.3 KiB, free 432.0 MiB)
[2025-09-09T16:40:32.371+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:32.373+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[39] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:32.374+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0
[2025-09-09T16:40:32.376+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:32.378+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
[2025-09-09T16:40:32.415+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO ShuffleBlockFetcherIterator: Getting 2 (2.5 KiB) non-empty blocks including 2 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:32.416+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 5 ms
[2025-09-09T16:40:32.469+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO CodeGenerator: Code generated in 52.10217 ms
[2025-09-09T16:40:32.471+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:32.472+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:32.473+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:32.476+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:32.476+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:32.477+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:32.477+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:32.477+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:32.478+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:32.561+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-17326526326574725792.tmp, offset=0} BaseContentProvider{size=1951, initiated at 2025-09-09T16:40:32.557697035, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:32.899+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:32.898175376, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:32.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640321824165781715151964_0010_m_000000_10' to s3a://job/gold/dim_date/dim_date_jobs.parquet/_temporary/0/task_202509091640321824165781715151964_0010_m_000000
[2025-09-09T16:40:32.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO SparkHadoopMapRedUtil: attempt_202509091640321824165781715151964_0010_m_000000_10: Committed. Elapsed time: 283 ms.
[2025-09-09T16:40:32.924+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 6729 bytes result sent to driver
[2025-09-09T16:40:32.925+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 550 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:32.925+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSchedulerImpl: Removed TaskSet 10.0 whose tasks have all completed, from pool
[2025-09-09T16:40:32.927+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 657 ms
[2025-09-09T16:40:32.927+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:32.929+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSchedulerImpl: Canceling stage 10
[2025-09-09T16:40:32.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished
[2025-09-09T16:40:32.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO DAGScheduler: Job 7 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 685.080634 ms
[2025-09-09T16:40:32.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:32 INFO FileFormatWriter: Start to commit write Job eb44b69c-5742-4830-8de2-6119836b4cac.
[2025-09-09T16:40:33.145+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:33.142954811, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:33.347+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-12465986591929802219.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:33.345245550, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:33.387+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileFormatWriter: Write Job eb44b69c-5742-4830-8de2-6119836b4cac committed. Elapsed time: 460 ms.
[2025-09-09T16:40:33.388+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileFormatWriter: Finished processing stats for write job eb44b69c-5742-4830-8de2-6119836b4cac.
[2025-09-09T16:40:33.395+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/dim_date/dim_date_jobs.parquet
[2025-09-09T16:40:33.420+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:33.420+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:33.421+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:33.421+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:33.446+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 12.071136 ms
[2025-09-09T16:40:33.456+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 4.413663 ms
[2025-09-09T16:40:33.459+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 219.2 KiB, free 432.8 MiB)
[2025-09-09T16:40:33.465+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.8 MiB)
[2025-09-09T16:40:33.467+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SparkContext: Created broadcast 14 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:33.468+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:33.483+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 6.00257 ms
[2025-09-09T16:40:33.487+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 219.2 KiB, free 432.6 MiB)
[2025-09-09T16:40:33.504+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.5 MiB)
[2025-09-09T16:40:33.509+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SparkContext: Created broadcast 15 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:33.510+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:33.511+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Registering RDD 48 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 3
[2025-09-09T16:40:33.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Got map stage job 8 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:33.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Final stage: ShuffleMapStage 11 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:33.512+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:33.513+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:33.513+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:33.514+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 41.5 KiB, free 432.5 MiB)
[2025-09-09T16:40:33.517+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 432.5 MiB)
[2025-09-09T16:40:33.518+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:33.519+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[48] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:33.519+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks resource profile 0
[2025-09-09T16:40:33.522+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:33.523+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 12) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:33.524+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
[2025-09-09T16:40:33.524+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO Executor: Running task 1.0 in stage 11.0 (TID 12)
[2025-09-09T16:40:33.539+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 10.959507 ms
[2025-09-09T16:40:33.540+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 12.043577 ms
[2025-09-09T16:40:33.570+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 20.151933 ms
[2025-09-09T16:40:33.580+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 6.269683 ms
[2025-09-09T16:40:33.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 4.597292 ms
[2025-09-09T16:40:33.593+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:33.594+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:33.700+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 3353 bytes result sent to driver
[2025-09-09T16:40:33.702+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 181 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:33.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO Executor: Finished task 1.0 in stage 11.0 (TID 12). 3353 bytes result sent to driver
[2025-09-09T16:40:33.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 12) in 185 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:33.707+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSchedulerImpl: Removed TaskSet 11.0 whose tasks have all completed, from pool
[2025-09-09T16:40:33.708+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: ShuffleMapStage 11 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 201 ms
[2025-09-09T16:40:33.708+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:33.709+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:33.709+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:33.710+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:33.712+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO ShufflePartitionsUtil: For shuffle(3, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:33.731+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:33.733+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:33.734+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:33.734+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:33.740+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:33.740+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:33.740+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:33.811+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:33.809202931, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:33.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 14.622241 ms
[2025-09-09T16:40:33.873+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:33.873+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Got job 9 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:33.875+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Final stage: ResultStage 13 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:33.876+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)
[2025-09-09T16:40:33.876+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:33.877+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:33.889+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 269.9 KiB, free 432.2 MiB)
[2025-09-09T16:40:33.893+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 98.8 KiB, free 432.1 MiB)
[2025-09-09T16:40:33.894+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:33.895+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[51] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:33.895+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-09-09T16:40:33.896+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:33.897+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
[2025-09-09T16:40:33.911+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO ShuffleBlockFetcherIterator: Getting 2 (248.0 B) non-empty blocks including 2 (248.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:33.912+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-09-09T16:40:33.926+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodeGenerator: Code generated in 14.53269 ms
[2025-09-09T16:40:33.928+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:33.929+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:33.929+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:33.929+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:33.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:33.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:33.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:33.930+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:33.931+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:33.971+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:33 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-11239442099844885202.tmp, offset=0} BaseContentProvider{size=853, initiated at 2025-09-09T16:40:33.969335635, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:34.268+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:34.264770785, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:34.320+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640331078551725279542176_0013_m_000000_13' to s3a://job/gold/dim_workform/dim_workform_jobs.parquet/_temporary/0/task_202509091640331078551725279542176_0013_m_000000
[2025-09-09T16:40:34.321+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO SparkHadoopMapRedUtil: attempt_202509091640331078551725279542176_0013_m_000000_13: Committed. Elapsed time: 256 ms.
[2025-09-09T16:40:34.323+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 6686 bytes result sent to driver
[2025-09-09T16:40:34.325+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 429 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:34.326+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSchedulerImpl: Removed TaskSet 13.0 whose tasks have all completed, from pool
[2025-09-09T16:40:34.326+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: ResultStage 13 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 451 ms
[2025-09-09T16:40:34.327+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:34.327+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSchedulerImpl: Canceling stage 13
[2025-09-09T16:40:34.327+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-09-09T16:40:34.328+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Job 9 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 453.713837 ms
[2025-09-09T16:40:34.329+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileFormatWriter: Start to commit write Job c2fa2f5c-8d00-4656-8850-670cccfb0952.
[2025-09-09T16:40:34.539+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:34.538304049, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:34.736+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-6041723613914753784.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:34.733764632, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:34.765+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileFormatWriter: Write Job c2fa2f5c-8d00-4656-8850-670cccfb0952 committed. Elapsed time: 435 ms.
[2025-09-09T16:40:34.767+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileFormatWriter: Finished processing stats for write job c2fa2f5c-8d00-4656-8850-670cccfb0952.
[2025-09-09T16:40:34.770+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/dim_workform/dim_workform_jobs.parquet
[2025-09-09T16:40:34.820+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:34.820+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#13, false) > 0), isnotnull(skills_extracted#13))
[2025-09-09T16:40:34.823+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:34.823+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#30, false) > 0), isnotnull(skills_extracted#30))
[2025-09-09T16:40:34.892+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO CodeGenerator: Code generated in 23.690054 ms
[2025-09-09T16:40:34.910+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO CodeGenerator: Code generated in 5.739405 ms
[2025-09-09T16:40:34.913+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 219.3 KiB, free 431.9 MiB)
[2025-09-09T16:40:34.924+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 431.8 MiB)
[2025-09-09T16:40:34.925+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO SparkContext: Created broadcast 18 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:34.927+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:34.940+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO CodeGenerator: Code generated in 6.118995 ms
[2025-09-09T16:40:34.943+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 219.3 KiB, free 431.6 MiB)
[2025-09-09T16:40:34.953+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 431.6 MiB)
[2025-09-09T16:40:34.955+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO SparkContext: Created broadcast 19 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:34.956+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:34.984+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Registering RDD 60 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 4
[2025-09-09T16:40:34.986+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Got map stage job 10 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:34.986+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Final stage: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:34.987+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:34.987+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:34.987+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:34.989+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 48.4 KiB, free 431.5 MiB)
[2025-09-09T16:40:34.991+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 431.5 MiB)
[2025-09-09T16:40:34.993+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:34.994+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[60] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:34.994+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2025-09-09T16:40:34.996+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:34.997+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 15) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:34.998+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
[2025-09-09T16:40:34.999+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:34 INFO Executor: Running task 1.0 in stage 14.0 (TID 15)
[2025-09-09T16:40:35.023+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO CodeGenerator: Code generated in 8.326764 ms
[2025-09-09T16:40:35.385+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO CodeGenerator: Code generated in 31.414067 ms
[2025-09-09T16:40:35.386+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO CodeGenerator: Code generated in 38.848218 ms
[2025-09-09T16:40:35.389+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO CodeGenerator: Code generated in 9.838828 ms
[2025-09-09T16:40:35.390+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:35.392+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:35.565+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 3583 bytes result sent to driver
[2025-09-09T16:40:35.600+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 595 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:35.775+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO Executor: Finished task 1.0 in stage 14.0 (TID 15). 3540 bytes result sent to driver
[2025-09-09T16:40:35.777+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 15) in 780 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:35.780+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO TaskSchedulerImpl: Removed TaskSet 14.0 whose tasks have all completed, from pool
[2025-09-09T16:40:35.781+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO DAGScheduler: ShuffleMapStage 14 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 791 ms
[2025-09-09T16:40:35.781+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:35.784+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:35.784+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:35.784+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:35.791+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO ShufflePartitionsUtil: For shuffle(4, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:35.844+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:35.846+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:35.848+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:35.849+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:35.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:35.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:35.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:36.002+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:35 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:35.969455326, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:36.132+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO CodeGenerator: Code generated in 53.9281 ms
[2025-09-09T16:40:36.138+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:36.142+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Got job 11 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:36.144+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Final stage: ResultStage 16 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:36.144+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[2025-09-09T16:40:36.144+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:36.144+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:36.156+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 274.5 KiB, free 433.1 MiB)
[2025-09-09T16:40:36.159+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 100.5 KiB, free 433.0 MiB)
[2025-09-09T16:40:36.160+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:36.162+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[63] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:36.162+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0
[2025-09-09T16:40:36.164+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:36.167+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
[2025-09-09T16:40:36.179+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO ShuffleBlockFetcherIterator: Getting 2 (46.9 KiB) non-empty blocks including 2 (46.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:36.179+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2025-09-09T16:40:36.198+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO CodeGenerator: Code generated in 17.94216 ms
[2025-09-09T16:40:36.199+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:36.200+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:36.202+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:36.202+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:36.203+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:36.203+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:36.204+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:36.206+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:36.207+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:36.274+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-3079036097501553153.tmp, offset=0} BaseContentProvider{size=19519, initiated at 2025-09-09T16:40:36.270915096, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:36.834+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:36.832947306, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:36.864+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640366201173204114410146_0016_m_000000_16' to s3a://job/gold/dim_skills/dim_skills_jobs.parquet/_temporary/0/task_202509091640366201173204114410146_0016_m_000000
[2025-09-09T16:40:36.865+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO SparkHadoopMapRedUtil: attempt_202509091640366201173204114410146_0016_m_000000_16: Committed. Elapsed time: 406 ms.
[2025-09-09T16:40:36.866+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 6830 bytes result sent to driver
[2025-09-09T16:40:36.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 703 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:36.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSchedulerImpl: Removed TaskSet 16.0 whose tasks have all completed, from pool
[2025-09-09T16:40:36.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: ResultStage 16 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 723 ms
[2025-09-09T16:40:36.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:36.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSchedulerImpl: Canceling stage 16
[2025-09-09T16:40:36.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-09-09T16:40:36.869+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO DAGScheduler: Job 11 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 729.628232 ms
[2025-09-09T16:40:36.869+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:36 INFO FileFormatWriter: Start to commit write Job 6bac5524-858f-4b41-87d0-6a16cde78666.
[2025-09-09T16:40:37.580+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:37 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:37.579255493, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:37.853+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:37 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-9102586105638153061.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:37.851722507, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:37.900+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:37 INFO FileFormatWriter: Write Job 6bac5524-858f-4b41-87d0-6a16cde78666 committed. Elapsed time: 1030 ms.
[2025-09-09T16:40:37.901+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:37 INFO FileFormatWriter: Finished processing stats for write job 6bac5524-858f-4b41-87d0-6a16cde78666.
[2025-09-09T16:40:37.906+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/dim_skills/dim_skills_jobs.parquet
[2025-09-09T16:40:38.062+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.063+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.063+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.064+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.066+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.066+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.067+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.067+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.068+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.068+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.069+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(job_posting_date)
[2025-09-09T16:40:38.069+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(job_posting_date#148))
[2025-09-09T16:40:38.070+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters: IsNotNull(job_posting_date)
[2025-09-09T16:40:38.070+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set(isnotnull(job_posting_date#168))
[2025-09-09T16:40:38.071+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.072+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.072+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.073+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.147+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-09-09T16:40:38.150+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 219.4 KiB, free 433.1 MiB)
[2025-09-09T16:40:38.158+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 433.1 MiB)
[2025-09-09T16:40:38.159+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 8.744136 ms
[2025-09-09T16:40:38.160+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 22 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.161+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.169+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 219.4 KiB, free 432.9 MiB)
[2025-09-09T16:40:38.176+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 17.878503 ms
[2025-09-09T16:40:38.179+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.8 MiB)
[2025-09-09T16:40:38.181+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 14.701683 ms
[2025-09-09T16:40:38.186+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 23 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.189+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.190+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 7.566371 ms
[2025-09-09T16:40:38.193+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 219.6 KiB, free 432.6 MiB)
[2025-09-09T16:40:38.195+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 8.633891 ms
[2025-09-09T16:40:38.198+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Registering RDD 72 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 5
[2025-09-09T16:40:38.199+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got map stage job 12 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:38.200+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ShuffleMapStage 17 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.200+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:38.200+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.201+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.201+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 44.0 KiB, free 432.6 MiB)
[2025-09-09T16:40:38.201+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 16.49079 ms
[2025-09-09T16:40:38.204+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 16.7 KiB, free 432.5 MiB)
[2025-09-09T16:40:38.204+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 219.2 KiB, free 432.3 MiB)
[2025-09-09T16:40:38.205+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.205+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[72] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:38.205+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 17.0 with 2 tasks resource profile 0
[2025-09-09T16:40:38.206+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 219.2 KiB, free 432.1 MiB)
[2025-09-09T16:40:38.206+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:38.207+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 18) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:38.207+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 1.0 in stage 17.0 (TID 18)
[2025-09-09T16:40:38.209+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
[2025-09-09T16:40:38.211+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 432.1 MiB)
[2025-09-09T16:40:38.212+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 24 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.213+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.214+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 432.0 MiB)
[2025-09-09T16:40:38.215+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 27 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.216+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.218+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:38.219+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 427.7 MiB)
[2025-09-09T16:40:38.224+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 26 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.228+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 6.856464 ms
[2025-09-09T16:40:38.229+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 8.190174 ms
[2025-09-09T16:40:38.230+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.230+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 219.2 KiB, free 423.3 MiB)
[2025-09-09T16:40:38.233+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 219.6 KiB, free 423.1 MiB)
[2025-09-09T16:40:38.234+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:38.237+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 423.0 MiB)
[2025-09-09T16:40:38.238+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 28 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.239+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.241+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 423.0 MiB)
[2025-09-09T16:40:38.244+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 29 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.245+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.247+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 9.112422 ms
[2025-09-09T16:40:38.250+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Registering RDD 91 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 6
[2025-09-09T16:40:38.251+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got map stage job 13 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:38.251+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.251+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:38.252+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.252+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[91] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.254+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 46.1 KiB, free 422.7 MiB)
[2025-09-09T16:40:38.254+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 219.2 KiB, free 422.7 MiB)
[2025-09-09T16:40:38.255+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 422.7 MiB)
[2025-09-09T16:40:38.256+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.256+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[91] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:38.256+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks resource profile 0
[2025-09-09T16:40:38.258+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Registering RDD 93 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 7
[2025-09-09T16:40:38.258+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got map stage job 14 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:38.258+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ShuffleMapStage 19 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.258+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:38.259+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.259+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 19) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:38.260+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 20) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:38.260+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[93] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.262+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 42.1 KiB, free 422.7 MiB)
[2025-09-09T16:40:38.264+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 16.3 KiB, free 422.7 MiB)
[2025-09-09T16:40:38.264+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 422.6 MiB)
[2025-09-09T16:40:38.264+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.266+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[93] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:38.266+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0
[2025-09-09T16:40:38.267+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 30 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.267+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:38.268+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 1.0 in stage 18.0 (TID 20)
[2025-09-09T16:40:38.271+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 18.0 (TID 19)
[2025-09-09T16:40:38.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 15.065133 ms
[2025-09-09T16:40:38.290+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Registering RDD 99 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 8
[2025-09-09T16:40:38.290+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got map stage job 15 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:38.291+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ShuffleMapStage 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.292+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:38.292+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.292+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[99] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.294+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 41.5 KiB, free 422.6 MiB)
[2025-09-09T16:40:38.295+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 16.2 KiB, free 422.6 MiB)
[2025-09-09T16:40:38.298+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.299+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[99] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:38.299+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0
[2025-09-09T16:40:38.310+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 23.617403 ms
[2025-09-09T16:40:38.310+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 20.783265 ms
[2025-09-09T16:40:38.317+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:38.332+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:38.332+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 3353 bytes result sent to driver
[2025-09-09T16:40:38.338+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 21) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:38.340+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 128 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:38.341+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 19.0 (TID 21)
[2025-09-09T16:40:38.365+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 27.088966 ms
[2025-09-09T16:40:38.387+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 20.194003 ms
[2025-09-09T16:40:38.389+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 1.0 in stage 17.0 (TID 18). 3396 bytes result sent to driver
[2025-09-09T16:40:38.390+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 22) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:38.393+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 18) in 185 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:38.395+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 17.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.395+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 1.0 in stage 19.0 (TID 22)
[2025-09-09T16:40:38.396+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ShuffleMapStage 17 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 193 ms
[2025-09-09T16:40:38.396+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:38.397+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: running: HashSet(ShuffleMapStage 18, ShuffleMapStage 19, ShuffleMapStage 20)
[2025-09-09T16:40:38.397+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:38.397+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:38.399+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:38.413+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.415+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.417+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.418+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.428+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShufflePartitionsUtil: For shuffle(5, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:38.441+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 30.167694 ms
[2025-09-09T16:40:38.464+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:38.519+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 18.0 (TID 19). 3396 bytes result sent to driver
[2025-09-09T16:40:38.527+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 23) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:38.529+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 20.0 (TID 23)
[2025-09-09T16:40:38.530+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 19) in 272 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:38.553+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 17.099961 ms
[2025-09-09T16:40:38.560+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FilterCompat: Filtering using predicate: noteq(job_posting_date, null)
[2025-09-09T16:40:38.573+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FilterCompat: Filtering using predicate: noteq(job_posting_date, null)
[2025-09-09T16:40:38.588+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 1.0 in stage 18.0 (TID 20). 3439 bytes result sent to driver
[2025-09-09T16:40:38.590+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 20) in 331 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:38.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 18.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.592+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 24) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:38.595+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ShuffleMapStage 18 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 341 ms
[2025-09-09T16:40:38.595+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:38.596+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: running: HashSet(ShuffleMapStage 19, ShuffleMapStage 20)
[2025-09-09T16:40:38.596+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:38.596+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:38.612+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 1.0 in stage 20.0 (TID 24)
[2025-09-09T16:40:38.612+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.613+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.614+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.616+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.637+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 109.028513 ms
[2025-09-09T16:40:38.643+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShufflePartitionsUtil: For shuffle(6, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:38.645+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 87.899311 ms
[2025-09-09T16:40:38.695+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.704+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got job 16 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:38.705+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ResultStage 22 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.705+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)
[2025-09-09T16:40:38.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[102] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 44.6 KiB, free 418.3 MiB)
[2025-09-09T16:40:38.709+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:38.710+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 67.422802 ms
[2025-09-09T16:40:38.730+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 418.2 MiB)
[2025-09-09T16:40:38.734+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
[2025-09-09T16:40:38.742+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.745+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[102] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:38.746+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0
[2025-09-09T16:40:38.768+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:38.788+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 1.0 in stage 19.0 (TID 22). 3492 bytes result sent to driver
[2025-09-09T16:40:38.792+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 25) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:38.795+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 22) in 404 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:38.803+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 19.0 (TID 21). 3535 bytes result sent to driver
[2025-09-09T16:40:38.809+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 21) in 473 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:38.810+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ShuffleMapStage 19 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 546 ms
[2025-09-09T16:40:38.810+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:38.811+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: running: HashSet(ShuffleMapStage 20, ResultStage 22)
[2025-09-09T16:40:38.811+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:38.813+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:38.814+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 19.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.816+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 22.0 (TID 25)
[2025-09-09T16:40:38.816+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.816+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.816+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.817+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.820+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 50.760133 ms
[2025-09-09T16:40:38.825+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShufflePartitionsUtil: For shuffle(7, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:38.833+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Getting 2 (44.3 KiB) non-empty blocks including 2 (44.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:38.834+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms
[2025-09-09T16:40:38.852+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.854+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 15.633717 ms
[2025-09-09T16:40:38.861+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got job 17 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:38.861+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ResultStage 24 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.862+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
[2025-09-09T16:40:38.863+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.863+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[105] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.864+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 42.1 KiB, free 423.0 MiB)
[2025-09-09T16:40:38.865+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 422.9 MiB)
[2025-09-09T16:40:38.866+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.866+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[105] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:38.867+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0
[2025-09-09T16:40:38.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 26) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:38.868+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 24.0 (TID 26)
[2025-09-09T16:40:38.886+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Getting 2 (68.0 KiB) non-empty blocks including 2 (68.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:38.887+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-09-09T16:40:38.887+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 1.0 in stage 20.0 (TID 24). 3396 bytes result sent to driver
[2025-09-09T16:40:38.887+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 24) in 297 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:38.892+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 10.938932 ms
[2025-09-09T16:40:38.893+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 20.0 (TID 23). 3396 bytes result sent to driver
[2025-09-09T16:40:38.894+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 23) in 368 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:38.895+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 20.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.896+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ShuffleMapStage 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 605 ms
[2025-09-09T16:40:38.897+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:38.897+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: running: HashSet(ResultStage 22, ResultStage 24)
[2025-09-09T16:40:38.898+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:38.898+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:38.901+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 12.289119 ms
[2025-09-09T16:40:38.921+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:38.922+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:38.933+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShufflePartitionsUtil: For shuffle(8, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:38.933+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 22.0 (TID 25). 19041 bytes result sent to driver
[2025-09-09T16:40:38.934+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 25) in 143 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:38.934+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 22.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.935+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ResultStage 22 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 235 ms
[2025-09-09T16:40:38.935+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:38.936+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Canceling stage 22
[2025-09-09T16:40:38.937+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished
[2025-09-09T16:40:38.937+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Job 16 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 240.383179 ms
[2025-09-09T16:40:38.947+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO CodeGenerator: Code generated in 3.028878 ms
[2025-09-09T16:40:38.950+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Finished task 0.0 in stage 24.0 (TID 26). 26610 bytes result sent to driver
[2025-09-09T16:40:38.952+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 26) in 83 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:38.959+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Removed TaskSet 24.0 whose tasks have all completed, from pool
[2025-09-09T16:40:38.961+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: ResultStage 24 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 90 ms
[2025-09-09T16:40:38.961+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:38.962+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Canceling stage 24
[2025-09-09T16:40:38.962+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished
[2025-09-09T16:40:38.963+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Job 17 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 98.28508 ms
[2025-09-09T16:40:38.963+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 4.0 MiB, free 426.9 MiB)
[2025-09-09T16:40:38.963+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 4.0 MiB, free 422.9 MiB)
[2025-09-09T16:40:38.965+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:38.966+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Got job 18 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:38.967+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Final stage: ResultStage 26 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:38.967+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
[2025-09-09T16:40:38.967+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:38.967+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[108] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:38.968+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 42.6 KiB, free 422.9 MiB)
[2025-09-09T16:40:38.977+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 422.8 MiB)
[2025-09-09T16:40:38.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:38.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[108] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:38.979+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0
[2025-09-09T16:40:38.980+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 27) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:38.980+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO Executor: Running task 0.0 in stage 26.0 (TID 27)
[2025-09-09T16:40:38.983+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Getting 2 (2.5 KiB) non-empty blocks including 2 (2.5 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:38.984+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-09-09T16:40:38.993+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 422.9 MiB)
[2025-09-09T16:40:38.996+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:38 INFO SparkContext: Created broadcast 36 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.019+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 28.5 KiB, free 422.9 MiB)
[2025-09-09T16:40:39.020+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 35.663123 ms
[2025-09-09T16:40:39.022+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 37 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.024+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.025+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.026+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.026+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.039+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 56.536925 ms
[2025-09-09T16:40:39.044+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.044+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.045+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.045+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.048+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Finished task 0.0 in stage 26.0 (TID 27). 5916 bytes result sent to driver
[2025-09-09T16:40:39.049+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 27) in 70 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:39.051+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Removed TaskSet 26.0 whose tasks have all completed, from pool
[2025-09-09T16:40:39.051+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: ResultStage 26 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 82 ms
[2025-09-09T16:40:39.051+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:39.052+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Canceling stage 26
[2025-09-09T16:40:39.052+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished
[2025-09-09T16:40:39.052+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 18 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 86.396609 ms
[2025-09-09T16:40:39.058+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 3.70203 ms
[2025-09-09T16:40:39.059+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 4.0 MiB, free 418.9 MiB)
[2025-09-09T16:40:39.061+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 749.0 B, free 418.9 MiB)
[2025-09-09T16:40:39.062+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 39 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.066+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.066+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.068+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.078+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Got job 19 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:39.079+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Final stage: ResultStage 28 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:39.079+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)
[2025-09-09T16:40:39.080+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:39.080+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[111] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:39.080+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 41.5 KiB, free 418.9 MiB)
[2025-09-09T16:40:39.081+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 418.9 MiB)
[2025-09-09T16:40:39.081+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:39.082+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[111] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:39.082+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0
[2025-09-09T16:40:39.083+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:39.084+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
[2025-09-09T16:40:39.088+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO ShuffleBlockFetcherIterator: Getting 2 (248.0 B) non-empty blocks including 2 (248.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:39.089+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-09-09T16:40:39.102+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 13.709439 ms
[2025-09-09T16:40:39.110+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 5597 bytes result sent to driver
[2025-09-09T16:40:39.112+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 28 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:39.112+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Removed TaskSet 28.0 whose tasks have all completed, from pool
[2025-09-09T16:40:39.113+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: ResultStage 28 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 41 ms
[2025-09-09T16:40:39.113+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:39.113+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Canceling stage 28
[2025-09-09T16:40:39.113+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2025-09-09T16:40:39.114+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 19 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 43.256209 ms
[2025-09-09T16:40:39.117+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 4.0 MiB, free 414.9 MiB)
[2025-09-09T16:40:39.118+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 232.0 B, free 414.9 MiB)
[2025-09-09T16:40:39.119+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 41 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.129+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.130+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.130+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Pushed Filters:
[2025-09-09T16:40:39.131+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceStrategy: Post-Scan Filters: Set()
[2025-09-09T16:40:39.151+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.152+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.152+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.153+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.153+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.153+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.153+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.216+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:39.215292881, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:39.265+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 8.540374 ms
[2025-09-09T16:40:39.281+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 12.650059 ms
[2025-09-09T16:40:39.284+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 220.2 KiB, free 414.6 MiB)
[2025-09-09T16:40:39.292+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 414.6 MiB)
[2025-09-09T16:40:39.295+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 42 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.296+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:39.312+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 9.494045 ms
[2025-09-09T16:40:39.316+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 220.1 KiB, free 414.4 MiB)
[2025-09-09T16:40:39.323+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 414.4 MiB)
[2025-09-09T16:40:39.324+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 43 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.325+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:39.335+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:39.336+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Got job 20 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:39.337+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Final stage: ResultStage 29 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:39.338+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:39.338+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:39.339+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[120] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:39.350+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 272.5 KiB, free 414.1 MiB)
[2025-09-09T16:40:39.352+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 94.3 KiB, free 414.0 MiB)
[2025-09-09T16:40:39.353+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:39.353+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 29 (MapPartitionsRDD[120] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:39.353+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Adding task set 29.0 with 2 tasks resource profile 0
[2025-09-09T16:40:39.356+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11361 bytes)
[2025-09-09T16:40:39.356+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Starting task 1.0 in stage 29.0 (TID 30) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11377 bytes)
[2025-09-09T16:40:39.357+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
[2025-09-09T16:40:39.358+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Running task 1.0 in stage 29.0 (TID 30)
[2025-09-09T16:40:39.382+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 11.638096 ms
[2025-09-09T16:40:39.385+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 9.583013 ms
[2025-09-09T16:40:39.393+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodeGenerator: Code generated in 9.642805 ms
[2025-09-09T16:40:39.395+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.397+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.399+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.400+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.401+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.401+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.402+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.402+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:39.402+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.403+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:39.405+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.406+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:39.406+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:39.407+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:39.407+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:39.408+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:39.448+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:39.477+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:39.478+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:39.482+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:39.505+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-09-09T16:40:39.539+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-12596172885841027826.tmp, offset=0} BaseContentProvider{size=14871, initiated at 2025-09-09T16:40:39.536160993, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:39.541+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-8952730168693474134.tmp, offset=0} BaseContentProvider{size=3633, initiated at 2025-09-09T16:40:39.538930785, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:39.798+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640397669052251528873875_0029_m_000000_29' to s3a://job/gold/fact_job/fact_job_jobs.parquet/_temporary/0/task_202509091640397669052251528873875_0029_m_000000
[2025-09-09T16:40:39.799+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkHadoopMapRedUtil: attempt_202509091640397669052251528873875_0029_m_000000_29: Committed. Elapsed time: 184 ms.
[2025-09-09T16:40:39.801+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Finished task 0.0 in stage 29.0 (TID 29). 3514 bytes result sent to driver
[2025-09-09T16:40:39.803+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 29) in 448 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:39.830+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:39.828888081, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:39.855+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640397669052251528873875_0029_m_000001_30' to s3a://job/gold/fact_job/fact_job_jobs.parquet/_temporary/0/task_202509091640397669052251528873875_0029_m_000001
[2025-09-09T16:40:39.856+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO SparkHadoopMapRedUtil: attempt_202509091640397669052251528873875_0029_m_000001_30: Committed. Elapsed time: 223 ms.
[2025-09-09T16:40:39.856+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO Executor: Finished task 1.0 in stage 29.0 (TID 30). 3471 bytes result sent to driver
[2025-09-09T16:40:39.858+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSetManager: Finished task 1.0 in stage 29.0 (TID 30) in 503 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:39.858+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Removed TaskSet 29.0 whose tasks have all completed, from pool
[2025-09-09T16:40:39.859+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: ResultStage 29 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 521 ms
[2025-09-09T16:40:39.859+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:39.860+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Canceling stage 29
[2025-09-09T16:40:39.860+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished
[2025-09-09T16:40:39.860+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO DAGScheduler: Job 20 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 523.259617 ms
[2025-09-09T16:40:39.861+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:39 INFO FileFormatWriter: Start to commit write Job 99c84028-93d2-4465-9254-66595b47598f.
[2025-09-09T16:40:40.065+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:40.063961405, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:40.263+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:40.262399507, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:40.485+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-8292576603895774603.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:40.483452938, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:40.526+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileFormatWriter: Write Job 99c84028-93d2-4465-9254-66595b47598f committed. Elapsed time: 666 ms.
[2025-09-09T16:40:40.527+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileFormatWriter: Finished processing stats for write job 99c84028-93d2-4465-9254-66595b47598f.
[2025-09-09T16:40:40.554+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/fact_job/fact_job_jobs.parquet
[2025-09-09T16:40:40.630+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.631+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#13, false) > 0), isnotnull(skills_extracted#13))
[2025-09-09T16:40:40.631+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.631+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#30, false) > 0), isnotnull(skills_extracted#30))
[2025-09-09T16:40:40.634+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.634+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#257, false) > 0), isnotnull(skills_extracted#257))
[2025-09-09T16:40:40.635+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.635+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#277, false) > 0), isnotnull(skills_extracted#277))
[2025-09-09T16:40:40.706+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 219.3 KiB, free 413.8 MiB)
[2025-09-09T16:40:40.727+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 413.7 MiB)
[2025-09-09T16:40:40.729+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO SparkContext: Created broadcast 45 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:40.730+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:40.752+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 219.3 KiB, free 413.5 MiB)
[2025-09-09T16:40:40.759+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 40.2 KiB, free 413.5 MiB)
[2025-09-09T16:40:40.760+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO SparkContext: Created broadcast 46 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:40.762+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:40.775+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Registering RDD 129 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 9
[2025-09-09T16:40:40.777+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Got map stage job 21 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:40.779+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Final stage: ShuffleMapStage 30 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:40.779+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:40.780+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:40.780+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[129] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:40.780+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 48.5 KiB, free 413.4 MiB)
[2025-09-09T16:40:40.781+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 18.4 KiB, free 413.4 MiB)
[2025-09-09T16:40:40.783+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:40.785+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[129] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:40.785+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks resource profile 0
[2025-09-09T16:40:40.789+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 31) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:40.791+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 32) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:40.791+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO Executor: Running task 0.0 in stage 30.0 (TID 31)
[2025-09-09T16:40:40.801+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO Executor: Running task 1.0 in stage 30.0 (TID 32)
[2025-09-09T16:40:40.806+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:40.816+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:40.918+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO Executor: Finished task 0.0 in stage 30.0 (TID 31). 3497 bytes result sent to driver
[2025-09-09T16:40:40.919+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 31) in 132 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:40.975+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO Executor: Finished task 1.0 in stage 30.0 (TID 32). 3497 bytes result sent to driver
[2025-09-09T16:40:40.977+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 32) in 189 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:40.977+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO TaskSchedulerImpl: Removed TaskSet 30.0 whose tasks have all completed, from pool
[2025-09-09T16:40:40.977+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: ShuffleMapStage 30 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 200 ms
[2025-09-09T16:40:40.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:40.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:40.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:40.978+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:40.982+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.982+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#13, false) > 0), isnotnull(skills_extracted#13))
[2025-09-09T16:40:40.982+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:40.983+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#30, false) > 0), isnotnull(skills_extracted#30))
[2025-09-09T16:40:40.989+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:40 INFO ShufflePartitionsUtil: For shuffle(9, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:41.020+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 15.933063 ms
[2025-09-09T16:40:41.040+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:41.044+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Got job 22 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:41.044+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Final stage: ResultStage 32 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:41.045+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)
[2025-09-09T16:40:41.045+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:41.045+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[132] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:41.049+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 46.7 KiB, free 413.4 MiB)
[2025-09-09T16:40:41.069+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 413.4 MiB)
[2025-09-09T16:40:41.071+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:41.073+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[132] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:41.073+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0
[2025-09-09T16:40:41.075+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 33) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:41.076+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Running task 0.0 in stage 32.0 (TID 33)
[2025-09-09T16:40:41.083+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ShuffleBlockFetcherIterator: Getting 2 (46.9 KiB) non-empty blocks including 2 (46.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:41.083+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-09-09T16:40:41.098+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 14.743394 ms
[2025-09-09T16:40:41.112+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Finished task 0.0 in stage 32.0 (TID 33). 32503 bytes result sent to driver
[2025-09-09T16:40:41.114+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 33) in 39 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:41.114+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Removed TaskSet 32.0 whose tasks have all completed, from pool
[2025-09-09T16:40:41.115+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: ResultStage 32 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 68 ms
[2025-09-09T16:40:41.115+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:41.115+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Canceling stage 32
[2025-09-09T16:40:41.115+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished
[2025-09-09T16:40:41.115+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Job 22 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 73.20802 ms
[2025-09-09T16:40:41.118+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 4.1 MiB, free 409.7 MiB)
[2025-09-09T16:40:41.121+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 43.5 KiB, free 409.7 MiB)
[2025-09-09T16:40:41.122+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 49 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:41.126+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:41.128+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#13, false) > 0), isnotnull(skills_extracted#13))
[2025-09-09T16:40:41.129+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceStrategy: Pushed Filters: IsNotNull(skills_extracted)
[2025-09-09T16:40:41.130+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceStrategy: Post-Scan Filters: Set((size(skills_extracted#30, false) > 0), isnotnull(skills_extracted#30))
[2025-09-09T16:40:41.165+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 16.244608 ms
[2025-09-09T16:40:41.202+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 16.360084 ms
[2025-09-09T16:40:41.207+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 219.7 KiB, free 409.6 MiB)
[2025-09-09T16:40:41.218+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 409.5 MiB)
[2025-09-09T16:40:41.221+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 50 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:41.223+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:41.252+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 13.851106 ms
[2025-09-09T16:40:41.256+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 219.7 KiB, free 409.3 MiB)
[2025-09-09T16:40:41.266+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 40.3 KiB, free 409.3 MiB)
[2025-09-09T16:40:41.269+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 51 from $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:41.271+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-09-09T16:40:41.285+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Registering RDD 141 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) as input to shuffle 10
[2025-09-09T16:40:41.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Got map stage job 23 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 2 output partitions
[2025-09-09T16:40:41.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Final stage: ShuffleMapStage 33 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:41.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Parents of final stage: List()
[2025-09-09T16:40:41.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:41.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[141] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:41.294+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 80.5 KiB, free 409.2 MiB)
[2025-09-09T16:40:41.295+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 28.1 KiB, free 409.2 MiB)
[2025-09-09T16:40:41.295+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:41.296+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[141] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0, 1))
[2025-09-09T16:40:41.297+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
[2025-09-09T16:40:41.299+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 34) (118acb478e13,executor driver, partition 0, PROCESS_LOCAL, 11350 bytes)
[2025-09-09T16:40:41.306+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 35) (118acb478e13,executor driver, partition 1, PROCESS_LOCAL, 11366 bytes)
[2025-09-09T16:40:41.306+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Running task 1.0 in stage 33.0 (TID 35)
[2025-09-09T16:40:41.308+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Running task 0.0 in stage 33.0 (TID 34)
[2025-09-09T16:40:41.310+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 6.066633 ms
[2025-09-09T16:40:41.323+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 12.590702 ms
[2025-09-09T16:40:41.326+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 22.847494 ms
[2025-09-09T16:40:41.336+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 8.157496 ms
[2025-09-09T16:40:41.350+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 5.472875 ms
[2025-09-09T16:40:41.353+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileScanRDD: Reading File path: s3a://job/silver/vietnamworkcv/vietnamworkcv_jobs.parquet/part-00000-3022c0fc-78e4-42c4-b87f-2c79f58b4358-c000.snappy.parquet, range: 0-51324, partition values: [empty row]
[2025-09-09T16:40:41.354+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileScanRDD: Reading File path: s3a://job/silver/topcv/topcv_jobs.parquet/part-00000-323c1c84-a86f-4c40-8422-45e3f8444460-c000.snappy.parquet, range: 0-8446, partition values: [empty row]
[2025-09-09T16:40:41.487+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Finished task 0.0 in stage 33.0 (TID 34). 7000 bytes result sent to driver
[2025-09-09T16:40:41.488+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 34) in 190 ms on 118acb478e13 (executor driver) (1/2)
[2025-09-09T16:40:41.534+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Finished task 1.0 in stage 33.0 (TID 35). 7000 bytes result sent to driver
[2025-09-09T16:40:41.538+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 35) in 239 ms on 118acb478e13 (executor driver) (2/2)
[2025-09-09T16:40:41.538+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Removed TaskSet 33.0 whose tasks have all completed, from pool
[2025-09-09T16:40:41.550+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: ShuffleMapStage 33 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 262 ms
[2025-09-09T16:40:41.551+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: looking for newly runnable stages
[2025-09-09T16:40:41.552+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: running: HashSet()
[2025-09-09T16:40:41.552+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: waiting: HashSet()
[2025-09-09T16:40:41.553+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: failed: HashSet()
[2025-09-09T16:40:41.556+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ShufflePartitionsUtil: For shuffle(10, advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
[2025-09-09T16:40:41.588+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:41.589+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:41.590+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:41.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:41.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:41.591+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:41.592+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:41.686+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:41.684969629, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:41.758+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 20.480385 ms
[2025-09-09T16:40:41.767+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768
[2025-09-09T16:40:41.769+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Got job 24 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) with 1 output partitions
[2025-09-09T16:40:41.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Final stage: ResultStage 35 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768)
[2025-09-09T16:40:41.770+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)
[2025-09-09T16:40:41.771+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Missing parents: List()
[2025-09-09T16:40:41.771+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[144] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768), which has no missing parents
[2025-09-09T16:40:41.787+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 299.1 KiB, free 408.9 MiB)
[2025-09-09T16:40:41.790+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 108.1 KiB, free 408.8 MiB)
[2025-09-09T16:40:41.792+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1676
[2025-09-09T16:40:41.794+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[144] at $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) (first 15 tasks are for partitions Vector(0))
[2025-09-09T16:40:41.795+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0
[2025-09-09T16:40:41.796+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 36) (118acb478e13,executor driver, partition 0, NODE_LOCAL, 10580 bytes)
[2025-09-09T16:40:41.797+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO Executor: Running task 0.0 in stage 35.0 (TID 36)
[2025-09-09T16:40:41.812+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ShuffleBlockFetcherIterator: Getting 2 (56.0 KiB) non-empty blocks including 2 (56.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-09-09T16:40:41.813+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2025-09-09T16:40:41.829+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodeGenerator: Code generated in 15.871547 ms
[2025-09-09T16:40:41.830+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:41.831+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:41.831+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:41.831+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-09-09T16:40:41.831+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-09-09T16:40:41.832+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-09-09T16:40:41.832+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:41.832+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO CodecConfig: Compression: SNAPPY
[2025-09-09T16:40:41.832+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-09-09T16:40:41.885+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:41 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-2239730556785084731.tmp, offset=0} BaseContentProvider{size=14474, initiated at 2025-09-09T16:40:41.883618779, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:42.242+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:42.238827462, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:42.277+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO FileOutputCommitter: Saved output of task 'attempt_202509091640413621094528788268816_0035_m_000000_36' to s3a://job/gold/fact_jobskill/fact_jobskill_jobs.parquet/_temporary/0/task_202509091640413621094528788268816_0035_m_000000
[2025-09-09T16:40:42.278+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO SparkHadoopMapRedUtil: attempt_202509091640413621094528788268816_0035_m_000000_36: Committed. Elapsed time: 314 ms.
[2025-09-09T16:40:42.282+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO Executor: Finished task 0.0 in stage 35.0 (TID 36). 9906 bytes result sent to driver
[2025-09-09T16:40:42.285+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 36) in 490 ms on 118acb478e13 (executor driver) (1/1)
[2025-09-09T16:40:42.286+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO TaskSchedulerImpl: Removed TaskSet 35.0 whose tasks have all completed, from pool
[2025-09-09T16:40:42.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO DAGScheduler: ResultStage 35 ($anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768) finished in 514 ms
[2025-09-09T16:40:42.287+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-09-09T16:40:42.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO TaskSchedulerImpl: Canceling stage 35
[2025-09-09T16:40:42.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 35: Stage finished
[2025-09-09T16:40:42.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO DAGScheduler: Job 24 finished: $anonfun$withThreadLocalCaptured$2 at CompletableFuture.java:1768, took 519.223516 ms
[2025-09-09T16:40:42.288+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO FileFormatWriter: Start to commit write Job 4572d93d-ce58-4da2-bb1d-bd8c2df423bd.
[2025-09-09T16:40:42.487+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO UploadContentProviders: Stream recreated: ByteArrayContentProvider{buffer with length=0, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:42.486211598, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:42.633+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO UploadContentProviders: Stream recreated: FileWithOffsetContentProvider{file=/tmp/hadoop-***/s3a/s3ablock-0001-18412012006523837402.tmp, offset=0} BaseContentProvider{size=0, initiated at 2025-09-09T16:40:42.632429174, streamCreationCount=2, currentStream=null}
[2025-09-09T16:40:42.666+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO FileFormatWriter: Write Job 4572d93d-ce58-4da2-bb1d-bd8c2df423bd committed. Elapsed time: 379 ms.
[2025-09-09T16:40:42.667+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO FileFormatWriter: Finished processing stats for write job 4572d93d-ce58-4da2-bb1d-bd8c2df423bd.
[2025-09-09T16:40:42.671+0000] {spark_submit.py:521} INFO - ✅ Uploaded Spark DataFrame to MinIO: s3a://job/gold/fact_jobskill/fact_jobskill_jobs.parquet
[2025-09-09T16:40:42.673+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO SparkContext: SparkContext is stopping with exitCode 0 from stop at NativeMethodAccessorImpl.java:0.
[2025-09-09T16:40:42.703+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO SparkUI: Stopped Spark web UI at http://118acb478e13:4040
[2025-09-09T16:40:42.742+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-09-09T16:40:42.814+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO MemoryStore: MemoryStore cleared
[2025-09-09T16:40:42.820+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO BlockManager: BlockManager stopped
[2025-09-09T16:40:42.823+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-09-09T16:40:42.829+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-09-09T16:40:42.852+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:42 INFO SparkContext: Successfully stopped SparkContext
[2025-09-09T16:40:45.767+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO ShutdownHookManager: Shutdown hook called
[2025-09-09T16:40:45.768+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b/pyspark-46344320-2030-431f-a1f2-0f64e483990d
[2025-09-09T16:40:45.775+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-e71a419e-65a6-4506-9d67-cbd951eeb60b
[2025-09-09T16:40:45.786+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO ShutdownHookManager: Deleting directory /tmp/artifacts-497ca1f5-899c-45c3-af42-d7d3e76e8d77
[2025-09-09T16:40:45.792+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-90a38b46-b712-48d3-a7ee-8388fdad693f
[2025-09-09T16:40:45.809+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-09-09T16:40:45.810+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-09-09T16:40:45.810+0000] {spark_submit.py:521} INFO - 25/09/09 16:40:45 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-09-09T16:40:46.466+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-09-09T16:40:46.847+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=spark_transform_pipeline, task_id=gold_transform, run_id=manual__2025-09-09T16:37:17.070695+00:00, execution_date=20250909T163717, start_date=20250909T163939, end_date=20250909T164046
[2025-09-09T16:40:47.194+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 0
[2025-09-09T16:40:47.670+0000] {taskinstance.py:3498} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-09-09T16:40:47.673+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
